++ echo PROB=imo-2024-q4
PROB=imo-2024-q4
++ echo PROB_FILE=/home/peng/dreammie_e/vs/Colab/GoogleDrive-Colab-Files/ColabExamples.txt
PROB_FILE=/home/peng/dreammie_e/vs/Colab/GoogleDrive-Colab-Files/ColabExamples.txt
++ echo MODEL=alphageometry
MODEL=alphageometry
++ echo OUTFILE=/home/peng/ag4mtest/imo-2024-q4.out
OUTFILE=/home/peng/ag4mtest/imo-2024-q4.out
++ echo ERRFILE=/home/peng/ag4mtest/imo-2024-q4.log
ERRFILE=/home/peng/ag4mtest/imo-2024-q4.log
++ BATCH_SIZE=8
++ BEAM_SIZE=32
++ DEPTH=8
++ NWORKERS=1
++ DATA=/home/peng/aglib/ag_ckpt_vocab
++ MELIAD_PATH=/home/peng/aglib/meliad
++ export PYTHONPATH=:/home/peng/ag4masses/alphageometry:/home/peng/aglib:/home/peng/aglib/meliad
++ PYTHONPATH=:/home/peng/ag4masses/alphageometry:/home/peng/aglib:/home/peng/aglib/meliad
++ DDAR_ARGS=(--defs_file=$AGDIR/defs.txt --rules_file=$AGDIR/rules.txt)
++ SEARCH_ARGS=(--beam_size=$BEAM_SIZE --search_depth=$DEPTH)
++ LM_ARGS=(--ckpt_path=$DATA --vocab_path=$DATA/geometry.757.model --gin_search_paths=$MELIAD_PATH/transformer/configs,$AGDIR --gin_file=base_htrans.gin --gin_file=size/medium_150M.gin --gin_file=options/positions_t5.gin --gin_file=options/lr_cosine_decay.gin --gin_file=options/seq_1024_nocache.gin --gin_file=geometry_150M_generate.gin --gin_param=DecoderOnlyLanguageModelGenerate.output_token_losses=True --gin_param=TransformerTaskConfig.batch_size=$BATCH_SIZE --gin_param=TransformerTaskConfig.sequence_length=128 --gin_param=Trainer.restore_state_variables=False)
++ true ==========================================
++ python -m alphageometry --alsologtostderr --problems_file=/home/peng/dreammie_e/vs/Colab/GoogleDrive-Colab-Files/ColabExamples.txt --problem_name=imo-2024-q4 --mode=alphageometry --defs_file=/home/peng/ag4masses/alphageometry/defs.txt --rules_file=/home/peng/ag4masses/alphageometry/rules.txt --beam_size=32 --search_depth=8 --ckpt_path=/home/peng/aglib/ag_ckpt_vocab --vocab_path=/home/peng/aglib/ag_ckpt_vocab/geometry.757.model --gin_search_paths=/home/peng/aglib/meliad/transformer/configs,/home/peng/ag4masses/alphageometry --gin_file=base_htrans.gin --gin_file=size/medium_150M.gin --gin_file=options/positions_t5.gin --gin_file=options/lr_cosine_decay.gin --gin_file=options/seq_1024_nocache.gin --gin_file=geometry_150M_generate.gin --gin_param=DecoderOnlyLanguageModelGenerate.output_token_losses=True --gin_param=TransformerTaskConfig.batch_size=8 --gin_param=TransformerTaskConfig.sequence_length=128 --gin_param=Trainer.restore_state_variables=False --out_file=/home/peng/ag4mtest/imo-2024-q4.out --n_workers=1
/home/peng/pythovenv/lib/python3.10/site-packages/flax/struct.py:132: FutureWarning: jax.tree_util.register_keypaths is deprecated, and will be removed in a future release. Please use `register_pytree_with_keys()` instead.
  jax.tree_util.register_keypaths(data_clz, keypaths)
/home/peng/pythovenv/lib/python3.10/site-packages/flax/struct.py:132: FutureWarning: jax.tree_util.register_keypaths is deprecated, and will be removed in a future release. Please use `register_pytree_with_keys()` instead.
  jax.tree_util.register_keypaths(data_clz, keypaths)
/home/peng/pythovenv/lib/python3.10/site-packages/flax/struct.py:132: FutureWarning: jax.tree_util.register_keypaths is deprecated, and will be removed in a future release. Please use `register_pytree_with_keys()` instead.
  jax.tree_util.register_keypaths(data_clz, keypaths)
2025-01-04 22:19:41.541186: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
/home/peng/pythovenv/lib/python3.10/site-packages/flax/struct.py:132: FutureWarning: jax.tree_util.register_keypaths is deprecated, and will be removed in a future release. Please use `register_pytree_with_keys()` instead.
  jax.tree_util.register_keypaths(data_clz, keypaths)
/home/peng/pythovenv/lib/python3.10/site-packages/flax/struct.py:132: FutureWarning: jax.tree_util.register_keypaths is deprecated, and will be removed in a future release. Please use `register_pytree_with_keys()` instead.
  jax.tree_util.register_keypaths(data_clz, keypaths)
/home/peng/pythovenv/lib/python3.10/site-packages/flax/struct.py:132: FutureWarning: jax.tree_util.register_keypaths is deprecated, and will be removed in a future release. Please use `register_pytree_with_keys()` instead.
  jax.tree_util.register_keypaths(data_clz, keypaths)
I0104 22:19:44.047581 140379616616448 graph.py:498] imo-2024-q4
I0104 22:19:44.047939 140379616616448 graph.py:499] a b c = triangle a b c; d = incenter d a b c; e = foot e d a c; f = foot f d a b; g = on_line g d e, on_circle g d e; h = on_line h d f, on_circle h d f; i = on_line i b c, on_pline i g a c; j = on_line j b c, on_pline j h a b; k = midpoint k a c; l = midpoint l a b; m = on_line m a d, on_circum m a b c ? eqangle m j m i d l d k
I0104 22:19:45.152215 140379616616448 ddar.py:60] Depth 1/1000 time = 1.033886194229126
I0104 22:19:49.690268 140379616616448 ddar.py:60] Depth 2/1000 time = 4.537763595581055
I0104 22:19:57.315907 140379616616448 ddar.py:60] Depth 3/1000 time = 7.625299453735352
I0104 22:20:05.277178 140379616616448 ddar.py:60] Depth 4/1000 time = 7.960901975631714
I0104 22:20:12.829260 140379616616448 ddar.py:60] Depth 5/1000 time = 7.55128812789917
I0104 22:20:21.250066 140379616616448 ddar.py:60] Depth 6/1000 time = 8.407060384750366
I0104 22:20:30.033410 140379616616448 ddar.py:60] Depth 7/1000 time = 8.782946586608887
I0104 22:20:38.563013 140379616616448 ddar.py:60] Depth 8/1000 time = 8.529223203659058
I0104 22:20:46.571357 140379616616448 ddar.py:60] Depth 9/1000 time = 7.976615905761719
I0104 22:20:54.558488 140379616616448 ddar.py:60] Depth 10/1000 time = 7.986857175827026
I0104 22:21:03.277331 140379616616448 ddar.py:60] Depth 11/1000 time = 8.709133625030518
I0104 22:21:03.277714 140379616616448 ddar.py:130] Nothing added, breaking
I0104 22:21:03.277882 140379616616448 alphageometry.py:231] DD+AR failed to solve the problem.
I0104 22:21:03.280198 140379616616448 alphageometry.py:541] Worker 0 initializing. PID=10401
I0104 22:21:03.280366 140379616616448 inference_utils.py:69] Parsing gin configuration.
I0104 22:21:03.280439 140379616616448 inference_utils.py:71] Added Gin search path /home/peng/aglib/meliad/transformer/configs
I0104 22:21:03.280718 140379616616448 inference_utils.py:71] Added Gin search path /home/peng/ag4masses/alphageometry
I0104 22:21:03.280766 140379616616448 inference_utils.py:74] Loading Gin config file base_htrans.gin
I0104 22:21:03.280818 140379616616448 inference_utils.py:74] Loading Gin config file size/medium_150M.gin
I0104 22:21:03.280870 140379616616448 inference_utils.py:74] Loading Gin config file options/positions_t5.gin
I0104 22:21:03.280920 140379616616448 inference_utils.py:74] Loading Gin config file options/lr_cosine_decay.gin
I0104 22:21:03.280969 140379616616448 inference_utils.py:74] Loading Gin config file options/seq_1024_nocache.gin
I0104 22:21:03.281017 140379616616448 inference_utils.py:74] Loading Gin config file geometry_150M_generate.gin
I0104 22:21:03.281078 140379616616448 inference_utils.py:74] Loading Gin config file base_htrans.gin
I0104 22:21:03.281141 140379616616448 inference_utils.py:74] Loading Gin config file size/medium_150M.gin
I0104 22:21:03.281188 140379616616448 inference_utils.py:74] Loading Gin config file options/positions_t5.gin
I0104 22:21:03.281240 140379616616448 inference_utils.py:74] Loading Gin config file options/lr_cosine_decay.gin
I0104 22:21:03.281288 140379616616448 inference_utils.py:74] Loading Gin config file options/seq_1024_nocache.gin
I0104 22:21:03.281328 140379616616448 inference_utils.py:74] Loading Gin config file geometry_150M_generate.gin
I0104 22:21:03.281368 140379616616448 inference_utils.py:76] Overriding Gin param DecoderOnlyLanguageModelGenerate.output_token_losses=True
I0104 22:21:03.281408 140379616616448 inference_utils.py:76] Overriding Gin param TransformerTaskConfig.batch_size=8
I0104 22:21:03.281448 140379616616448 inference_utils.py:76] Overriding Gin param TransformerTaskConfig.sequence_length=128
I0104 22:21:03.281512 140379616616448 inference_utils.py:76] Overriding Gin param Trainer.restore_state_variables=False
I0104 22:21:03.281564 140379616616448 inference_utils.py:76] Overriding Gin param DecoderOnlyLanguageModelGenerate.output_token_losses=True
I0104 22:21:03.281606 140379616616448 inference_utils.py:76] Overriding Gin param TransformerTaskConfig.batch_size=8
I0104 22:21:03.281648 140379616616448 inference_utils.py:76] Overriding Gin param TransformerTaskConfig.sequence_length=128
I0104 22:21:03.281835 140379616616448 inference_utils.py:76] Overriding Gin param Trainer.restore_state_variables=False
I0104 22:21:03.282037 140379616616448 resource_reader.py:50] system_path_file_exists:base_htrans.gin
E0104 22:21:03.282307 140379616616448 resource_reader.py:55] Path not found: base_htrans.gin
I0104 22:21:03.283516 140379616616448 resource_reader.py:50] system_path_file_exists:trainer_configuration.gin
E0104 22:21:03.283735 140379616616448 resource_reader.py:55] Path not found: trainer_configuration.gin
I0104 22:21:03.293648 140379616616448 resource_reader.py:50] system_path_file_exists:size/medium_150M.gin
E0104 22:21:03.294259 140379616616448 resource_reader.py:55] Path not found: size/medium_150M.gin
I0104 22:21:03.296396 140379616616448 resource_reader.py:50] system_path_file_exists:options/positions_t5.gin
E0104 22:21:03.296910 140379616616448 resource_reader.py:55] Path not found: options/positions_t5.gin
I0104 22:21:03.298407 140379616616448 resource_reader.py:50] system_path_file_exists:options/lr_cosine_decay.gin
E0104 22:21:03.298766 140379616616448 resource_reader.py:55] Path not found: options/lr_cosine_decay.gin
I0104 22:21:03.315902 140379616616448 resource_reader.py:50] system_path_file_exists:options/seq_1024_nocache.gin
E0104 22:21:03.316347 140379616616448 resource_reader.py:55] Path not found: options/seq_1024_nocache.gin
I0104 22:21:03.319318 140379616616448 resource_reader.py:50] system_path_file_exists:geometry_150M_generate.gin
E0104 22:21:03.319772 140379616616448 resource_reader.py:55] Path not found: geometry_150M_generate.gin
I0104 22:21:03.319944 140379616616448 resource_reader.py:50] system_path_file_exists:/home/peng/aglib/meliad/transformer/configs/geometry_150M_generate.gin
E0104 22:21:03.320073 140379616616448 resource_reader.py:55] Path not found: /home/peng/aglib/meliad/transformer/configs/geometry_150M_generate.gin
I0104 22:21:03.325539 140379616616448 resource_reader.py:50] system_path_file_exists:base_htrans.gin
E0104 22:21:03.325975 140379616616448 resource_reader.py:55] Path not found: base_htrans.gin
I0104 22:21:03.326529 140379616616448 resource_reader.py:50] system_path_file_exists:trainer_configuration.gin
E0104 22:21:03.326823 140379616616448 resource_reader.py:55] Path not found: trainer_configuration.gin
I0104 22:21:03.337360 140379616616448 resource_reader.py:50] system_path_file_exists:size/medium_150M.gin
E0104 22:21:03.337844 140379616616448 resource_reader.py:55] Path not found: size/medium_150M.gin
I0104 22:21:03.338548 140379616616448 resource_reader.py:50] system_path_file_exists:options/positions_t5.gin
E0104 22:21:03.338961 140379616616448 resource_reader.py:55] Path not found: options/positions_t5.gin
I0104 22:21:03.339585 140379616616448 resource_reader.py:50] system_path_file_exists:options/lr_cosine_decay.gin
E0104 22:21:03.339902 140379616616448 resource_reader.py:55] Path not found: options/lr_cosine_decay.gin
I0104 22:21:03.340735 140379616616448 resource_reader.py:50] system_path_file_exists:options/seq_1024_nocache.gin
E0104 22:21:03.341053 140379616616448 resource_reader.py:55] Path not found: options/seq_1024_nocache.gin
I0104 22:21:03.341852 140379616616448 resource_reader.py:50] system_path_file_exists:geometry_150M_generate.gin
E0104 22:21:03.342162 140379616616448 resource_reader.py:55] Path not found: geometry_150M_generate.gin
I0104 22:21:03.342311 140379616616448 resource_reader.py:50] system_path_file_exists:/home/peng/aglib/meliad/transformer/configs/geometry_150M_generate.gin
E0104 22:21:03.342432 140379616616448 resource_reader.py:55] Path not found: /home/peng/aglib/meliad/transformer/configs/geometry_150M_generate.gin
I0104 22:21:03.348567 140379616616448 training_loop.py:334] ==== Training loop: initializing model ====
I0104 22:21:03.370223 140379616616448 xla_bridge.py:166] Remote TPU is not linked into jax; skipping remote TPU.
I0104 22:21:03.370817 140379616616448 xla_bridge.py:413] Unable to initialize backend 'tpu_driver': Could not initialize backend 'tpu_driver'
I0104 22:21:03.371213 140379616616448 xla_bridge.py:413] Unable to initialize backend 'cuda': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig'
I0104 22:21:03.371419 140379616616448 xla_bridge.py:413] Unable to initialize backend 'rocm': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig'
I0104 22:21:03.382731 140379616616448 xla_bridge.py:413] Unable to initialize backend 'tpu': INVALID_ARGUMENT: TpuPlatform is not available.
I0104 22:21:03.383449 140379616616448 xla_bridge.py:413] Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.
W0104 22:21:03.383751 140379616616448 xla_bridge.py:420] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
I0104 22:21:03.383997 140379616616448 training_loop.py:335] Process 0 of 1
I0104 22:21:03.384178 140379616616448 training_loop.py:336] Local device count = 1
I0104 22:21:03.384360 140379616616448 training_loop.py:337] Number of replicas = 1
I0104 22:21:03.384507 140379616616448 training_loop.py:339] Using random number seed 42
I0104 22:21:04.047214 140379616616448 training_loop.py:359] Initializing the model.
I0104 22:21:04.275544 140379616616448 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:21:04.276214 140379616616448 decoder_stack.py:316] dstack: scanning over 1 windows.
I0104 22:21:04.276455 140379616616448 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0104 22:21:04.276602 140379616616448 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0104 22:21:04.276757 140379616616448 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0104 22:21:04.276883 140379616616448 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0104 22:21:04.277023 140379616616448 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0104 22:21:04.277148 140379616616448 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0104 22:21:04.277270 140379616616448 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0104 22:21:04.277416 140379616616448 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0104 22:21:04.277530 140379616616448 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0104 22:21:04.277643 140379616616448 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0104 22:21:04.277814 140379616616448 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0104 22:21:04.277979 140379616616448 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0104 22:21:04.278088 140379616616448 attention.py:418] Single window, no scan.
I0104 22:21:04.278200 140379616616448 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0104 22:21:04.278447 140379616616448 transformer_layer.py:154] tlayer: recurrent = False
I0104 22:21:04.278564 140379616616448 transformer_layer.py:155] tlayer: compute_importance = False
I0104 22:21:04.278661 140379616616448 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0104 22:21:04.281568 140379616616448 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:21:04.295068 140379616616448 transformer_base.py:161] kvq: pre_attn dropout.
I0104 22:21:04.324253 140379616616448 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:21:04.325264 140379616616448 transformer_base.py:194] kvq: normalize keys, queries.
I0104 22:21:04.335605 140379616616448 transformer_layer.py:213] tlayer: windowed attention.
I0104 22:21:04.400518 140379616616448 transformer_layer.py:299] tlayer: num_windows = 1.
I0104 22:21:04.400884 140379616616448 attention.py:418] Single window, no scan.
I0104 22:21:04.401012 140379616616448 transformer_layer.py:389] tlayer: self-attention.
I0104 22:21:04.401160 140379616616448 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:21:04.401347 140379616616448 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:21:04.404435 140379616616448 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[8,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:21:04.404799 140379616616448 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:21:04.408087 140379616616448 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:21:04.415586 140379616616448 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[8,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:21:04.437388 140379616616448 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[8,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:21:04.439852 140379616616448 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:21:04.440170 140379616616448 transformer_layer.py:468] tlayer: End windows.
I0104 22:21:04.440295 140379616616448 transformer_layer.py:472] tlayer: final FFN.
I0104 22:21:04.440445 140379616616448 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:21:04.441064 140379616616448 transformer_base.py:410] tbase: post-attention MLP.
I0104 22:21:04.441819 140379616616448 nn_components.py:325] mlp: activation = None
I0104 22:21:04.442049 140379616616448 nn_components.py:329] mlp: final activation = None
I0104 22:21:04.449307 140379616616448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:21:04.449899 140379616616448 nn_components.py:261] mlp: residual
I0104 22:21:04.456476 140379616616448 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:21:04.456816 140379616616448 transformer_base.py:443] tbase: final FFN
I0104 22:21:04.457501 140379616616448 nn_components.py:320] mlp: hidden 4096, relu
I0104 22:21:04.482180 140379616616448 nn_components.py:329] mlp: final activation = None
I0104 22:21:04.522669 140379616616448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:21:04.523138 140379616616448 nn_components.py:261] mlp: residual
I0104 22:21:04.523768 140379616616448 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:21:04.524045 140379616616448 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0104 22:21:04.524352 140379616616448 transformer_layer.py:154] tlayer: recurrent = False
I0104 22:21:04.524492 140379616616448 transformer_layer.py:155] tlayer: compute_importance = False
I0104 22:21:04.524638 140379616616448 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0104 22:21:04.529861 140379616616448 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:21:04.535638 140379616616448 transformer_base.py:161] kvq: pre_attn dropout.
I0104 22:21:04.555969 140379616616448 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:21:04.557034 140379616616448 transformer_base.py:194] kvq: normalize keys, queries.
I0104 22:21:04.565812 140379616616448 transformer_layer.py:213] tlayer: windowed attention.
I0104 22:21:04.576372 140379616616448 transformer_layer.py:299] tlayer: num_windows = 1.
I0104 22:21:04.576768 140379616616448 attention.py:418] Single window, no scan.
I0104 22:21:04.576912 140379616616448 transformer_layer.py:389] tlayer: self-attention.
I0104 22:21:04.577037 140379616616448 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:21:04.577231 140379616616448 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:21:04.578190 140379616616448 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[8,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:21:04.578456 140379616616448 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:21:04.579449 140379616616448 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:21:04.581170 140379616616448 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[8,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:21:04.587584 140379616616448 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[8,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:21:04.588705 140379616616448 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:21:04.588940 140379616616448 transformer_layer.py:468] tlayer: End windows.
I0104 22:21:04.589071 140379616616448 transformer_layer.py:472] tlayer: final FFN.
I0104 22:21:04.589226 140379616616448 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:21:04.589716 140379616616448 transformer_base.py:410] tbase: post-attention MLP.
I0104 22:21:04.590302 140379616616448 nn_components.py:325] mlp: activation = None
I0104 22:21:04.590453 140379616616448 nn_components.py:329] mlp: final activation = None
I0104 22:21:04.598488 140379616616448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:21:04.598917 140379616616448 nn_components.py:261] mlp: residual
I0104 22:21:04.603538 140379616616448 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:21:04.603878 140379616616448 transformer_base.py:443] tbase: final FFN
I0104 22:21:04.604762 140379616616448 nn_components.py:320] mlp: hidden 4096, relu
I0104 22:21:04.612920 140379616616448 nn_components.py:329] mlp: final activation = None
I0104 22:21:04.620547 140379616616448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:21:04.621054 140379616616448 nn_components.py:261] mlp: residual
I0104 22:21:04.621713 140379616616448 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:21:04.621958 140379616616448 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0104 22:21:04.622258 140379616616448 transformer_layer.py:154] tlayer: recurrent = False
I0104 22:21:04.622403 140379616616448 transformer_layer.py:155] tlayer: compute_importance = False
I0104 22:21:04.622518 140379616616448 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0104 22:21:04.625987 140379616616448 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:21:04.633158 140379616616448 transformer_base.py:161] kvq: pre_attn dropout.
I0104 22:21:04.654692 140379616616448 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:21:04.656499 140379616616448 transformer_base.py:194] kvq: normalize keys, queries.
I0104 22:21:04.663105 140379616616448 transformer_layer.py:213] tlayer: windowed attention.
I0104 22:21:04.672649 140379616616448 transformer_layer.py:299] tlayer: num_windows = 1.
I0104 22:21:04.673243 140379616616448 attention.py:418] Single window, no scan.
I0104 22:21:04.673443 140379616616448 transformer_layer.py:389] tlayer: self-attention.
I0104 22:21:04.673609 140379616616448 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:21:04.673931 140379616616448 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:21:04.675282 140379616616448 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[8,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:21:04.675699 140379616616448 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:21:04.676510 140379616616448 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:21:04.678503 140379616616448 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[8,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:21:04.683329 140379616616448 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[8,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:21:04.685121 140379616616448 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:21:04.685568 140379616616448 transformer_layer.py:468] tlayer: End windows.
I0104 22:21:04.685699 140379616616448 transformer_layer.py:472] tlayer: final FFN.
I0104 22:21:04.685859 140379616616448 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:21:04.686512 140379616616448 transformer_base.py:410] tbase: post-attention MLP.
I0104 22:21:04.687554 140379616616448 nn_components.py:325] mlp: activation = None
I0104 22:21:04.687966 140379616616448 nn_components.py:329] mlp: final activation = None
I0104 22:21:04.695777 140379616616448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:21:04.696174 140379616616448 nn_components.py:261] mlp: residual
I0104 22:21:04.700995 140379616616448 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:21:04.701304 140379616616448 transformer_base.py:443] tbase: final FFN
I0104 22:21:04.701902 140379616616448 nn_components.py:320] mlp: hidden 4096, relu
I0104 22:21:04.709870 140379616616448 nn_components.py:329] mlp: final activation = None
I0104 22:21:04.717256 140379616616448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:21:04.717863 140379616616448 nn_components.py:261] mlp: residual
I0104 22:21:04.719269 140379616616448 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:21:04.719856 140379616616448 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0104 22:21:04.720367 140379616616448 transformer_layer.py:154] tlayer: recurrent = False
I0104 22:21:04.720572 140379616616448 transformer_layer.py:155] tlayer: compute_importance = False
I0104 22:21:04.720734 140379616616448 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0104 22:21:04.723743 140379616616448 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:21:04.730027 140379616616448 transformer_base.py:161] kvq: pre_attn dropout.
I0104 22:21:04.748909 140379616616448 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:21:04.750442 140379616616448 transformer_base.py:194] kvq: normalize keys, queries.
I0104 22:21:04.756889 140379616616448 transformer_layer.py:213] tlayer: windowed attention.
I0104 22:21:04.767893 140379616616448 transformer_layer.py:299] tlayer: num_windows = 1.
I0104 22:21:04.768189 140379616616448 attention.py:418] Single window, no scan.
I0104 22:21:04.768335 140379616616448 transformer_layer.py:389] tlayer: self-attention.
I0104 22:21:04.768455 140379616616448 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:21:04.768658 140379616616448 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:21:04.769297 140379616616448 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[8,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:21:04.769495 140379616616448 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:21:04.770164 140379616616448 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:21:04.772175 140379616616448 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[8,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:21:04.777847 140379616616448 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[8,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:21:04.778901 140379616616448 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:21:04.779186 140379616616448 transformer_layer.py:468] tlayer: End windows.
I0104 22:21:04.779304 140379616616448 transformer_layer.py:472] tlayer: final FFN.
I0104 22:21:04.779451 140379616616448 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:21:04.779906 140379616616448 transformer_base.py:410] tbase: post-attention MLP.
I0104 22:21:04.780543 140379616616448 nn_components.py:325] mlp: activation = None
I0104 22:21:04.780787 140379616616448 nn_components.py:329] mlp: final activation = None
I0104 22:21:04.788552 140379616616448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:21:04.788938 140379616616448 nn_components.py:261] mlp: residual
I0104 22:21:04.795513 140379616616448 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:21:04.795868 140379616616448 transformer_base.py:443] tbase: final FFN
I0104 22:21:04.796483 140379616616448 nn_components.py:320] mlp: hidden 4096, relu
I0104 22:21:04.804101 140379616616448 nn_components.py:329] mlp: final activation = None
I0104 22:21:04.811752 140379616616448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:21:04.812164 140379616616448 nn_components.py:261] mlp: residual
I0104 22:21:04.812841 140379616616448 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:21:04.813113 140379616616448 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0104 22:21:04.813395 140379616616448 transformer_layer.py:154] tlayer: recurrent = False
I0104 22:21:04.813543 140379616616448 transformer_layer.py:155] tlayer: compute_importance = False
I0104 22:21:04.813682 140379616616448 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0104 22:21:04.817653 140379616616448 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:21:04.823503 140379616616448 transformer_base.py:161] kvq: pre_attn dropout.
I0104 22:21:04.840612 140379616616448 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:21:04.842063 140379616616448 transformer_base.py:194] kvq: normalize keys, queries.
I0104 22:21:04.847475 140379616616448 transformer_layer.py:213] tlayer: windowed attention.
I0104 22:21:04.857447 140379616616448 transformer_layer.py:299] tlayer: num_windows = 1.
I0104 22:21:04.857872 140379616616448 attention.py:418] Single window, no scan.
I0104 22:21:04.858046 140379616616448 transformer_layer.py:389] tlayer: self-attention.
I0104 22:21:04.858200 140379616616448 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:21:04.858530 140379616616448 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:21:04.860355 140379616616448 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[8,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:21:04.861196 140379616616448 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:21:04.862092 140379616616448 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:21:04.863632 140379616616448 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[8,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:21:04.867372 140379616616448 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[8,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:21:04.868104 140379616616448 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:21:04.868279 140379616616448 transformer_layer.py:468] tlayer: End windows.
I0104 22:21:04.868368 140379616616448 transformer_layer.py:472] tlayer: final FFN.
I0104 22:21:04.868482 140379616616448 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:21:04.868813 140379616616448 transformer_base.py:410] tbase: post-attention MLP.
I0104 22:21:04.869262 140379616616448 nn_components.py:325] mlp: activation = None
I0104 22:21:04.869375 140379616616448 nn_components.py:329] mlp: final activation = None
I0104 22:21:04.877290 140379616616448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:21:04.877665 140379616616448 nn_components.py:261] mlp: residual
I0104 22:21:04.883514 140379616616448 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:21:04.884106 140379616616448 transformer_base.py:443] tbase: final FFN
I0104 22:21:04.885155 140379616616448 nn_components.py:320] mlp: hidden 4096, relu
I0104 22:21:04.892974 140379616616448 nn_components.py:329] mlp: final activation = None
I0104 22:21:04.898755 140379616616448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:21:04.899081 140379616616448 nn_components.py:261] mlp: residual
I0104 22:21:04.899518 140379616616448 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:21:04.899698 140379616616448 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0104 22:21:04.899966 140379616616448 transformer_layer.py:154] tlayer: recurrent = False
I0104 22:21:04.900076 140379616616448 transformer_layer.py:155] tlayer: compute_importance = False
I0104 22:21:04.900160 140379616616448 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0104 22:21:04.902215 140379616616448 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:21:04.906675 140379616616448 transformer_base.py:161] kvq: pre_attn dropout.
I0104 22:21:04.928389 140379616616448 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:21:04.929336 140379616616448 transformer_base.py:194] kvq: normalize keys, queries.
I0104 22:21:04.932955 140379616616448 transformer_layer.py:213] tlayer: windowed attention.
I0104 22:21:04.942041 140379616616448 transformer_layer.py:299] tlayer: num_windows = 1.
I0104 22:21:04.942406 140379616616448 attention.py:418] Single window, no scan.
I0104 22:21:04.942542 140379616616448 transformer_layer.py:389] tlayer: self-attention.
I0104 22:21:04.942725 140379616616448 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:21:04.943007 140379616616448 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:21:04.943914 140379616616448 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[8,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:21:04.944159 140379616616448 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:21:04.944950 140379616616448 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:21:04.946696 140379616616448 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[8,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:21:04.952485 140379616616448 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[8,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:21:04.953481 140379616616448 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:21:04.953693 140379616616448 transformer_layer.py:468] tlayer: End windows.
I0104 22:21:04.953819 140379616616448 transformer_layer.py:472] tlayer: final FFN.
I0104 22:21:04.953981 140379616616448 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:21:04.954464 140379616616448 transformer_base.py:410] tbase: post-attention MLP.
I0104 22:21:04.955419 140379616616448 nn_components.py:325] mlp: activation = None
I0104 22:21:04.955658 140379616616448 nn_components.py:329] mlp: final activation = None
I0104 22:21:04.963585 140379616616448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:21:04.963909 140379616616448 nn_components.py:261] mlp: residual
I0104 22:21:04.969323 140379616616448 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:21:04.969728 140379616616448 transformer_base.py:443] tbase: final FFN
I0104 22:21:04.970417 140379616616448 nn_components.py:320] mlp: hidden 4096, relu
I0104 22:21:04.979958 140379616616448 nn_components.py:329] mlp: final activation = None
I0104 22:21:04.989371 140379616616448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:21:04.989941 140379616616448 nn_components.py:261] mlp: residual
I0104 22:21:04.990866 140379616616448 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:21:04.992779 140379616616448 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0104 22:21:04.993374 140379616616448 transformer_layer.py:154] tlayer: recurrent = False
I0104 22:21:04.993665 140379616616448 transformer_layer.py:155] tlayer: compute_importance = False
I0104 22:21:04.993817 140379616616448 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0104 22:21:04.998350 140379616616448 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:21:05.003352 140379616616448 transformer_base.py:161] kvq: pre_attn dropout.
I0104 22:21:05.028060 140379616616448 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:21:05.029183 140379616616448 transformer_base.py:194] kvq: normalize keys, queries.
I0104 22:21:05.034260 140379616616448 transformer_layer.py:213] tlayer: windowed attention.
I0104 22:21:05.045001 140379616616448 transformer_layer.py:299] tlayer: num_windows = 1.
I0104 22:21:05.045297 140379616616448 attention.py:418] Single window, no scan.
I0104 22:21:05.045439 140379616616448 transformer_layer.py:389] tlayer: self-attention.
I0104 22:21:05.045593 140379616616448 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:21:05.045801 140379616616448 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:21:05.046550 140379616616448 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[8,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:21:05.046795 140379616616448 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:21:05.047662 140379616616448 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:21:05.050126 140379616616448 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[8,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:21:05.055961 140379616616448 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[8,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:21:05.056882 140379616616448 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:21:05.057125 140379616616448 transformer_layer.py:468] tlayer: End windows.
I0104 22:21:05.057256 140379616616448 transformer_layer.py:472] tlayer: final FFN.
I0104 22:21:05.057429 140379616616448 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:21:05.058004 140379616616448 transformer_base.py:410] tbase: post-attention MLP.
I0104 22:21:05.058753 140379616616448 nn_components.py:325] mlp: activation = None
I0104 22:21:05.059066 140379616616448 nn_components.py:329] mlp: final activation = None
I0104 22:21:05.066554 140379616616448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:21:05.066901 140379616616448 nn_components.py:261] mlp: residual
I0104 22:21:05.071868 140379616616448 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:21:05.072247 140379616616448 transformer_base.py:443] tbase: final FFN
I0104 22:21:05.073476 140379616616448 nn_components.py:320] mlp: hidden 4096, relu
I0104 22:21:05.082511 140379616616448 nn_components.py:329] mlp: final activation = None
I0104 22:21:05.090399 140379616616448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:21:05.090963 140379616616448 nn_components.py:261] mlp: residual
I0104 22:21:05.092213 140379616616448 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:21:05.092728 140379616616448 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0104 22:21:05.093191 140379616616448 transformer_layer.py:154] tlayer: recurrent = False
I0104 22:21:05.093389 140379616616448 transformer_layer.py:155] tlayer: compute_importance = False
I0104 22:21:05.093523 140379616616448 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0104 22:21:05.097687 140379616616448 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:21:05.102647 140379616616448 transformer_base.py:161] kvq: pre_attn dropout.
I0104 22:21:05.125899 140379616616448 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:21:05.128729 140379616616448 transformer_base.py:194] kvq: normalize keys, queries.
I0104 22:21:05.133995 140379616616448 transformer_layer.py:213] tlayer: windowed attention.
I0104 22:21:05.144444 140379616616448 transformer_layer.py:299] tlayer: num_windows = 1.
I0104 22:21:05.144779 140379616616448 attention.py:418] Single window, no scan.
I0104 22:21:05.144924 140379616616448 transformer_layer.py:389] tlayer: self-attention.
I0104 22:21:05.145057 140379616616448 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:21:05.145272 140379616616448 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:21:05.146174 140379616616448 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[8,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:21:05.146677 140379616616448 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:21:05.148133 140379616616448 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:21:05.151942 140379616616448 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[8,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:21:05.158822 140379616616448 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[8,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:21:05.160273 140379616616448 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:21:05.161054 140379616616448 transformer_layer.py:468] tlayer: End windows.
I0104 22:21:05.161314 140379616616448 transformer_layer.py:472] tlayer: final FFN.
I0104 22:21:05.161570 140379616616448 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:21:05.162744 140379616616448 transformer_base.py:410] tbase: post-attention MLP.
I0104 22:21:05.163761 140379616616448 nn_components.py:325] mlp: activation = None
I0104 22:21:05.163965 140379616616448 nn_components.py:329] mlp: final activation = None
I0104 22:21:05.169926 140379616616448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:21:05.170264 140379616616448 nn_components.py:261] mlp: residual
I0104 22:21:05.176651 140379616616448 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:21:05.177015 140379616616448 transformer_base.py:443] tbase: final FFN
I0104 22:21:05.177689 140379616616448 nn_components.py:320] mlp: hidden 4096, relu
I0104 22:21:05.186123 140379616616448 nn_components.py:329] mlp: final activation = None
I0104 22:21:05.194194 140379616616448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:21:05.195152 140379616616448 nn_components.py:261] mlp: residual
I0104 22:21:05.196314 140379616616448 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:21:05.196603 140379616616448 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0104 22:21:05.196916 140379616616448 transformer_layer.py:154] tlayer: recurrent = False
I0104 22:21:05.197051 140379616616448 transformer_layer.py:155] tlayer: compute_importance = False
I0104 22:21:05.197157 140379616616448 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0104 22:21:05.199774 140379616616448 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:21:05.205099 140379616616448 transformer_base.py:161] kvq: pre_attn dropout.
I0104 22:21:05.230139 140379616616448 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:21:05.231405 140379616616448 transformer_base.py:194] kvq: normalize keys, queries.
I0104 22:21:05.235674 140379616616448 transformer_layer.py:213] tlayer: windowed attention.
I0104 22:21:05.246012 140379616616448 transformer_layer.py:299] tlayer: num_windows = 1.
I0104 22:21:05.246368 140379616616448 attention.py:418] Single window, no scan.
I0104 22:21:05.246580 140379616616448 transformer_layer.py:389] tlayer: self-attention.
I0104 22:21:05.246766 140379616616448 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:21:05.247220 140379616616448 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:21:05.248072 140379616616448 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[8,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:21:05.248249 140379616616448 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:21:05.249155 140379616616448 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:21:05.253185 140379616616448 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[8,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:21:05.257770 140379616616448 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[8,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:21:05.259427 140379616616448 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:21:05.260188 140379616616448 transformer_layer.py:468] tlayer: End windows.
I0104 22:21:05.260412 140379616616448 transformer_layer.py:472] tlayer: final FFN.
I0104 22:21:05.260649 140379616616448 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:21:05.261585 140379616616448 transformer_base.py:410] tbase: post-attention MLP.
I0104 22:21:05.262203 140379616616448 nn_components.py:325] mlp: activation = None
I0104 22:21:05.262348 140379616616448 nn_components.py:329] mlp: final activation = None
I0104 22:21:05.268279 140379616616448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:21:05.268684 140379616616448 nn_components.py:261] mlp: residual
I0104 22:21:05.274457 140379616616448 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:21:05.275081 140379616616448 transformer_base.py:443] tbase: final FFN
I0104 22:21:05.276207 140379616616448 nn_components.py:320] mlp: hidden 4096, relu
I0104 22:21:05.284180 140379616616448 nn_components.py:329] mlp: final activation = None
I0104 22:21:05.292161 140379616616448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:21:05.292707 140379616616448 nn_components.py:261] mlp: residual
I0104 22:21:05.293740 140379616616448 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:21:05.294481 140379616616448 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0104 22:21:05.295271 140379616616448 transformer_layer.py:154] tlayer: recurrent = False
I0104 22:21:05.295739 140379616616448 transformer_layer.py:155] tlayer: compute_importance = False
I0104 22:21:05.295995 140379616616448 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0104 22:21:05.298983 140379616616448 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:21:05.303330 140379616616448 transformer_base.py:161] kvq: pre_attn dropout.
I0104 22:21:05.327584 140379616616448 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:21:05.328852 140379616616448 transformer_base.py:194] kvq: normalize keys, queries.
I0104 22:21:05.333207 140379616616448 transformer_layer.py:213] tlayer: windowed attention.
I0104 22:21:05.342977 140379616616448 transformer_layer.py:299] tlayer: num_windows = 1.
I0104 22:21:05.343271 140379616616448 attention.py:418] Single window, no scan.
I0104 22:21:05.343441 140379616616448 transformer_layer.py:389] tlayer: self-attention.
I0104 22:21:05.343568 140379616616448 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:21:05.343773 140379616616448 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:21:05.344534 140379616616448 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[8,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:21:05.344761 140379616616448 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:21:05.345542 140379616616448 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:21:05.348726 140379616616448 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[8,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:21:05.358322 140379616616448 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[8,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:21:05.359735 140379616616448 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:21:05.360214 140379616616448 transformer_layer.py:468] tlayer: End windows.
I0104 22:21:05.360446 140379616616448 transformer_layer.py:472] tlayer: final FFN.
I0104 22:21:05.360633 140379616616448 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:21:05.361318 140379616616448 transformer_base.py:410] tbase: post-attention MLP.
I0104 22:21:05.362214 140379616616448 nn_components.py:325] mlp: activation = None
I0104 22:21:05.362580 140379616616448 nn_components.py:329] mlp: final activation = None
I0104 22:21:05.371551 140379616616448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:21:05.373070 140379616616448 nn_components.py:261] mlp: residual
I0104 22:21:05.381072 140379616616448 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:21:05.381438 140379616616448 transformer_base.py:443] tbase: final FFN
I0104 22:21:05.382097 140379616616448 nn_components.py:320] mlp: hidden 4096, relu
I0104 22:21:05.391877 140379616616448 nn_components.py:329] mlp: final activation = None
I0104 22:21:05.401744 140379616616448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:21:05.402234 140379616616448 nn_components.py:261] mlp: residual
I0104 22:21:05.403226 140379616616448 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:21:05.403599 140379616616448 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0104 22:21:05.403975 140379616616448 transformer_layer.py:154] tlayer: recurrent = False
I0104 22:21:05.404267 140379616616448 transformer_layer.py:155] tlayer: compute_importance = False
I0104 22:21:05.406544 140379616616448 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0104 22:21:05.411801 140379616616448 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:21:05.419754 140379616616448 transformer_base.py:161] kvq: pre_attn dropout.
I0104 22:21:05.448290 140379616616448 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:21:05.449402 140379616616448 transformer_base.py:194] kvq: normalize keys, queries.
I0104 22:21:05.458076 140379616616448 transformer_layer.py:213] tlayer: windowed attention.
I0104 22:21:05.472853 140379616616448 transformer_layer.py:299] tlayer: num_windows = 1.
I0104 22:21:05.473570 140379616616448 attention.py:418] Single window, no scan.
I0104 22:21:05.473903 140379616616448 transformer_layer.py:389] tlayer: self-attention.
I0104 22:21:05.474106 140379616616448 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:21:05.474531 140379616616448 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:21:05.476078 140379616616448 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[8,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:21:05.476661 140379616616448 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:21:05.477874 140379616616448 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:21:05.480166 140379616616448 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[8,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:21:05.487553 140379616616448 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[8,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:21:05.489669 140379616616448 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:21:05.490446 140379616616448 transformer_layer.py:468] tlayer: End windows.
I0104 22:21:05.490711 140379616616448 transformer_layer.py:472] tlayer: final FFN.
I0104 22:21:05.491043 140379616616448 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:21:05.491846 140379616616448 transformer_base.py:410] tbase: post-attention MLP.
I0104 22:21:05.492874 140379616616448 nn_components.py:325] mlp: activation = None
I0104 22:21:05.493213 140379616616448 nn_components.py:329] mlp: final activation = None
I0104 22:21:05.501842 140379616616448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:21:05.502307 140379616616448 nn_components.py:261] mlp: residual
I0104 22:21:05.511390 140379616616448 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:21:05.511809 140379616616448 transformer_base.py:443] tbase: final FFN
I0104 22:21:05.512541 140379616616448 nn_components.py:320] mlp: hidden 4096, relu
I0104 22:21:05.521693 140379616616448 nn_components.py:329] mlp: final activation = None
I0104 22:21:05.532101 140379616616448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:21:05.532761 140379616616448 nn_components.py:261] mlp: residual
I0104 22:21:05.533693 140379616616448 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:21:05.534034 140379616616448 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0104 22:21:05.534402 140379616616448 transformer_layer.py:154] tlayer: recurrent = False
I0104 22:21:05.534577 140379616616448 transformer_layer.py:155] tlayer: compute_importance = False
I0104 22:21:05.534711 140379616616448 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0104 22:21:05.538254 140379616616448 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:21:05.545928 140379616616448 transformer_base.py:161] kvq: pre_attn dropout.
I0104 22:21:05.569443 140379616616448 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:21:05.570467 140379616616448 transformer_base.py:194] kvq: normalize keys, queries.
I0104 22:21:05.578148 140379616616448 transformer_layer.py:213] tlayer: windowed attention.
I0104 22:21:05.591227 140379616616448 transformer_layer.py:299] tlayer: num_windows = 1.
I0104 22:21:05.591689 140379616616448 attention.py:418] Single window, no scan.
I0104 22:21:05.591978 140379616616448 transformer_layer.py:389] tlayer: self-attention.
I0104 22:21:05.592157 140379616616448 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:21:05.592424 140379616616448 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:21:05.593877 140379616616448 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[8,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:21:05.594500 140379616616448 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:21:05.596258 140379616616448 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:21:05.598885 140379616616448 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[8,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:21:05.603470 140379616616448 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[8,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:21:05.604658 140379616616448 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:21:05.605701 140379616616448 transformer_layer.py:468] tlayer: End windows.
I0104 22:21:05.606372 140379616616448 transformer_layer.py:472] tlayer: final FFN.
I0104 22:21:05.606831 140379616616448 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:21:05.607897 140379616616448 transformer_base.py:410] tbase: post-attention MLP.
I0104 22:21:05.609256 140379616616448 nn_components.py:325] mlp: activation = None
I0104 22:21:05.609549 140379616616448 nn_components.py:329] mlp: final activation = None
I0104 22:21:05.616519 140379616616448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:21:05.616957 140379616616448 nn_components.py:261] mlp: residual
I0104 22:21:05.623049 140379616616448 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:21:05.623397 140379616616448 transformer_base.py:443] tbase: final FFN
I0104 22:21:05.624014 140379616616448 nn_components.py:320] mlp: hidden 4096, relu
I0104 22:21:05.632400 140379616616448 nn_components.py:329] mlp: final activation = None
I0104 22:21:05.638347 140379616616448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:21:05.638864 140379616616448 nn_components.py:261] mlp: residual
I0104 22:21:05.640165 140379616616448 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:21:05.641601 140379616616448 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0104 22:21:05.642151 140379616616448 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0104 22:21:05.642403 140379616616448 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0104 22:21:05.642654 140379616616448 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0104 22:21:05.642838 140379616616448 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0104 22:21:05.643126 140379616616448 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0104 22:21:05.643262 140379616616448 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0104 22:21:05.643380 140379616616448 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0104 22:21:05.643493 140379616616448 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0104 22:21:05.643602 140379616616448 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0104 22:21:05.643707 140379616616448 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0104 22:21:05.643814 140379616616448 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0104 22:21:05.643925 140379616616448 decoder_stack.py:344] dstack: Final layernorm.
I0104 22:21:05.651932 140379616616448 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[8,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:21:05.785345 140379616616448 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:21:05.785806 140379616616448 decoder_stack.py:333] dstack: autoregressive generator.
I0104 22:21:05.786042 140379616616448 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0104 22:21:05.786371 140379616616448 transformer_layer.py:154] tlayer: recurrent = False
I0104 22:21:05.786536 140379616616448 transformer_layer.py:155] tlayer: compute_importance = False
I0104 22:21:05.786648 140379616616448 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0104 22:21:05.786818 140379616616448 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:21:05.791019 140379616616448 transformer_base.py:161] kvq: pre_attn dropout.
I0104 22:21:05.808479 140379616616448 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:21:05.809770 140379616616448 transformer_base.py:194] kvq: normalize keys, queries.
I0104 22:21:05.815683 140379616616448 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0104 22:21:05.849761 140379616616448 transformer_layer.py:299] tlayer: num_windows = 1.
I0104 22:21:05.850053 140379616616448 attention.py:418] Single window, no scan.
I0104 22:21:05.850197 140379616616448 transformer_layer.py:389] tlayer: self-attention.
I0104 22:21:05.850322 140379616616448 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[8,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:21:05.850534 140379616616448 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:21:05.852495 140379616616448 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:21:05.852787 140379616616448 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:21:05.854475 140379616616448 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:21:05.859099 140379616616448 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:21:05.872179 140379616616448 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:21:05.877111 140379616616448 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:21:05.877735 140379616616448 transformer_layer.py:468] tlayer: End windows.
I0104 22:21:05.877944 140379616616448 transformer_layer.py:472] tlayer: final FFN.
I0104 22:21:05.878199 140379616616448 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:21:05.879469 140379616616448 transformer_base.py:410] tbase: post-attention MLP.
I0104 22:21:05.880433 140379616616448 nn_components.py:325] mlp: activation = None
I0104 22:21:05.880851 140379616616448 nn_components.py:329] mlp: final activation = None
I0104 22:21:05.890721 140379616616448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:21:05.891691 140379616616448 nn_components.py:261] mlp: residual
I0104 22:21:05.897610 140379616616448 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:21:05.898283 140379616616448 transformer_base.py:443] tbase: final FFN
I0104 22:21:05.899130 140379616616448 nn_components.py:320] mlp: hidden 4096, relu
I0104 22:21:05.910472 140379616616448 nn_components.py:329] mlp: final activation = None
I0104 22:21:05.920997 140379616616448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:21:05.921373 140379616616448 nn_components.py:261] mlp: residual
I0104 22:21:05.922517 140379616616448 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:21:05.923443 140379616616448 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0104 22:21:05.923958 140379616616448 transformer_layer.py:154] tlayer: recurrent = False
I0104 22:21:05.924369 140379616616448 transformer_layer.py:155] tlayer: compute_importance = False
I0104 22:21:05.924524 140379616616448 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0104 22:21:05.924707 140379616616448 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:21:05.929749 140379616616448 transformer_base.py:161] kvq: pre_attn dropout.
I0104 22:21:05.953683 140379616616448 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:21:05.954736 140379616616448 transformer_base.py:194] kvq: normalize keys, queries.
I0104 22:21:05.961215 140379616616448 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0104 22:21:05.989357 140379616616448 transformer_layer.py:299] tlayer: num_windows = 1.
I0104 22:21:05.989949 140379616616448 attention.py:418] Single window, no scan.
I0104 22:21:05.990293 140379616616448 transformer_layer.py:389] tlayer: self-attention.
I0104 22:21:05.990508 140379616616448 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[8,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:21:05.990994 140379616616448 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:21:05.992427 140379616616448 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:21:05.993144 140379616616448 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:21:05.994131 140379616616448 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:21:05.995776 140379616616448 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:21:06.001102 140379616616448 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:21:06.002911 140379616616448 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:21:06.003654 140379616616448 transformer_layer.py:468] tlayer: End windows.
I0104 22:21:06.003914 140379616616448 transformer_layer.py:472] tlayer: final FFN.
I0104 22:21:06.004198 140379616616448 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:21:06.005380 140379616616448 transformer_base.py:410] tbase: post-attention MLP.
I0104 22:21:06.006297 140379616616448 nn_components.py:325] mlp: activation = None
I0104 22:21:06.006712 140379616616448 nn_components.py:329] mlp: final activation = None
I0104 22:21:06.016341 140379616616448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:21:06.016708 140379616616448 nn_components.py:261] mlp: residual
I0104 22:21:06.021301 140379616616448 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:21:06.021929 140379616616448 transformer_base.py:443] tbase: final FFN
I0104 22:21:06.022674 140379616616448 nn_components.py:320] mlp: hidden 4096, relu
I0104 22:21:06.031743 140379616616448 nn_components.py:329] mlp: final activation = None
I0104 22:21:06.041939 140379616616448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:21:06.042731 140379616616448 nn_components.py:261] mlp: residual
I0104 22:21:06.044012 140379616616448 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:21:06.044847 140379616616448 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0104 22:21:06.045626 140379616616448 transformer_layer.py:154] tlayer: recurrent = False
I0104 22:21:06.046087 140379616616448 transformer_layer.py:155] tlayer: compute_importance = False
I0104 22:21:06.046253 140379616616448 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0104 22:21:06.046460 140379616616448 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:21:06.050611 140379616616448 transformer_base.py:161] kvq: pre_attn dropout.
I0104 22:21:06.073398 140379616616448 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:21:06.074843 140379616616448 transformer_base.py:194] kvq: normalize keys, queries.
I0104 22:21:06.080838 140379616616448 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0104 22:21:06.109217 140379616616448 transformer_layer.py:299] tlayer: num_windows = 1.
I0104 22:21:06.109608 140379616616448 attention.py:418] Single window, no scan.
I0104 22:21:06.109786 140379616616448 transformer_layer.py:389] tlayer: self-attention.
I0104 22:21:06.109910 140379616616448 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[8,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:21:06.110119 140379616616448 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:21:06.111642 140379616616448 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:21:06.112527 140379616616448 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:21:06.113870 140379616616448 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:21:06.115548 140379616616448 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:21:06.119843 140379616616448 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:21:06.120790 140379616616448 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:21:06.121046 140379616616448 transformer_layer.py:468] tlayer: End windows.
I0104 22:21:06.121183 140379616616448 transformer_layer.py:472] tlayer: final FFN.
I0104 22:21:06.121385 140379616616448 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:21:06.122389 140379616616448 transformer_base.py:410] tbase: post-attention MLP.
I0104 22:21:06.123300 140379616616448 nn_components.py:325] mlp: activation = None
I0104 22:21:06.123553 140379616616448 nn_components.py:329] mlp: final activation = None
I0104 22:21:06.131031 140379616616448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:21:06.131679 140379616616448 nn_components.py:261] mlp: residual
I0104 22:21:06.137748 140379616616448 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:21:06.138177 140379616616448 transformer_base.py:443] tbase: final FFN
I0104 22:21:06.138502 140379616616448 nn_components.py:320] mlp: hidden 4096, relu
I0104 22:21:06.147563 140379616616448 nn_components.py:329] mlp: final activation = None
I0104 22:21:06.153959 140379616616448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:21:06.154356 140379616616448 nn_components.py:261] mlp: residual
I0104 22:21:06.155241 140379616616448 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:21:06.155581 140379616616448 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0104 22:21:06.156124 140379616616448 transformer_layer.py:154] tlayer: recurrent = False
I0104 22:21:06.156436 140379616616448 transformer_layer.py:155] tlayer: compute_importance = False
I0104 22:21:06.156580 140379616616448 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0104 22:21:06.156841 140379616616448 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:21:06.162612 140379616616448 transformer_base.py:161] kvq: pre_attn dropout.
I0104 22:21:06.193333 140379616616448 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:21:06.194577 140379616616448 transformer_base.py:194] kvq: normalize keys, queries.
I0104 22:21:06.200328 140379616616448 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0104 22:21:06.228501 140379616616448 transformer_layer.py:299] tlayer: num_windows = 1.
I0104 22:21:06.228811 140379616616448 attention.py:418] Single window, no scan.
I0104 22:21:06.228984 140379616616448 transformer_layer.py:389] tlayer: self-attention.
I0104 22:21:06.229101 140379616616448 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[8,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:21:06.229323 140379616616448 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:21:06.230144 140379616616448 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:21:06.230373 140379616616448 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:21:06.231314 140379616616448 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:21:06.233356 140379616616448 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:21:06.239365 140379616616448 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:21:06.240274 140379616616448 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:21:06.240508 140379616616448 transformer_layer.py:468] tlayer: End windows.
I0104 22:21:06.240630 140379616616448 transformer_layer.py:472] tlayer: final FFN.
I0104 22:21:06.240780 140379616616448 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:21:06.241225 140379616616448 transformer_base.py:410] tbase: post-attention MLP.
I0104 22:21:06.241542 140379616616448 nn_components.py:325] mlp: activation = None
I0104 22:21:06.241685 140379616616448 nn_components.py:329] mlp: final activation = None
I0104 22:21:06.250244 140379616616448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:21:06.250673 140379616616448 nn_components.py:261] mlp: residual
I0104 22:21:06.255838 140379616616448 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:21:06.256579 140379616616448 transformer_base.py:443] tbase: final FFN
I0104 22:21:06.257202 140379616616448 nn_components.py:320] mlp: hidden 4096, relu
I0104 22:21:06.266028 140379616616448 nn_components.py:329] mlp: final activation = None
I0104 22:21:06.274210 140379616616448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:21:06.274615 140379616616448 nn_components.py:261] mlp: residual
I0104 22:21:06.275453 140379616616448 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:21:06.275985 140379616616448 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0104 22:21:06.276653 140379616616448 transformer_layer.py:154] tlayer: recurrent = False
I0104 22:21:06.277018 140379616616448 transformer_layer.py:155] tlayer: compute_importance = False
I0104 22:21:06.277204 140379616616448 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0104 22:21:06.277522 140379616616448 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:21:06.282457 140379616616448 transformer_base.py:161] kvq: pre_attn dropout.
I0104 22:21:06.304666 140379616616448 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:21:06.305778 140379616616448 transformer_base.py:194] kvq: normalize keys, queries.
I0104 22:21:06.312218 140379616616448 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0104 22:21:06.340209 140379616616448 transformer_layer.py:299] tlayer: num_windows = 1.
I0104 22:21:06.340559 140379616616448 attention.py:418] Single window, no scan.
I0104 22:21:06.340737 140379616616448 transformer_layer.py:389] tlayer: self-attention.
I0104 22:21:06.340867 140379616616448 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[8,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:21:06.341105 140379616616448 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:21:06.341912 140379616616448 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:21:06.342391 140379616616448 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:21:06.344201 140379616616448 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:21:06.346761 140379616616448 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:21:06.355695 140379616616448 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:21:06.358361 140379616616448 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:21:06.359336 140379616616448 transformer_layer.py:468] tlayer: End windows.
I0104 22:21:06.359639 140379616616448 transformer_layer.py:472] tlayer: final FFN.
I0104 22:21:06.359919 140379616616448 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:21:06.360763 140379616616448 transformer_base.py:410] tbase: post-attention MLP.
I0104 22:21:06.361192 140379616616448 nn_components.py:325] mlp: activation = None
I0104 22:21:06.361363 140379616616448 nn_components.py:329] mlp: final activation = None
I0104 22:21:06.370089 140379616616448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:21:06.371093 140379616616448 nn_components.py:261] mlp: residual
I0104 22:21:06.380079 140379616616448 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:21:06.380591 140379616616448 transformer_base.py:443] tbase: final FFN
I0104 22:21:06.381272 140379616616448 nn_components.py:320] mlp: hidden 4096, relu
I0104 22:21:06.394145 140379616616448 nn_components.py:329] mlp: final activation = None
I0104 22:21:06.404714 140379616616448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:21:06.405656 140379616616448 nn_components.py:261] mlp: residual
I0104 22:21:06.407278 140379616616448 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:21:06.407794 140379616616448 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0104 22:21:06.408248 140379616616448 transformer_layer.py:154] tlayer: recurrent = False
I0104 22:21:06.408487 140379616616448 transformer_layer.py:155] tlayer: compute_importance = False
I0104 22:21:06.408851 140379616616448 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0104 22:21:06.409209 140379616616448 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:21:06.417059 140379616616448 transformer_base.py:161] kvq: pre_attn dropout.
I0104 22:21:06.442460 140379616616448 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:21:06.444756 140379616616448 transformer_base.py:194] kvq: normalize keys, queries.
I0104 22:21:06.450630 140379616616448 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0104 22:21:06.480455 140379616616448 transformer_layer.py:299] tlayer: num_windows = 1.
I0104 22:21:06.480754 140379616616448 attention.py:418] Single window, no scan.
I0104 22:21:06.480900 140379616616448 transformer_layer.py:389] tlayer: self-attention.
I0104 22:21:06.481024 140379616616448 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[8,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:21:06.481210 140379616616448 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:21:06.481840 140379616616448 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:21:06.482040 140379616616448 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:21:06.482826 140379616616448 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:21:06.484189 140379616616448 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:21:06.488468 140379616616448 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:21:06.490412 140379616616448 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:21:06.491050 140379616616448 transformer_layer.py:468] tlayer: End windows.
I0104 22:21:06.491284 140379616616448 transformer_layer.py:472] tlayer: final FFN.
I0104 22:21:06.491509 140379616616448 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:21:06.492126 140379616616448 transformer_base.py:410] tbase: post-attention MLP.
I0104 22:21:06.492483 140379616616448 nn_components.py:325] mlp: activation = None
I0104 22:21:06.492648 140379616616448 nn_components.py:329] mlp: final activation = None
I0104 22:21:06.500006 140379616616448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:21:06.500852 140379616616448 nn_components.py:261] mlp: residual
I0104 22:21:06.507094 140379616616448 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:21:06.507466 140379616616448 transformer_base.py:443] tbase: final FFN
I0104 22:21:06.507751 140379616616448 nn_components.py:320] mlp: hidden 4096, relu
I0104 22:21:06.516729 140379616616448 nn_components.py:329] mlp: final activation = None
I0104 22:21:06.524945 140379616616448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:21:06.525819 140379616616448 nn_components.py:261] mlp: residual
I0104 22:21:06.526853 140379616616448 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:21:06.527389 140379616616448 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0104 22:21:06.527898 140379616616448 transformer_layer.py:154] tlayer: recurrent = False
I0104 22:21:06.528087 140379616616448 transformer_layer.py:155] tlayer: compute_importance = False
I0104 22:21:06.528229 140379616616448 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0104 22:21:06.528413 140379616616448 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:21:06.533874 140379616616448 transformer_base.py:161] kvq: pre_attn dropout.
I0104 22:21:06.555725 140379616616448 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:21:06.556945 140379616616448 transformer_base.py:194] kvq: normalize keys, queries.
I0104 22:21:06.563127 140379616616448 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0104 22:21:06.591938 140379616616448 transformer_layer.py:299] tlayer: num_windows = 1.
I0104 22:21:06.592519 140379616616448 attention.py:418] Single window, no scan.
I0104 22:21:06.592707 140379616616448 transformer_layer.py:389] tlayer: self-attention.
I0104 22:21:06.592851 140379616616448 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[8,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:21:06.593068 140379616616448 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:21:06.593932 140379616616448 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:21:06.594206 140379616616448 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:21:06.595243 140379616616448 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:21:06.596805 140379616616448 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:21:06.603077 140379616616448 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:21:06.604774 140379616616448 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:21:06.605249 140379616616448 transformer_layer.py:468] tlayer: End windows.
I0104 22:21:06.605399 140379616616448 transformer_layer.py:472] tlayer: final FFN.
I0104 22:21:06.605564 140379616616448 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:21:06.606130 140379616616448 transformer_base.py:410] tbase: post-attention MLP.
I0104 22:21:06.606587 140379616616448 nn_components.py:325] mlp: activation = None
I0104 22:21:06.606747 140379616616448 nn_components.py:329] mlp: final activation = None
I0104 22:21:06.614707 140379616616448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:21:06.615205 140379616616448 nn_components.py:261] mlp: residual
I0104 22:21:06.619224 140379616616448 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:21:06.619505 140379616616448 transformer_base.py:443] tbase: final FFN
I0104 22:21:06.619756 140379616616448 nn_components.py:320] mlp: hidden 4096, relu
I0104 22:21:06.627944 140379616616448 nn_components.py:329] mlp: final activation = None
I0104 22:21:06.636831 140379616616448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:21:06.637615 140379616616448 nn_components.py:261] mlp: residual
I0104 22:21:06.638435 140379616616448 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:21:06.638697 140379616616448 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0104 22:21:06.639091 140379616616448 transformer_layer.py:154] tlayer: recurrent = False
I0104 22:21:06.639240 140379616616448 transformer_layer.py:155] tlayer: compute_importance = False
I0104 22:21:06.639363 140379616616448 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0104 22:21:06.639516 140379616616448 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:21:06.644585 140379616616448 transformer_base.py:161] kvq: pre_attn dropout.
I0104 22:21:06.667223 140379616616448 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:21:06.668614 140379616616448 transformer_base.py:194] kvq: normalize keys, queries.
I0104 22:21:06.674572 140379616616448 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0104 22:21:06.704608 140379616616448 transformer_layer.py:299] tlayer: num_windows = 1.
I0104 22:21:06.704997 140379616616448 attention.py:418] Single window, no scan.
I0104 22:21:06.705157 140379616616448 transformer_layer.py:389] tlayer: self-attention.
I0104 22:21:06.705294 140379616616448 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[8,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:21:06.705513 140379616616448 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:21:06.706297 140379616616448 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:21:06.706511 140379616616448 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:21:06.707329 140379616616448 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:21:06.708773 140379616616448 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:21:06.714978 140379616616448 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:21:06.715919 140379616616448 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:21:06.716156 140379616616448 transformer_layer.py:468] tlayer: End windows.
I0104 22:21:06.716273 140379616616448 transformer_layer.py:472] tlayer: final FFN.
I0104 22:21:06.716418 140379616616448 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:21:06.716882 140379616616448 transformer_base.py:410] tbase: post-attention MLP.
I0104 22:21:06.717161 140379616616448 nn_components.py:325] mlp: activation = None
I0104 22:21:06.717287 140379616616448 nn_components.py:329] mlp: final activation = None
I0104 22:21:06.724591 140379616616448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:21:06.725072 140379616616448 nn_components.py:261] mlp: residual
I0104 22:21:06.729773 140379616616448 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:21:06.730117 140379616616448 transformer_base.py:443] tbase: final FFN
I0104 22:21:06.730397 140379616616448 nn_components.py:320] mlp: hidden 4096, relu
I0104 22:21:06.739461 140379616616448 nn_components.py:329] mlp: final activation = None
I0104 22:21:06.747745 140379616616448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:21:06.748101 140379616616448 nn_components.py:261] mlp: residual
I0104 22:21:06.748723 140379616616448 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:21:06.748958 140379616616448 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0104 22:21:06.749207 140379616616448 transformer_layer.py:154] tlayer: recurrent = False
I0104 22:21:06.749338 140379616616448 transformer_layer.py:155] tlayer: compute_importance = False
I0104 22:21:06.749445 140379616616448 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0104 22:21:06.749584 140379616616448 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:21:06.754192 140379616616448 transformer_base.py:161] kvq: pre_attn dropout.
I0104 22:21:06.777857 140379616616448 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:21:06.779375 140379616616448 transformer_base.py:194] kvq: normalize keys, queries.
I0104 22:21:06.784143 140379616616448 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0104 22:21:06.813684 140379616616448 transformer_layer.py:299] tlayer: num_windows = 1.
I0104 22:21:06.814004 140379616616448 attention.py:418] Single window, no scan.
I0104 22:21:06.814167 140379616616448 transformer_layer.py:389] tlayer: self-attention.
I0104 22:21:06.814321 140379616616448 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[8,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:21:06.814566 140379616616448 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:21:06.815422 140379616616448 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:21:06.815641 140379616616448 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:21:06.816409 140379616616448 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:21:06.817717 140379616616448 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:21:06.826743 140379616616448 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:21:06.827885 140379616616448 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:21:06.828168 140379616616448 transformer_layer.py:468] tlayer: End windows.
I0104 22:21:06.828303 140379616616448 transformer_layer.py:472] tlayer: final FFN.
I0104 22:21:06.828502 140379616616448 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:21:06.829015 140379616616448 transformer_base.py:410] tbase: post-attention MLP.
I0104 22:21:06.829331 140379616616448 nn_components.py:325] mlp: activation = None
I0104 22:21:06.829481 140379616616448 nn_components.py:329] mlp: final activation = None
I0104 22:21:06.837692 140379616616448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:21:06.838103 140379616616448 nn_components.py:261] mlp: residual
I0104 22:21:06.843179 140379616616448 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:21:06.843856 140379616616448 transformer_base.py:443] tbase: final FFN
I0104 22:21:06.844356 140379616616448 nn_components.py:320] mlp: hidden 4096, relu
I0104 22:21:06.852513 140379616616448 nn_components.py:329] mlp: final activation = None
I0104 22:21:06.863089 140379616616448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:21:06.863523 140379616616448 nn_components.py:261] mlp: residual
I0104 22:21:06.864360 140379616616448 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:21:06.864675 140379616616448 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0104 22:21:06.865028 140379616616448 transformer_layer.py:154] tlayer: recurrent = False
I0104 22:21:06.865284 140379616616448 transformer_layer.py:155] tlayer: compute_importance = False
I0104 22:21:06.865459 140379616616448 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0104 22:21:06.865719 140379616616448 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:21:06.872145 140379616616448 transformer_base.py:161] kvq: pre_attn dropout.
I0104 22:21:06.894206 140379616616448 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:21:06.895350 140379616616448 transformer_base.py:194] kvq: normalize keys, queries.
I0104 22:21:06.900642 140379616616448 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0104 22:21:06.928599 140379616616448 transformer_layer.py:299] tlayer: num_windows = 1.
I0104 22:21:06.928903 140379616616448 attention.py:418] Single window, no scan.
I0104 22:21:06.929055 140379616616448 transformer_layer.py:389] tlayer: self-attention.
I0104 22:21:06.929207 140379616616448 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[8,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:21:06.929421 140379616616448 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:21:06.930162 140379616616448 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:21:06.930388 140379616616448 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:21:06.931282 140379616616448 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:21:06.933169 140379616616448 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:21:06.938789 140379616616448 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:21:06.940060 140379616616448 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:21:06.940439 140379616616448 transformer_layer.py:468] tlayer: End windows.
I0104 22:21:06.940580 140379616616448 transformer_layer.py:472] tlayer: final FFN.
I0104 22:21:06.940747 140379616616448 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:21:06.941306 140379616616448 transformer_base.py:410] tbase: post-attention MLP.
I0104 22:21:06.941612 140379616616448 nn_components.py:325] mlp: activation = None
I0104 22:21:06.941753 140379616616448 nn_components.py:329] mlp: final activation = None
I0104 22:21:06.949932 140379616616448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:21:06.950271 140379616616448 nn_components.py:261] mlp: residual
I0104 22:21:06.955306 140379616616448 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:21:06.956012 140379616616448 transformer_base.py:443] tbase: final FFN
I0104 22:21:06.956654 140379616616448 nn_components.py:320] mlp: hidden 4096, relu
I0104 22:21:06.964676 140379616616448 nn_components.py:329] mlp: final activation = None
I0104 22:21:06.971533 140379616616448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:21:06.971879 140379616616448 nn_components.py:261] mlp: residual
I0104 22:21:06.972498 140379616616448 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:21:06.972743 140379616616448 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0104 22:21:06.973025 140379616616448 transformer_layer.py:154] tlayer: recurrent = False
I0104 22:21:06.973201 140379616616448 transformer_layer.py:155] tlayer: compute_importance = False
I0104 22:21:06.973332 140379616616448 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0104 22:21:06.973505 140379616616448 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:21:06.977795 140379616616448 transformer_base.py:161] kvq: pre_attn dropout.
I0104 22:21:07.002736 140379616616448 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:21:07.003807 140379616616448 transformer_base.py:194] kvq: normalize keys, queries.
I0104 22:21:07.010361 140379616616448 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0104 22:21:07.042174 140379616616448 transformer_layer.py:299] tlayer: num_windows = 1.
I0104 22:21:07.042576 140379616616448 attention.py:418] Single window, no scan.
I0104 22:21:07.042829 140379616616448 transformer_layer.py:389] tlayer: self-attention.
I0104 22:21:07.043120 140379616616448 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[8,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:21:07.043401 140379616616448 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:21:07.044229 140379616616448 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:21:07.044466 140379616616448 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:21:07.045262 140379616616448 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:21:07.046665 140379616616448 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:21:07.054369 140379616616448 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:21:07.055932 140379616616448 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:21:07.056341 140379616616448 transformer_layer.py:468] tlayer: End windows.
I0104 22:21:07.056482 140379616616448 transformer_layer.py:472] tlayer: final FFN.
I0104 22:21:07.056644 140379616616448 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:21:07.057168 140379616616448 transformer_base.py:410] tbase: post-attention MLP.
I0104 22:21:07.057474 140379616616448 nn_components.py:325] mlp: activation = None
I0104 22:21:07.057616 140379616616448 nn_components.py:329] mlp: final activation = None
I0104 22:21:07.065576 140379616616448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:21:07.065930 140379616616448 nn_components.py:261] mlp: residual
I0104 22:21:07.070182 140379616616448 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:21:07.070483 140379616616448 transformer_base.py:443] tbase: final FFN
I0104 22:21:07.070782 140379616616448 nn_components.py:320] mlp: hidden 4096, relu
I0104 22:21:07.080077 140379616616448 nn_components.py:329] mlp: final activation = None
I0104 22:21:07.088431 140379616616448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:21:07.088864 140379616616448 nn_components.py:261] mlp: residual
I0104 22:21:07.089536 140379616616448 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:21:07.089794 140379616616448 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0104 22:21:07.090093 140379616616448 transformer_layer.py:154] tlayer: recurrent = False
I0104 22:21:07.090238 140379616616448 transformer_layer.py:155] tlayer: compute_importance = False
I0104 22:21:07.090391 140379616616448 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0104 22:21:07.090548 140379616616448 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:21:07.098065 140379616616448 transformer_base.py:161] kvq: pre_attn dropout.
I0104 22:21:07.127808 140379616616448 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:21:07.129991 140379616616448 transformer_base.py:194] kvq: normalize keys, queries.
I0104 22:21:07.135652 140379616616448 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0104 22:21:07.169159 140379616616448 transformer_layer.py:299] tlayer: num_windows = 1.
I0104 22:21:07.169514 140379616616448 attention.py:418] Single window, no scan.
I0104 22:21:07.169694 140379616616448 transformer_layer.py:389] tlayer: self-attention.
I0104 22:21:07.169814 140379616616448 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[8,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:21:07.170031 140379616616448 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:21:07.170766 140379616616448 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:21:07.171023 140379616616448 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:21:07.171780 140379616616448 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:21:07.173062 140379616616448 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:21:07.177570 140379616616448 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:21:07.180395 140379616616448 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:21:07.181001 140379616616448 transformer_layer.py:468] tlayer: End windows.
I0104 22:21:07.181175 140379616616448 transformer_layer.py:472] tlayer: final FFN.
I0104 22:21:07.181439 140379616616448 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:21:07.182184 140379616616448 transformer_base.py:410] tbase: post-attention MLP.
I0104 22:21:07.182633 140379616616448 nn_components.py:325] mlp: activation = None
I0104 22:21:07.182809 140379616616448 nn_components.py:329] mlp: final activation = None
I0104 22:21:07.193366 140379616616448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:21:07.194021 140379616616448 nn_components.py:261] mlp: residual
I0104 22:21:07.200169 140379616616448 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:21:07.200611 140379616616448 transformer_base.py:443] tbase: final FFN
I0104 22:21:07.201097 140379616616448 nn_components.py:320] mlp: hidden 4096, relu
I0104 22:21:07.208925 140379616616448 nn_components.py:329] mlp: final activation = None
I0104 22:21:07.217799 140379616616448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:21:07.218205 140379616616448 nn_components.py:261] mlp: residual
I0104 22:21:07.219557 140379616616448 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:21:07.220253 140379616616448 decoder_stack.py:344] dstack: Final layernorm.
I0104 22:21:07.228130 140379616616448 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:21:07.365852 140379616616448 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:21:07.366424 140379616616448 decoder_stack.py:333] dstack: autoregressive generator.
I0104 22:21:07.366772 140379616616448 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0104 22:21:07.367228 140379616616448 transformer_layer.py:154] tlayer: recurrent = False
I0104 22:21:07.367428 140379616616448 transformer_layer.py:155] tlayer: compute_importance = False
I0104 22:21:07.367584 140379616616448 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0104 22:21:07.367832 140379616616448 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:21:07.374964 140379616616448 transformer_base.py:161] kvq: pre_attn dropout.
I0104 22:21:07.398918 140379616616448 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:21:07.399956 140379616616448 transformer_base.py:194] kvq: normalize keys, queries.
I0104 22:21:07.404564 140379616616448 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0104 22:21:07.435150 140379616616448 transformer_layer.py:299] tlayer: num_windows = 1.
I0104 22:21:07.435437 140379616616448 attention.py:418] Single window, no scan.
I0104 22:21:07.435577 140379616616448 transformer_layer.py:389] tlayer: self-attention.
I0104 22:21:07.435712 140379616616448 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[8,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:21:07.435917 140379616616448 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:21:07.436678 140379616616448 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:21:07.436896 140379616616448 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:21:07.437633 140379616616448 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:21:07.439197 140379616616448 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:21:07.443519 140379616616448 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:21:07.444408 140379616616448 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:21:07.444658 140379616616448 transformer_layer.py:468] tlayer: End windows.
I0104 22:21:07.444786 140379616616448 transformer_layer.py:472] tlayer: final FFN.
I0104 22:21:07.444941 140379616616448 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:21:07.445412 140379616616448 transformer_base.py:410] tbase: post-attention MLP.
I0104 22:21:07.445714 140379616616448 nn_components.py:325] mlp: activation = None
I0104 22:21:07.445858 140379616616448 nn_components.py:329] mlp: final activation = None
I0104 22:21:07.456897 140379616616448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:21:07.457490 140379616616448 nn_components.py:261] mlp: residual
I0104 22:21:07.470090 140379616616448 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:21:07.470747 140379616616448 transformer_base.py:443] tbase: final FFN
I0104 22:21:07.471300 140379616616448 nn_components.py:320] mlp: hidden 4096, relu
I0104 22:21:07.482507 140379616616448 nn_components.py:329] mlp: final activation = None
I0104 22:21:07.491103 140379616616448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:21:07.491660 140379616616448 nn_components.py:261] mlp: residual
I0104 22:21:07.492763 140379616616448 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:21:07.493443 140379616616448 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0104 22:21:07.493996 140379616616448 transformer_layer.py:154] tlayer: recurrent = False
I0104 22:21:07.494235 140379616616448 transformer_layer.py:155] tlayer: compute_importance = False
I0104 22:21:07.494395 140379616616448 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0104 22:21:07.494639 140379616616448 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:21:07.501087 140379616616448 transformer_base.py:161] kvq: pre_attn dropout.
I0104 22:21:07.524754 140379616616448 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:21:07.525833 140379616616448 transformer_base.py:194] kvq: normalize keys, queries.
I0104 22:21:07.531817 140379616616448 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0104 22:21:07.562984 140379616616448 transformer_layer.py:299] tlayer: num_windows = 1.
I0104 22:21:07.563735 140379616616448 attention.py:418] Single window, no scan.
I0104 22:21:07.564238 140379616616448 transformer_layer.py:389] tlayer: self-attention.
I0104 22:21:07.564496 140379616616448 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[8,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:21:07.564799 140379616616448 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:21:07.565659 140379616616448 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:21:07.565901 140379616616448 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:21:07.566662 140379616616448 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:21:07.568351 140379616616448 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:21:07.574186 140379616616448 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:21:07.575247 140379616616448 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:21:07.575511 140379616616448 transformer_layer.py:468] tlayer: End windows.
I0104 22:21:07.575661 140379616616448 transformer_layer.py:472] tlayer: final FFN.
I0104 22:21:07.575849 140379616616448 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:21:07.576366 140379616616448 transformer_base.py:410] tbase: post-attention MLP.
I0104 22:21:07.576669 140379616616448 nn_components.py:325] mlp: activation = None
I0104 22:21:07.576776 140379616616448 nn_components.py:329] mlp: final activation = None
I0104 22:21:07.584605 140379616616448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:21:07.584956 140379616616448 nn_components.py:261] mlp: residual
I0104 22:21:07.591281 140379616616448 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:21:07.591634 140379616616448 transformer_base.py:443] tbase: final FFN
I0104 22:21:07.591912 140379616616448 nn_components.py:320] mlp: hidden 4096, relu
I0104 22:21:07.601131 140379616616448 nn_components.py:329] mlp: final activation = None
I0104 22:21:07.609703 140379616616448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:21:07.610309 140379616616448 nn_components.py:261] mlp: residual
I0104 22:21:07.611103 140379616616448 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:21:07.611370 140379616616448 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0104 22:21:07.611616 140379616616448 transformer_layer.py:154] tlayer: recurrent = False
I0104 22:21:07.611718 140379616616448 transformer_layer.py:155] tlayer: compute_importance = False
I0104 22:21:07.611794 140379616616448 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0104 22:21:07.611958 140379616616448 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:21:07.617260 140379616616448 transformer_base.py:161] kvq: pre_attn dropout.
I0104 22:21:07.640583 140379616616448 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:21:07.642284 140379616616448 transformer_base.py:194] kvq: normalize keys, queries.
I0104 22:21:07.647466 140379616616448 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0104 22:21:07.676961 140379616616448 transformer_layer.py:299] tlayer: num_windows = 1.
I0104 22:21:07.677267 140379616616448 attention.py:418] Single window, no scan.
I0104 22:21:07.677403 140379616616448 transformer_layer.py:389] tlayer: self-attention.
I0104 22:21:07.677495 140379616616448 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[8,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:21:07.677701 140379616616448 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:21:07.678451 140379616616448 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:21:07.678737 140379616616448 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:21:07.679526 140379616616448 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:21:07.681760 140379616616448 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:21:07.687900 140379616616448 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:21:07.688777 140379616616448 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:21:07.689011 140379616616448 transformer_layer.py:468] tlayer: End windows.
I0104 22:21:07.689155 140379616616448 transformer_layer.py:472] tlayer: final FFN.
I0104 22:21:07.689318 140379616616448 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:21:07.689792 140379616616448 transformer_base.py:410] tbase: post-attention MLP.
I0104 22:21:07.690084 140379616616448 nn_components.py:325] mlp: activation = None
I0104 22:21:07.690219 140379616616448 nn_components.py:329] mlp: final activation = None
I0104 22:21:07.698489 140379616616448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:21:07.698847 140379616616448 nn_components.py:261] mlp: residual
I0104 22:21:07.703154 140379616616448 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:21:07.703581 140379616616448 transformer_base.py:443] tbase: final FFN
I0104 22:21:07.703878 140379616616448 nn_components.py:320] mlp: hidden 4096, relu
I0104 22:21:07.714546 140379616616448 nn_components.py:329] mlp: final activation = None
I0104 22:21:07.723795 140379616616448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:21:07.724274 140379616616448 nn_components.py:261] mlp: residual
I0104 22:21:07.724952 140379616616448 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:21:07.725228 140379616616448 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0104 22:21:07.725608 140379616616448 transformer_layer.py:154] tlayer: recurrent = False
I0104 22:21:07.725835 140379616616448 transformer_layer.py:155] tlayer: compute_importance = False
I0104 22:21:07.726047 140379616616448 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0104 22:21:07.726306 140379616616448 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:21:07.731993 140379616616448 transformer_base.py:161] kvq: pre_attn dropout.
I0104 22:21:07.756584 140379616616448 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:21:07.757646 140379616616448 transformer_base.py:194] kvq: normalize keys, queries.
I0104 22:21:07.765122 140379616616448 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0104 22:21:07.795431 140379616616448 transformer_layer.py:299] tlayer: num_windows = 1.
I0104 22:21:07.795827 140379616616448 attention.py:418] Single window, no scan.
I0104 22:21:07.796018 140379616616448 transformer_layer.py:389] tlayer: self-attention.
I0104 22:21:07.796331 140379616616448 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[8,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:21:07.796614 140379616616448 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:21:07.797420 140379616616448 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:21:07.797682 140379616616448 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:21:07.798536 140379616616448 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:21:07.799972 140379616616448 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:21:07.804465 140379616616448 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:21:07.805619 140379616616448 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:21:07.806410 140379616616448 transformer_layer.py:468] tlayer: End windows.
I0104 22:21:07.806696 140379616616448 transformer_layer.py:472] tlayer: final FFN.
I0104 22:21:07.807068 140379616616448 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:21:07.808119 140379616616448 transformer_base.py:410] tbase: post-attention MLP.
I0104 22:21:07.808802 140379616616448 nn_components.py:325] mlp: activation = None
I0104 22:21:07.808952 140379616616448 nn_components.py:329] mlp: final activation = None
I0104 22:21:07.817795 140379616616448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:21:07.818595 140379616616448 nn_components.py:261] mlp: residual
I0104 22:21:07.824783 140379616616448 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:21:07.825187 140379616616448 transformer_base.py:443] tbase: final FFN
I0104 22:21:07.825472 140379616616448 nn_components.py:320] mlp: hidden 4096, relu
I0104 22:21:07.834833 140379616616448 nn_components.py:329] mlp: final activation = None
I0104 22:21:07.843063 140379616616448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:21:07.843514 140379616616448 nn_components.py:261] mlp: residual
I0104 22:21:07.844256 140379616616448 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:21:07.844555 140379616616448 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0104 22:21:07.844841 140379616616448 transformer_layer.py:154] tlayer: recurrent = False
I0104 22:21:07.844980 140379616616448 transformer_layer.py:155] tlayer: compute_importance = False
I0104 22:21:07.845092 140379616616448 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0104 22:21:07.845235 140379616616448 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:21:07.851385 140379616616448 transformer_base.py:161] kvq: pre_attn dropout.
I0104 22:21:07.874813 140379616616448 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:21:07.876323 140379616616448 transformer_base.py:194] kvq: normalize keys, queries.
I0104 22:21:07.882429 140379616616448 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0104 22:21:07.913733 140379616616448 transformer_layer.py:299] tlayer: num_windows = 1.
I0104 22:21:07.914241 140379616616448 attention.py:418] Single window, no scan.
I0104 22:21:07.914572 140379616616448 transformer_layer.py:389] tlayer: self-attention.
I0104 22:21:07.914744 140379616616448 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[8,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:21:07.915076 140379616616448 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:21:07.916045 140379616616448 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:21:07.916486 140379616616448 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:21:07.918193 140379616616448 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:21:07.920918 140379616616448 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:21:07.925476 140379616616448 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:21:07.927385 140379616616448 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:21:07.928056 140379616616448 transformer_layer.py:468] tlayer: End windows.
I0104 22:21:07.928281 140379616616448 transformer_layer.py:472] tlayer: final FFN.
I0104 22:21:07.928517 140379616616448 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:21:07.929285 140379616616448 transformer_base.py:410] tbase: post-attention MLP.
I0104 22:21:07.929723 140379616616448 nn_components.py:325] mlp: activation = None
I0104 22:21:07.929863 140379616616448 nn_components.py:329] mlp: final activation = None
I0104 22:21:07.936804 140379616616448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:21:07.937139 140379616616448 nn_components.py:261] mlp: residual
I0104 22:21:07.945126 140379616616448 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:21:07.945616 140379616616448 transformer_base.py:443] tbase: final FFN
I0104 22:21:07.945984 140379616616448 nn_components.py:320] mlp: hidden 4096, relu
I0104 22:21:07.956762 140379616616448 nn_components.py:329] mlp: final activation = None
I0104 22:21:07.965611 140379616616448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:21:07.965984 140379616616448 nn_components.py:261] mlp: residual
I0104 22:21:07.966607 140379616616448 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:21:07.966952 140379616616448 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0104 22:21:07.967208 140379616616448 transformer_layer.py:154] tlayer: recurrent = False
I0104 22:21:07.967337 140379616616448 transformer_layer.py:155] tlayer: compute_importance = False
I0104 22:21:07.967442 140379616616448 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0104 22:21:07.967574 140379616616448 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:21:07.971681 140379616616448 transformer_base.py:161] kvq: pre_attn dropout.
I0104 22:21:07.995279 140379616616448 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:21:07.997005 140379616616448 transformer_base.py:194] kvq: normalize keys, queries.
I0104 22:21:08.002512 140379616616448 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0104 22:21:08.034361 140379616616448 transformer_layer.py:299] tlayer: num_windows = 1.
I0104 22:21:08.034774 140379616616448 attention.py:418] Single window, no scan.
I0104 22:21:08.035131 140379616616448 transformer_layer.py:389] tlayer: self-attention.
I0104 22:21:08.035313 140379616616448 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[8,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:21:08.035556 140379616616448 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:21:08.036312 140379616616448 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:21:08.036519 140379616616448 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:21:08.037365 140379616616448 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:21:08.040079 140379616616448 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:21:08.045311 140379616616448 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:21:08.046211 140379616616448 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:21:08.046574 140379616616448 transformer_layer.py:468] tlayer: End windows.
I0104 22:21:08.046939 140379616616448 transformer_layer.py:472] tlayer: final FFN.
I0104 22:21:08.047223 140379616616448 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:21:08.047942 140379616616448 transformer_base.py:410] tbase: post-attention MLP.
I0104 22:21:08.048251 140379616616448 nn_components.py:325] mlp: activation = None
I0104 22:21:08.048349 140379616616448 nn_components.py:329] mlp: final activation = None
I0104 22:21:08.056586 140379616616448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:21:08.056991 140379616616448 nn_components.py:261] mlp: residual
I0104 22:21:08.065094 140379616616448 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:21:08.065589 140379616616448 transformer_base.py:443] tbase: final FFN
I0104 22:21:08.065949 140379616616448 nn_components.py:320] mlp: hidden 4096, relu
I0104 22:21:08.076309 140379616616448 nn_components.py:329] mlp: final activation = None
I0104 22:21:08.083863 140379616616448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:21:08.084434 140379616616448 nn_components.py:261] mlp: residual
I0104 22:21:08.085591 140379616616448 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:21:08.086191 140379616616448 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0104 22:21:08.086742 140379616616448 transformer_layer.py:154] tlayer: recurrent = False
I0104 22:21:08.087080 140379616616448 transformer_layer.py:155] tlayer: compute_importance = False
I0104 22:21:08.087258 140379616616448 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0104 22:21:08.087440 140379616616448 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:21:08.092529 140379616616448 transformer_base.py:161] kvq: pre_attn dropout.
I0104 22:21:08.117252 140379616616448 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:21:08.119218 140379616616448 transformer_base.py:194] kvq: normalize keys, queries.
I0104 22:21:08.124711 140379616616448 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0104 22:21:08.158908 140379616616448 transformer_layer.py:299] tlayer: num_windows = 1.
I0104 22:21:08.159238 140379616616448 attention.py:418] Single window, no scan.
I0104 22:21:08.159416 140379616616448 transformer_layer.py:389] tlayer: self-attention.
I0104 22:21:08.159547 140379616616448 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[8,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:21:08.159792 140379616616448 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:21:08.160852 140379616616448 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:21:08.161266 140379616616448 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:21:08.163465 140379616616448 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:21:08.166788 140379616616448 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:21:08.171763 140379616616448 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:21:08.173772 140379616616448 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:21:08.174409 140379616616448 transformer_layer.py:468] tlayer: End windows.
I0104 22:21:08.174696 140379616616448 transformer_layer.py:472] tlayer: final FFN.
I0104 22:21:08.174980 140379616616448 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:21:08.175588 140379616616448 transformer_base.py:410] tbase: post-attention MLP.
I0104 22:21:08.175896 140379616616448 nn_components.py:325] mlp: activation = None
I0104 22:21:08.176039 140379616616448 nn_components.py:329] mlp: final activation = None
I0104 22:21:08.183688 140379616616448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:21:08.184359 140379616616448 nn_components.py:261] mlp: residual
I0104 22:21:08.189794 140379616616448 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:21:08.190275 140379616616448 transformer_base.py:443] tbase: final FFN
I0104 22:21:08.190797 140379616616448 nn_components.py:320] mlp: hidden 4096, relu
I0104 22:21:08.201325 140379616616448 nn_components.py:329] mlp: final activation = None
I0104 22:21:08.209604 140379616616448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:21:08.210038 140379616616448 nn_components.py:261] mlp: residual
I0104 22:21:08.210853 140379616616448 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:21:08.211192 140379616616448 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0104 22:21:08.211471 140379616616448 transformer_layer.py:154] tlayer: recurrent = False
I0104 22:21:08.211610 140379616616448 transformer_layer.py:155] tlayer: compute_importance = False
I0104 22:21:08.211723 140379616616448 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0104 22:21:08.211865 140379616616448 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:21:08.217339 140379616616448 transformer_base.py:161] kvq: pre_attn dropout.
I0104 22:21:08.240999 140379616616448 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:21:08.242536 140379616616448 transformer_base.py:194] kvq: normalize keys, queries.
I0104 22:21:08.247409 140379616616448 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0104 22:21:08.284528 140379616616448 transformer_layer.py:299] tlayer: num_windows = 1.
I0104 22:21:08.284966 140379616616448 attention.py:418] Single window, no scan.
I0104 22:21:08.285203 140379616616448 transformer_layer.py:389] tlayer: self-attention.
I0104 22:21:08.285308 140379616616448 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[8,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:21:08.285764 140379616616448 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:21:08.287454 140379616616448 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:21:08.288130 140379616616448 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:21:08.290055 140379616616448 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:21:08.292642 140379616616448 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:21:08.300943 140379616616448 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:21:08.302081 140379616616448 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:21:08.302605 140379616616448 transformer_layer.py:468] tlayer: End windows.
I0104 22:21:08.302792 140379616616448 transformer_layer.py:472] tlayer: final FFN.
I0104 22:21:08.303112 140379616616448 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:21:08.303784 140379616616448 transformer_base.py:410] tbase: post-attention MLP.
I0104 22:21:08.304271 140379616616448 nn_components.py:325] mlp: activation = None
I0104 22:21:08.304476 140379616616448 nn_components.py:329] mlp: final activation = None
I0104 22:21:08.315608 140379616616448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:21:08.316188 140379616616448 nn_components.py:261] mlp: residual
I0104 22:21:08.325633 140379616616448 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:21:08.326105 140379616616448 transformer_base.py:443] tbase: final FFN
I0104 22:21:08.326523 140379616616448 nn_components.py:320] mlp: hidden 4096, relu
I0104 22:21:08.338137 140379616616448 nn_components.py:329] mlp: final activation = None
I0104 22:21:08.349605 140379616616448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:21:08.350092 140379616616448 nn_components.py:261] mlp: residual
I0104 22:21:08.351186 140379616616448 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:21:08.351781 140379616616448 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0104 22:21:08.352462 140379616616448 transformer_layer.py:154] tlayer: recurrent = False
I0104 22:21:08.352775 140379616616448 transformer_layer.py:155] tlayer: compute_importance = False
I0104 22:21:08.352903 140379616616448 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0104 22:21:08.353069 140379616616448 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:21:08.362450 140379616616448 transformer_base.py:161] kvq: pre_attn dropout.
I0104 22:21:08.394550 140379616616448 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:21:08.396299 140379616616448 transformer_base.py:194] kvq: normalize keys, queries.
I0104 22:21:08.403511 140379616616448 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0104 22:21:08.446795 140379616616448 transformer_layer.py:299] tlayer: num_windows = 1.
I0104 22:21:08.447461 140379616616448 attention.py:418] Single window, no scan.
I0104 22:21:08.447651 140379616616448 transformer_layer.py:389] tlayer: self-attention.
I0104 22:21:08.447790 140379616616448 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[8,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:21:08.448013 140379616616448 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:21:08.448892 140379616616448 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:21:08.449164 140379616616448 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:21:08.450086 140379616616448 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:21:08.452964 140379616616448 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:21:08.461013 140379616616448 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:21:08.462735 140379616616448 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:21:08.463484 140379616616448 transformer_layer.py:468] tlayer: End windows.
I0104 22:21:08.463839 140379616616448 transformer_layer.py:472] tlayer: final FFN.
I0104 22:21:08.464152 140379616616448 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:21:08.465083 140379616616448 transformer_base.py:410] tbase: post-attention MLP.
I0104 22:21:08.465779 140379616616448 nn_components.py:325] mlp: activation = None
I0104 22:21:08.466037 140379616616448 nn_components.py:329] mlp: final activation = None
I0104 22:21:08.476937 140379616616448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:21:08.477752 140379616616448 nn_components.py:261] mlp: residual
I0104 22:21:08.483742 140379616616448 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:21:08.484127 140379616616448 transformer_base.py:443] tbase: final FFN
I0104 22:21:08.484390 140379616616448 nn_components.py:320] mlp: hidden 4096, relu
I0104 22:21:08.494151 140379616616448 nn_components.py:329] mlp: final activation = None
I0104 22:21:08.506310 140379616616448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:21:08.506704 140379616616448 nn_components.py:261] mlp: residual
I0104 22:21:08.507693 140379616616448 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:21:08.508033 140379616616448 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0104 22:21:08.508523 140379616616448 transformer_layer.py:154] tlayer: recurrent = False
I0104 22:21:08.508675 140379616616448 transformer_layer.py:155] tlayer: compute_importance = False
I0104 22:21:08.508790 140379616616448 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0104 22:21:08.508933 140379616616448 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:21:08.513923 140379616616448 transformer_base.py:161] kvq: pre_attn dropout.
I0104 22:21:08.537590 140379616616448 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:21:08.538840 140379616616448 transformer_base.py:194] kvq: normalize keys, queries.
I0104 22:21:08.546139 140379616616448 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0104 22:21:08.592907 140379616616448 transformer_layer.py:299] tlayer: num_windows = 1.
I0104 22:21:08.593358 140379616616448 attention.py:418] Single window, no scan.
I0104 22:21:08.593573 140379616616448 transformer_layer.py:389] tlayer: self-attention.
I0104 22:21:08.593783 140379616616448 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[8,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:21:08.594184 140379616616448 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:21:08.595793 140379616616448 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:21:08.596509 140379616616448 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:21:08.600986 140379616616448 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:21:08.605077 140379616616448 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:21:08.609993 140379616616448 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:21:08.610989 140379616616448 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:21:08.611265 140379616616448 transformer_layer.py:468] tlayer: End windows.
I0104 22:21:08.611392 140379616616448 transformer_layer.py:472] tlayer: final FFN.
I0104 22:21:08.611544 140379616616448 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:21:08.612067 140379616616448 transformer_base.py:410] tbase: post-attention MLP.
I0104 22:21:08.612432 140379616616448 nn_components.py:325] mlp: activation = None
I0104 22:21:08.612595 140379616616448 nn_components.py:329] mlp: final activation = None
I0104 22:21:08.621297 140379616616448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:21:08.622053 140379616616448 nn_components.py:261] mlp: residual
I0104 22:21:08.639442 140379616616448 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:21:08.639916 140379616616448 transformer_base.py:443] tbase: final FFN
I0104 22:21:08.640462 140379616616448 nn_components.py:320] mlp: hidden 4096, relu
I0104 22:21:08.656790 140379616616448 nn_components.py:329] mlp: final activation = None
I0104 22:21:08.673524 140379616616448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:21:08.674502 140379616616448 nn_components.py:261] mlp: residual
I0104 22:21:08.675602 140379616616448 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:21:08.676067 140379616616448 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0104 22:21:08.676443 140379616616448 transformer_layer.py:154] tlayer: recurrent = False
I0104 22:21:08.676595 140379616616448 transformer_layer.py:155] tlayer: compute_importance = False
I0104 22:21:08.676707 140379616616448 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0104 22:21:08.676855 140379616616448 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:21:08.681706 140379616616448 transformer_base.py:161] kvq: pre_attn dropout.
I0104 22:21:08.717408 140379616616448 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:21:08.719564 140379616616448 transformer_base.py:194] kvq: normalize keys, queries.
I0104 22:21:08.726483 140379616616448 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0104 22:21:08.776343 140379616616448 transformer_layer.py:299] tlayer: num_windows = 1.
I0104 22:21:08.776798 140379616616448 attention.py:418] Single window, no scan.
I0104 22:21:08.776997 140379616616448 transformer_layer.py:389] tlayer: self-attention.
I0104 22:21:08.777246 140379616616448 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[8,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:21:08.777783 140379616616448 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:21:08.780106 140379616616448 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:21:08.780775 140379616616448 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:21:08.782292 140379616616448 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:21:08.784744 140379616616448 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:21:08.789959 140379616616448 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:21:08.791037 140379616616448 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:21:08.791370 140379616616448 transformer_layer.py:468] tlayer: End windows.
I0104 22:21:08.791498 140379616616448 transformer_layer.py:472] tlayer: final FFN.
I0104 22:21:08.791656 140379616616448 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:21:08.792173 140379616616448 transformer_base.py:410] tbase: post-attention MLP.
I0104 22:21:08.792585 140379616616448 nn_components.py:325] mlp: activation = None
I0104 22:21:08.792803 140379616616448 nn_components.py:329] mlp: final activation = None
I0104 22:21:08.802394 140379616616448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:21:08.802989 140379616616448 nn_components.py:261] mlp: residual
I0104 22:21:08.814319 140379616616448 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:21:08.815674 140379616616448 transformer_base.py:443] tbase: final FFN
I0104 22:21:08.816686 140379616616448 nn_components.py:320] mlp: hidden 4096, relu
I0104 22:21:08.826538 140379616616448 nn_components.py:329] mlp: final activation = None
I0104 22:21:08.838949 140379616616448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:21:08.840903 140379616616448 nn_components.py:261] mlp: residual
I0104 22:21:08.841771 140379616616448 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:21:08.842055 140379616616448 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0104 22:21:08.842380 140379616616448 transformer_layer.py:154] tlayer: recurrent = False
I0104 22:21:08.842528 140379616616448 transformer_layer.py:155] tlayer: compute_importance = False
I0104 22:21:08.842645 140379616616448 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0104 22:21:08.842790 140379616616448 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:21:08.848947 140379616616448 transformer_base.py:161] kvq: pre_attn dropout.
I0104 22:21:08.871700 140379616616448 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:21:08.872740 140379616616448 transformer_base.py:194] kvq: normalize keys, queries.
I0104 22:21:08.878625 140379616616448 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0104 22:21:08.934807 140379616616448 transformer_layer.py:299] tlayer: num_windows = 1.
I0104 22:21:08.936944 140379616616448 attention.py:418] Single window, no scan.
I0104 22:21:08.937278 140379616616448 transformer_layer.py:389] tlayer: self-attention.
I0104 22:21:08.937480 140379616616448 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[8,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:21:08.937997 140379616616448 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:21:08.939702 140379616616448 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:21:08.940429 140379616616448 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:21:08.942312 140379616616448 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:21:08.945607 140379616616448 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:21:08.953843 140379616616448 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:21:08.955006 140379616616448 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:21:08.955292 140379616616448 transformer_layer.py:468] tlayer: End windows.
I0104 22:21:08.955419 140379616616448 transformer_layer.py:472] tlayer: final FFN.
I0104 22:21:08.955579 140379616616448 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:21:08.956097 140379616616448 transformer_base.py:410] tbase: post-attention MLP.
I0104 22:21:08.956584 140379616616448 nn_components.py:325] mlp: activation = None
I0104 22:21:08.956962 140379616616448 nn_components.py:329] mlp: final activation = None
I0104 22:21:08.973403 140379616616448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:21:08.974033 140379616616448 nn_components.py:261] mlp: residual
I0104 22:21:08.979278 140379616616448 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:21:08.979685 140379616616448 transformer_base.py:443] tbase: final FFN
I0104 22:21:08.979993 140379616616448 nn_components.py:320] mlp: hidden 4096, relu
I0104 22:21:08.988209 140379616616448 nn_components.py:329] mlp: final activation = None
I0104 22:21:08.995722 140379616616448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:21:08.996094 140379616616448 nn_components.py:261] mlp: residual
I0104 22:21:08.996791 140379616616448 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:21:08.997068 140379616616448 decoder_stack.py:344] dstack: Final layernorm.
I0104 22:21:09.003455 140379616616448 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:21:42.256580 140379616616448 optimizer_config.py:74] Using Flax Adafactor Optimizer. lr=1.000000, b1=0.900000
/home/peng/pythovenv/lib/python3.10/site-packages/flax/optim/base.py:49: DeprecationWarning: Use `optax` instead of `flax.optim`. Refer to the update guide https://flax.readthedocs.io/en/latest/howtos/optax_update_guide.html for detailed instructions.
  warnings.warn(
I0104 22:21:45.003262 140379616616448 training_loop.py:419] Found existing checkpoint in .
I0104 22:21:45.003439 140379616616448 training_loop.py:428] Restoring model from last checkpoint .:
I0104 22:21:45.004766 140379616616448 checkpoints.py:429] Restoring checkpoint from ./checkpoint_999999
I0104 22:22:06.342037 140379616616448 training_loop.py:447] Only restoring trainable parameters.
I0104 22:22:06.343698 140379616616448 training_loop.py:724] parameter: decoder/embed/embedding, shape (1024, 1024), size 1048576
I0104 22:22:06.344095 140379616616448 training_loop.py:724] parameter: decoder/final_layernorm/scale, shape (1024,), size 1024
I0104 22:22:06.344285 140379616616448 training_loop.py:724] parameter: decoder/transformer0/relative_positions/rel_embedding, shape (8, 32), size 256
I0104 22:22:06.344405 140379616616448 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/attention_scale, shape (8,), size 8
I0104 22:22:06.344500 140379616616448 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0104 22:22:06.344585 140379616616448 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0104 22:22:06.344668 140379616616448 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0104 22:22:06.344743 140379616616448 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0104 22:22:06.344871 140379616616448 training_loop.py:724] parameter: decoder/transformer0/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0104 22:22:06.345010 140379616616448 training_loop.py:724] parameter: decoder/transformer0/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0104 22:22:06.345166 140379616616448 training_loop.py:724] parameter: decoder/transformer0/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0104 22:22:06.345296 140379616616448 training_loop.py:724] parameter: decoder/transformer0/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0104 22:22:06.345420 140379616616448 training_loop.py:724] parameter: decoder/transformer1/relative_positions/rel_embedding, shape (8, 32), size 256
I0104 22:22:06.345509 140379616616448 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/attention_scale, shape (8,), size 8
I0104 22:22:06.345582 140379616616448 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0104 22:22:06.345650 140379616616448 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0104 22:22:06.345778 140379616616448 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0104 22:22:06.345899 140379616616448 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0104 22:22:06.346012 140379616616448 training_loop.py:724] parameter: decoder/transformer1/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0104 22:22:06.346093 140379616616448 training_loop.py:724] parameter: decoder/transformer1/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0104 22:22:06.346210 140379616616448 training_loop.py:724] parameter: decoder/transformer1/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0104 22:22:06.346340 140379616616448 training_loop.py:724] parameter: decoder/transformer1/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0104 22:22:06.346456 140379616616448 training_loop.py:724] parameter: decoder/transformer10/relative_positions/rel_embedding, shape (8, 32), size 256
I0104 22:22:06.346565 140379616616448 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/attention_scale, shape (8,), size 8
I0104 22:22:06.346646 140379616616448 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0104 22:22:06.346743 140379616616448 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0104 22:22:06.346869 140379616616448 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0104 22:22:06.347026 140379616616448 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0104 22:22:06.347139 140379616616448 training_loop.py:724] parameter: decoder/transformer10/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0104 22:22:06.347235 140379616616448 training_loop.py:724] parameter: decoder/transformer10/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0104 22:22:06.347304 140379616616448 training_loop.py:724] parameter: decoder/transformer10/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0104 22:22:06.347366 140379616616448 training_loop.py:724] parameter: decoder/transformer10/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0104 22:22:06.347425 140379616616448 training_loop.py:724] parameter: decoder/transformer11/relative_positions/rel_embedding, shape (8, 32), size 256
I0104 22:22:06.347530 140379616616448 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/attention_scale, shape (8,), size 8
I0104 22:22:06.347657 140379616616448 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0104 22:22:06.347772 140379616616448 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0104 22:22:06.347853 140379616616448 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0104 22:22:06.347920 140379616616448 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0104 22:22:06.347981 140379616616448 training_loop.py:724] parameter: decoder/transformer11/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0104 22:22:06.348041 140379616616448 training_loop.py:724] parameter: decoder/transformer11/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0104 22:22:06.348099 140379616616448 training_loop.py:724] parameter: decoder/transformer11/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0104 22:22:06.348192 140379616616448 training_loop.py:724] parameter: decoder/transformer11/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0104 22:22:06.348297 140379616616448 training_loop.py:724] parameter: decoder/transformer2/relative_positions/rel_embedding, shape (8, 32), size 256
I0104 22:22:06.348374 140379616616448 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/attention_scale, shape (8,), size 8
I0104 22:22:06.348439 140379616616448 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0104 22:22:06.348503 140379616616448 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0104 22:22:06.348564 140379616616448 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0104 22:22:06.348623 140379616616448 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0104 22:22:06.348718 140379616616448 training_loop.py:724] parameter: decoder/transformer2/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0104 22:22:06.348832 140379616616448 training_loop.py:724] parameter: decoder/transformer2/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0104 22:22:06.348941 140379616616448 training_loop.py:724] parameter: decoder/transformer2/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0104 22:22:06.349027 140379616616448 training_loop.py:724] parameter: decoder/transformer2/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0104 22:22:06.349092 140379616616448 training_loop.py:724] parameter: decoder/transformer3/relative_positions/rel_embedding, shape (8, 32), size 256
I0104 22:22:06.349230 140379616616448 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/attention_scale, shape (8,), size 8
I0104 22:22:06.349341 140379616616448 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0104 22:22:06.349422 140379616616448 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0104 22:22:06.349489 140379616616448 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0104 22:22:06.349550 140379616616448 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0104 22:22:06.349612 140379616616448 training_loop.py:724] parameter: decoder/transformer3/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0104 22:22:06.349707 140379616616448 training_loop.py:724] parameter: decoder/transformer3/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0104 22:22:06.349816 140379616616448 training_loop.py:724] parameter: decoder/transformer3/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0104 22:22:06.349896 140379616616448 training_loop.py:724] parameter: decoder/transformer3/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0104 22:22:06.349961 140379616616448 training_loop.py:724] parameter: decoder/transformer4/relative_positions/rel_embedding, shape (8, 32), size 256
I0104 22:22:06.350023 140379616616448 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/attention_scale, shape (8,), size 8
I0104 22:22:06.350121 140379616616448 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0104 22:22:06.350237 140379616616448 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0104 22:22:06.350347 140379616616448 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0104 22:22:06.350454 140379616616448 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0104 22:22:06.350558 140379616616448 training_loop.py:724] parameter: decoder/transformer4/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0104 22:22:06.350665 140379616616448 training_loop.py:724] parameter: decoder/transformer4/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0104 22:22:06.350773 140379616616448 training_loop.py:724] parameter: decoder/transformer4/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0104 22:22:06.350923 140379616616448 training_loop.py:724] parameter: decoder/transformer4/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0104 22:22:06.351042 140379616616448 training_loop.py:724] parameter: decoder/transformer5/relative_positions/rel_embedding, shape (8, 32), size 256
I0104 22:22:06.351181 140379616616448 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/attention_scale, shape (8,), size 8
I0104 22:22:06.351294 140379616616448 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0104 22:22:06.351398 140379616616448 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0104 22:22:06.351475 140379616616448 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0104 22:22:06.351537 140379616616448 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0104 22:22:06.351625 140379616616448 training_loop.py:724] parameter: decoder/transformer5/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0104 22:22:06.351726 140379616616448 training_loop.py:724] parameter: decoder/transformer5/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0104 22:22:06.351800 140379616616448 training_loop.py:724] parameter: decoder/transformer5/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0104 22:22:06.351861 140379616616448 training_loop.py:724] parameter: decoder/transformer5/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0104 22:22:06.351949 140379616616448 training_loop.py:724] parameter: decoder/transformer6/relative_positions/rel_embedding, shape (8, 32), size 256
I0104 22:22:06.352025 140379616616448 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/attention_scale, shape (8,), size 8
I0104 22:22:06.352087 140379616616448 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0104 22:22:06.352175 140379616616448 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0104 22:22:06.352278 140379616616448 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0104 22:22:06.352356 140379616616448 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0104 22:22:06.352424 140379616616448 training_loop.py:724] parameter: decoder/transformer6/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0104 22:22:06.352487 140379616616448 training_loop.py:724] parameter: decoder/transformer6/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0104 22:22:06.352548 140379616616448 training_loop.py:724] parameter: decoder/transformer6/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0104 22:22:06.352643 140379616616448 training_loop.py:724] parameter: decoder/transformer6/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0104 22:22:06.352721 140379616616448 training_loop.py:724] parameter: decoder/transformer7/relative_positions/rel_embedding, shape (8, 32), size 256
I0104 22:22:06.352785 140379616616448 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/attention_scale, shape (8,), size 8
I0104 22:22:06.352878 140379616616448 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0104 22:22:06.352948 140379616616448 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0104 22:22:06.353006 140379616616448 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0104 22:22:06.353062 140379616616448 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0104 22:22:06.353161 140379616616448 training_loop.py:724] parameter: decoder/transformer7/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0104 22:22:06.353260 140379616616448 training_loop.py:724] parameter: decoder/transformer7/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0104 22:22:06.353321 140379616616448 training_loop.py:724] parameter: decoder/transformer7/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0104 22:22:06.353376 140379616616448 training_loop.py:724] parameter: decoder/transformer7/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0104 22:22:06.353432 140379616616448 training_loop.py:724] parameter: decoder/transformer8/relative_positions/rel_embedding, shape (8, 32), size 256
I0104 22:22:06.353487 140379616616448 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/attention_scale, shape (8,), size 8
I0104 22:22:06.353542 140379616616448 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0104 22:22:06.353596 140379616616448 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0104 22:22:06.353651 140379616616448 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0104 22:22:06.353707 140379616616448 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0104 22:22:06.353764 140379616616448 training_loop.py:724] parameter: decoder/transformer8/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0104 22:22:06.353819 140379616616448 training_loop.py:724] parameter: decoder/transformer8/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0104 22:22:06.353874 140379616616448 training_loop.py:724] parameter: decoder/transformer8/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0104 22:22:06.353929 140379616616448 training_loop.py:724] parameter: decoder/transformer8/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0104 22:22:06.353986 140379616616448 training_loop.py:724] parameter: decoder/transformer9/relative_positions/rel_embedding, shape (8, 32), size 256
I0104 22:22:06.354044 140379616616448 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/attention_scale, shape (8,), size 8
I0104 22:22:06.354101 140379616616448 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0104 22:22:06.354158 140379616616448 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0104 22:22:06.354215 140379616616448 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0104 22:22:06.354272 140379616616448 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0104 22:22:06.354327 140379616616448 training_loop.py:724] parameter: decoder/transformer9/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0104 22:22:06.354386 140379616616448 training_loop.py:724] parameter: decoder/transformer9/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0104 22:22:06.354454 140379616616448 training_loop.py:724] parameter: decoder/transformer9/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0104 22:22:06.354513 140379616616448 training_loop.py:724] parameter: decoder/transformer9/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0104 22:22:06.354555 140379616616448 training_loop.py:725] Total parameters: 152072288
I0104 22:22:06.355174 140379616616448 training_loop.py:739] Total state size: 0
I0104 22:22:06.661352 140379616616448 training_loop.py:492] Training loop: creating task for mode beam_search
I0104 22:22:06.662340 140379616616448 training_loop.py:690] Creating summary writer (train) for mode beam_search in directory .
I0104 22:22:07.002789 140379616616448 training_loop.py:652] Compiling mode beam_search with jit.
I0104 22:22:07.005149 140379616616448 training_loop.py:89] registering functions: dict_keys([])
I0104 22:22:07.018932 140379616616448 alphageometry.py:683] Depth 0. There are 1 nodes to expand:
I0104 22:22:07.019260 140379616616448 alphageometry.py:687] {S} a : ; b : ; c : ; d : ^ a b a d a d a c 00 ^ c a c d c d c b 01 ; e : C a c e 02 T a c d e 03 ; f : C a b f 04 T a b d f 05 ; g : C d e g 06 D d e d g 07 ; h : C d f h 08 D d f d h 09 ; i : C b c i 10 P a c g i 11 ; j : C b c j 12 P a b h j 13 ; k : C a c k 14 D a k c k 15 ; l : C a b l 16 D a l b l 17 ; m : C a d m 18 O a b c m 19 ? ^ m j m i d l d k {F1} x00
I0104 22:22:07.019378 140379616616448 alphageometry.py:553] Worker PID=10401 called for beam search node 0
I0104 22:22:07.019443 140379616616448 alphageometry.py:556] Worker PID=10401: Beam-searching and Decoding from {S} a : ; b : ; c : ; d : ^ a b a d a d a c 00 ^ c a c d c d c b 01 ; e : C a c e 02 T a c d e 03 ; f : C a b f 04 T a b d f 05 ; g : C d e g 06 D d e d g 07 ; h : C d f h 08 D d f d h 09 ; i : C b c i 10 P a c g i 11 ; j : C b c j 12 P a b h j 13 ; k : C a c k 14 D a k c k 15 ; l : C a b l 16 D a l b l 17 ; m : C a d m 18 O a b c m 19 ? ^ m j m i d l d k {F1} x00
I0104 22:22:07.247995 140379616616448 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:22:07.248275 140379616616448 decoder_stack.py:316] dstack: scanning over 1 windows.
I0104 22:22:07.248414 140379616616448 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0104 22:22:07.248506 140379616616448 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0104 22:22:07.248581 140379616616448 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0104 22:22:07.248650 140379616616448 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0104 22:22:07.248717 140379616616448 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0104 22:22:07.248780 140379616616448 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0104 22:22:07.248842 140379616616448 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0104 22:22:07.248901 140379616616448 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0104 22:22:07.248958 140379616616448 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0104 22:22:07.249014 140379616616448 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0104 22:22:07.249071 140379616616448 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0104 22:22:07.249127 140379616616448 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0104 22:22:07.249176 140379616616448 attention.py:418] Single window, no scan.
I0104 22:22:07.249228 140379616616448 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0104 22:22:07.249411 140379616616448 transformer_layer.py:154] tlayer: recurrent = False
I0104 22:22:07.249464 140379616616448 transformer_layer.py:155] tlayer: compute_importance = False
I0104 22:22:07.249505 140379616616448 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0104 22:22:07.253565 140379616616448 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:22:07.259679 140379616616448 transformer_base.py:161] kvq: pre_attn dropout.
I0104 22:22:07.280063 140379616616448 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:22:07.280961 140379616616448 transformer_base.py:194] kvq: normalize keys, queries.
I0104 22:22:07.289716 140379616616448 transformer_layer.py:213] tlayer: windowed attention.
I0104 22:22:07.307074 140379616616448 transformer_layer.py:299] tlayer: num_windows = 1.
I0104 22:22:07.307278 140379616616448 attention.py:418] Single window, no scan.
I0104 22:22:07.307359 140379616616448 transformer_layer.py:389] tlayer: self-attention.
I0104 22:22:07.307444 140379616616448 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:22:07.307610 140379616616448 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:22:07.308309 140379616616448 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[8,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:22:07.308502 140379616616448 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:22:07.309166 140379616616448 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:22:07.310833 140379616616448 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[8,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:22:07.315302 140379616616448 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[8,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:22:07.316109 140379616616448 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:22:07.316302 140379616616448 transformer_layer.py:468] tlayer: End windows.
I0104 22:22:07.316401 140379616616448 transformer_layer.py:472] tlayer: final FFN.
I0104 22:22:07.316521 140379616616448 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:22:07.316961 140379616616448 transformer_base.py:410] tbase: post-attention MLP.
I0104 22:22:07.317514 140379616616448 nn_components.py:325] mlp: activation = None
I0104 22:22:07.317624 140379616616448 nn_components.py:329] mlp: final activation = None
I0104 22:22:07.325219 140379616616448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:22:07.325633 140379616616448 nn_components.py:261] mlp: residual
I0104 22:22:07.331256 140379616616448 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:22:07.331531 140379616616448 transformer_base.py:443] tbase: final FFN
I0104 22:22:07.332206 140379616616448 nn_components.py:320] mlp: hidden 4096, relu
I0104 22:22:07.339693 140379616616448 nn_components.py:329] mlp: final activation = None
I0104 22:22:07.347213 140379616616448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:22:07.347496 140379616616448 nn_components.py:261] mlp: residual
I0104 22:22:07.348135 140379616616448 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:22:07.348339 140379616616448 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0104 22:22:07.348580 140379616616448 transformer_layer.py:154] tlayer: recurrent = False
I0104 22:22:07.348684 140379616616448 transformer_layer.py:155] tlayer: compute_importance = False
I0104 22:22:07.348766 140379616616448 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0104 22:22:07.351479 140379616616448 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:22:07.355812 140379616616448 transformer_base.py:161] kvq: pre_attn dropout.
I0104 22:22:07.383436 140379616616448 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:22:07.385556 140379616616448 transformer_base.py:194] kvq: normalize keys, queries.
I0104 22:22:07.396320 140379616616448 transformer_layer.py:213] tlayer: windowed attention.
I0104 22:22:07.414311 140379616616448 transformer_layer.py:299] tlayer: num_windows = 1.
I0104 22:22:07.414533 140379616616448 attention.py:418] Single window, no scan.
I0104 22:22:07.414605 140379616616448 transformer_layer.py:389] tlayer: self-attention.
I0104 22:22:07.414664 140379616616448 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:22:07.414793 140379616616448 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:22:07.415507 140379616616448 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[8,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:22:07.415642 140379616616448 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:22:07.416223 140379616616448 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:22:07.417880 140379616616448 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[8,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:22:07.427973 140379616616448 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[8,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:22:07.429180 140379616616448 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:22:07.429630 140379616616448 transformer_layer.py:468] tlayer: End windows.
I0104 22:22:07.429726 140379616616448 transformer_layer.py:472] tlayer: final FFN.
I0104 22:22:07.429837 140379616616448 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:22:07.430564 140379616616448 transformer_base.py:410] tbase: post-attention MLP.
I0104 22:22:07.431662 140379616616448 nn_components.py:325] mlp: activation = None
I0104 22:22:07.431894 140379616616448 nn_components.py:329] mlp: final activation = None
I0104 22:22:07.444859 140379616616448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:22:07.445327 140379616616448 nn_components.py:261] mlp: residual
I0104 22:22:07.450758 140379616616448 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:22:07.451208 140379616616448 transformer_base.py:443] tbase: final FFN
I0104 22:22:07.452205 140379616616448 nn_components.py:320] mlp: hidden 4096, relu
I0104 22:22:07.467906 140379616616448 nn_components.py:329] mlp: final activation = None
I0104 22:22:07.477627 140379616616448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:22:07.478250 140379616616448 nn_components.py:261] mlp: residual
I0104 22:22:07.479763 140379616616448 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:22:07.480245 140379616616448 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0104 22:22:07.480590 140379616616448 transformer_layer.py:154] tlayer: recurrent = False
I0104 22:22:07.480939 140379616616448 transformer_layer.py:155] tlayer: compute_importance = False
I0104 22:22:07.481076 140379616616448 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0104 22:22:07.484863 140379616616448 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:22:07.492219 140379616616448 transformer_base.py:161] kvq: pre_attn dropout.
I0104 22:22:07.526507 140379616616448 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:22:07.527570 140379616616448 transformer_base.py:194] kvq: normalize keys, queries.
I0104 22:22:07.533546 140379616616448 transformer_layer.py:213] tlayer: windowed attention.
I0104 22:22:07.550988 140379616616448 transformer_layer.py:299] tlayer: num_windows = 1.
I0104 22:22:07.551418 140379616616448 attention.py:418] Single window, no scan.
I0104 22:22:07.551581 140379616616448 transformer_layer.py:389] tlayer: self-attention.
I0104 22:22:07.551722 140379616616448 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:22:07.552035 140379616616448 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:22:07.553640 140379616616448 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[8,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:22:07.554091 140379616616448 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:22:07.555757 140379616616448 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:22:07.558853 140379616616448 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[8,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:22:07.565150 140379616616448 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[8,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:22:07.566164 140379616616448 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:22:07.566429 140379616616448 transformer_layer.py:468] tlayer: End windows.
I0104 22:22:07.566592 140379616616448 transformer_layer.py:472] tlayer: final FFN.
I0104 22:22:07.566740 140379616616448 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:22:07.570646 140379616616448 transformer_base.py:410] tbase: post-attention MLP.
I0104 22:22:07.571871 140379616616448 nn_components.py:325] mlp: activation = None
I0104 22:22:07.572073 140379616616448 nn_components.py:329] mlp: final activation = None
I0104 22:22:07.582070 140379616616448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:22:07.582363 140379616616448 nn_components.py:261] mlp: residual
I0104 22:22:07.588366 140379616616448 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:22:07.588614 140379616616448 transformer_base.py:443] tbase: final FFN
I0104 22:22:07.589261 140379616616448 nn_components.py:320] mlp: hidden 4096, relu
I0104 22:22:07.600434 140379616616448 nn_components.py:329] mlp: final activation = None
I0104 22:22:07.609181 140379616616448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:22:07.609489 140379616616448 nn_components.py:261] mlp: residual
I0104 22:22:07.610190 140379616616448 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:22:07.610404 140379616616448 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0104 22:22:07.610646 140379616616448 transformer_layer.py:154] tlayer: recurrent = False
I0104 22:22:07.610758 140379616616448 transformer_layer.py:155] tlayer: compute_importance = False
I0104 22:22:07.610857 140379616616448 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0104 22:22:07.614973 140379616616448 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:22:07.619258 140379616616448 transformer_base.py:161] kvq: pre_attn dropout.
I0104 22:22:07.642755 140379616616448 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:22:07.643851 140379616616448 transformer_base.py:194] kvq: normalize keys, queries.
I0104 22:22:07.650892 140379616616448 transformer_layer.py:213] tlayer: windowed attention.
I0104 22:22:07.663536 140379616616448 transformer_layer.py:299] tlayer: num_windows = 1.
I0104 22:22:07.663740 140379616616448 attention.py:418] Single window, no scan.
I0104 22:22:07.663821 140379616616448 transformer_layer.py:389] tlayer: self-attention.
I0104 22:22:07.663904 140379616616448 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:22:07.664074 140379616616448 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:22:07.664994 140379616616448 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[8,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:22:07.665309 140379616616448 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:22:07.666430 140379616616448 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:22:07.669330 140379616616448 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[8,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:22:07.675301 140379616616448 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[8,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:22:07.676225 140379616616448 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:22:07.676517 140379616616448 transformer_layer.py:468] tlayer: End windows.
I0104 22:22:07.676642 140379616616448 transformer_layer.py:472] tlayer: final FFN.
I0104 22:22:07.676800 140379616616448 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:22:07.677865 140379616616448 transformer_base.py:410] tbase: post-attention MLP.
I0104 22:22:07.679404 140379616616448 nn_components.py:325] mlp: activation = None
I0104 22:22:07.679645 140379616616448 nn_components.py:329] mlp: final activation = None
I0104 22:22:07.686138 140379616616448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:22:07.686396 140379616616448 nn_components.py:261] mlp: residual
I0104 22:22:07.693584 140379616616448 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:22:07.694039 140379616616448 transformer_base.py:443] tbase: final FFN
I0104 22:22:07.694693 140379616616448 nn_components.py:320] mlp: hidden 4096, relu
I0104 22:22:07.703574 140379616616448 nn_components.py:329] mlp: final activation = None
I0104 22:22:07.717319 140379616616448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:22:07.717781 140379616616448 nn_components.py:261] mlp: residual
I0104 22:22:07.718673 140379616616448 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:22:07.719166 140379616616448 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0104 22:22:07.719478 140379616616448 transformer_layer.py:154] tlayer: recurrent = False
I0104 22:22:07.719594 140379616616448 transformer_layer.py:155] tlayer: compute_importance = False
I0104 22:22:07.719668 140379616616448 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0104 22:22:07.723379 140379616616448 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:22:07.732282 140379616616448 transformer_base.py:161] kvq: pre_attn dropout.
I0104 22:22:07.760121 140379616616448 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:22:07.761109 140379616616448 transformer_base.py:194] kvq: normalize keys, queries.
I0104 22:22:07.765696 140379616616448 transformer_layer.py:213] tlayer: windowed attention.
I0104 22:22:07.780188 140379616616448 transformer_layer.py:299] tlayer: num_windows = 1.
I0104 22:22:07.780498 140379616616448 attention.py:418] Single window, no scan.
I0104 22:22:07.780588 140379616616448 transformer_layer.py:389] tlayer: self-attention.
I0104 22:22:07.780675 140379616616448 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:22:07.780829 140379616616448 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:22:07.782163 140379616616448 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[8,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:22:07.782806 140379616616448 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:22:07.783967 140379616616448 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:22:07.785971 140379616616448 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[8,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:22:07.791966 140379616616448 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[8,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:22:07.793348 140379616616448 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:22:07.793761 140379616616448 transformer_layer.py:468] tlayer: End windows.
I0104 22:22:07.793832 140379616616448 transformer_layer.py:472] tlayer: final FFN.
I0104 22:22:07.794034 140379616616448 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:22:07.794732 140379616616448 transformer_base.py:410] tbase: post-attention MLP.
I0104 22:22:07.795496 140379616616448 nn_components.py:325] mlp: activation = None
I0104 22:22:07.795627 140379616616448 nn_components.py:329] mlp: final activation = None
I0104 22:22:07.804010 140379616616448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:22:07.804627 140379616616448 nn_components.py:261] mlp: residual
I0104 22:22:07.809995 140379616616448 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:22:07.810361 140379616616448 transformer_base.py:443] tbase: final FFN
I0104 22:22:07.811344 140379616616448 nn_components.py:320] mlp: hidden 4096, relu
I0104 22:22:07.821426 140379616616448 nn_components.py:329] mlp: final activation = None
I0104 22:22:07.830086 140379616616448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:22:07.830388 140379616616448 nn_components.py:261] mlp: residual
I0104 22:22:07.831186 140379616616448 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:22:07.831491 140379616616448 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0104 22:22:07.831826 140379616616448 transformer_layer.py:154] tlayer: recurrent = False
I0104 22:22:07.831949 140379616616448 transformer_layer.py:155] tlayer: compute_importance = False
I0104 22:22:07.832025 140379616616448 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0104 22:22:07.835775 140379616616448 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:22:07.840881 140379616616448 transformer_base.py:161] kvq: pre_attn dropout.
I0104 22:22:07.866798 140379616616448 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:22:07.868285 140379616616448 transformer_base.py:194] kvq: normalize keys, queries.
I0104 22:22:07.874435 140379616616448 transformer_layer.py:213] tlayer: windowed attention.
I0104 22:22:07.886067 140379616616448 transformer_layer.py:299] tlayer: num_windows = 1.
I0104 22:22:07.886252 140379616616448 attention.py:418] Single window, no scan.
I0104 22:22:07.886326 140379616616448 transformer_layer.py:389] tlayer: self-attention.
I0104 22:22:07.886430 140379616616448 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:22:07.886588 140379616616448 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:22:07.887408 140379616616448 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[8,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:22:07.887605 140379616616448 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:22:07.888284 140379616616448 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:22:07.890203 140379616616448 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[8,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:22:07.896102 140379616616448 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[8,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:22:07.896892 140379616616448 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:22:07.897085 140379616616448 transformer_layer.py:468] tlayer: End windows.
I0104 22:22:07.897175 140379616616448 transformer_layer.py:472] tlayer: final FFN.
I0104 22:22:07.897279 140379616616448 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:22:07.897692 140379616616448 transformer_base.py:410] tbase: post-attention MLP.
I0104 22:22:07.898348 140379616616448 nn_components.py:325] mlp: activation = None
I0104 22:22:07.898524 140379616616448 nn_components.py:329] mlp: final activation = None
I0104 22:22:07.907487 140379616616448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:22:07.907836 140379616616448 nn_components.py:261] mlp: residual
I0104 22:22:07.915163 140379616616448 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:22:07.915476 140379616616448 transformer_base.py:443] tbase: final FFN
I0104 22:22:07.916095 140379616616448 nn_components.py:320] mlp: hidden 4096, relu
I0104 22:22:07.925636 140379616616448 nn_components.py:329] mlp: final activation = None
I0104 22:22:07.935726 140379616616448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:22:07.936092 140379616616448 nn_components.py:261] mlp: residual
I0104 22:22:07.936945 140379616616448 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:22:07.937327 140379616616448 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0104 22:22:07.937923 140379616616448 transformer_layer.py:154] tlayer: recurrent = False
I0104 22:22:07.938135 140379616616448 transformer_layer.py:155] tlayer: compute_importance = False
I0104 22:22:07.938407 140379616616448 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0104 22:22:07.943663 140379616616448 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:22:07.951519 140379616616448 transformer_base.py:161] kvq: pre_attn dropout.
I0104 22:22:07.985626 140379616616448 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:22:07.986751 140379616616448 transformer_base.py:194] kvq: normalize keys, queries.
I0104 22:22:07.994146 140379616616448 transformer_layer.py:213] tlayer: windowed attention.
I0104 22:22:08.008377 140379616616448 transformer_layer.py:299] tlayer: num_windows = 1.
I0104 22:22:08.008714 140379616616448 attention.py:418] Single window, no scan.
I0104 22:22:08.008831 140379616616448 transformer_layer.py:389] tlayer: self-attention.
I0104 22:22:08.008955 140379616616448 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:22:08.009287 140379616616448 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:22:08.010993 140379616616448 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[8,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:22:08.011325 140379616616448 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:22:08.012414 140379616616448 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:22:08.014274 140379616616448 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[8,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:22:08.019860 140379616616448 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[8,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:22:08.021806 140379616616448 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:22:08.022345 140379616616448 transformer_layer.py:468] tlayer: End windows.
I0104 22:22:08.022454 140379616616448 transformer_layer.py:472] tlayer: final FFN.
I0104 22:22:08.022612 140379616616448 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:22:08.023516 140379616616448 transformer_base.py:410] tbase: post-attention MLP.
I0104 22:22:08.024335 140379616616448 nn_components.py:325] mlp: activation = None
I0104 22:22:08.024460 140379616616448 nn_components.py:329] mlp: final activation = None
I0104 22:22:08.034362 140379616616448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:22:08.034823 140379616616448 nn_components.py:261] mlp: residual
I0104 22:22:08.041226 140379616616448 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:22:08.041795 140379616616448 transformer_base.py:443] tbase: final FFN
I0104 22:22:08.043165 140379616616448 nn_components.py:320] mlp: hidden 4096, relu
I0104 22:22:08.053304 140379616616448 nn_components.py:329] mlp: final activation = None
I0104 22:22:08.066102 140379616616448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:22:08.066495 140379616616448 nn_components.py:261] mlp: residual
I0104 22:22:08.067535 140379616616448 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:22:08.067874 140379616616448 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0104 22:22:08.068230 140379616616448 transformer_layer.py:154] tlayer: recurrent = False
I0104 22:22:08.068370 140379616616448 transformer_layer.py:155] tlayer: compute_importance = False
I0104 22:22:08.068453 140379616616448 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0104 22:22:08.071667 140379616616448 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:22:08.079150 140379616616448 transformer_base.py:161] kvq: pre_attn dropout.
I0104 22:22:08.109038 140379616616448 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:22:08.110087 140379616616448 transformer_base.py:194] kvq: normalize keys, queries.
I0104 22:22:08.115322 140379616616448 transformer_layer.py:213] tlayer: windowed attention.
I0104 22:22:08.129565 140379616616448 transformer_layer.py:299] tlayer: num_windows = 1.
I0104 22:22:08.129796 140379616616448 attention.py:418] Single window, no scan.
I0104 22:22:08.129887 140379616616448 transformer_layer.py:389] tlayer: self-attention.
I0104 22:22:08.129987 140379616616448 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:22:08.130213 140379616616448 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:22:08.131153 140379616616448 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[8,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:22:08.131362 140379616616448 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:22:08.132288 140379616616448 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:22:08.134417 140379616616448 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[8,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:22:08.140454 140379616616448 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[8,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:22:08.141341 140379616616448 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:22:08.141625 140379616616448 transformer_layer.py:468] tlayer: End windows.
I0104 22:22:08.141709 140379616616448 transformer_layer.py:472] tlayer: final FFN.
I0104 22:22:08.141818 140379616616448 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:22:08.142418 140379616616448 transformer_base.py:410] tbase: post-attention MLP.
I0104 22:22:08.143275 140379616616448 nn_components.py:325] mlp: activation = None
I0104 22:22:08.143523 140379616616448 nn_components.py:329] mlp: final activation = None
I0104 22:22:08.153810 140379616616448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:22:08.154199 140379616616448 nn_components.py:261] mlp: residual
I0104 22:22:08.163727 140379616616448 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:22:08.164052 140379616616448 transformer_base.py:443] tbase: final FFN
I0104 22:22:08.164901 140379616616448 nn_components.py:320] mlp: hidden 4096, relu
I0104 22:22:08.176705 140379616616448 nn_components.py:329] mlp: final activation = None
I0104 22:22:08.186250 140379616616448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:22:08.186774 140379616616448 nn_components.py:261] mlp: residual
I0104 22:22:08.188009 140379616616448 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:22:08.188335 140379616616448 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0104 22:22:08.188597 140379616616448 transformer_layer.py:154] tlayer: recurrent = False
I0104 22:22:08.188719 140379616616448 transformer_layer.py:155] tlayer: compute_importance = False
I0104 22:22:08.188812 140379616616448 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0104 22:22:08.192092 140379616616448 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:22:08.198900 140379616616448 transformer_base.py:161] kvq: pre_attn dropout.
I0104 22:22:08.226140 140379616616448 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:22:08.228178 140379616616448 transformer_base.py:194] kvq: normalize keys, queries.
I0104 22:22:08.235428 140379616616448 transformer_layer.py:213] tlayer: windowed attention.
I0104 22:22:08.246394 140379616616448 transformer_layer.py:299] tlayer: num_windows = 1.
I0104 22:22:08.246598 140379616616448 attention.py:418] Single window, no scan.
I0104 22:22:08.246676 140379616616448 transformer_layer.py:389] tlayer: self-attention.
I0104 22:22:08.246759 140379616616448 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:22:08.246975 140379616616448 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:22:08.247731 140379616616448 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[8,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:22:08.247907 140379616616448 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:22:08.248769 140379616616448 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:22:08.251654 140379616616448 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[8,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:22:08.258188 140379616616448 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[8,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:22:08.259395 140379616616448 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:22:08.259611 140379616616448 transformer_layer.py:468] tlayer: End windows.
I0104 22:22:08.259684 140379616616448 transformer_layer.py:472] tlayer: final FFN.
I0104 22:22:08.259824 140379616616448 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:22:08.260338 140379616616448 transformer_base.py:410] tbase: post-attention MLP.
I0104 22:22:08.261378 140379616616448 nn_components.py:325] mlp: activation = None
I0104 22:22:08.262192 140379616616448 nn_components.py:329] mlp: final activation = None
I0104 22:22:08.269965 140379616616448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:22:08.270263 140379616616448 nn_components.py:261] mlp: residual
I0104 22:22:08.276520 140379616616448 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:22:08.276958 140379616616448 transformer_base.py:443] tbase: final FFN
I0104 22:22:08.277589 140379616616448 nn_components.py:320] mlp: hidden 4096, relu
I0104 22:22:08.287583 140379616616448 nn_components.py:329] mlp: final activation = None
I0104 22:22:08.296308 140379616616448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:22:08.296735 140379616616448 nn_components.py:261] mlp: residual
I0104 22:22:08.297514 140379616616448 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:22:08.297725 140379616616448 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0104 22:22:08.297966 140379616616448 transformer_layer.py:154] tlayer: recurrent = False
I0104 22:22:08.298059 140379616616448 transformer_layer.py:155] tlayer: compute_importance = False
I0104 22:22:08.298134 140379616616448 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0104 22:22:08.300890 140379616616448 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:22:08.305251 140379616616448 transformer_base.py:161] kvq: pre_attn dropout.
I0104 22:22:08.329933 140379616616448 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:22:08.330805 140379616616448 transformer_base.py:194] kvq: normalize keys, queries.
I0104 22:22:08.335081 140379616616448 transformer_layer.py:213] tlayer: windowed attention.
I0104 22:22:08.346400 140379616616448 transformer_layer.py:299] tlayer: num_windows = 1.
I0104 22:22:08.346615 140379616616448 attention.py:418] Single window, no scan.
I0104 22:22:08.346691 140379616616448 transformer_layer.py:389] tlayer: self-attention.
I0104 22:22:08.346754 140379616616448 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:22:08.347029 140379616616448 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:22:08.347966 140379616616448 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[8,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:22:08.348181 140379616616448 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:22:08.349001 140379616616448 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:22:08.351475 140379616616448 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[8,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:22:08.360438 140379616616448 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[8,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:22:08.361762 140379616616448 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:22:08.362081 140379616616448 transformer_layer.py:468] tlayer: End windows.
I0104 22:22:08.362154 140379616616448 transformer_layer.py:472] tlayer: final FFN.
I0104 22:22:08.362263 140379616616448 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:22:08.363289 140379616616448 transformer_base.py:410] tbase: post-attention MLP.
I0104 22:22:08.364220 140379616616448 nn_components.py:325] mlp: activation = None
I0104 22:22:08.364489 140379616616448 nn_components.py:329] mlp: final activation = None
I0104 22:22:08.375347 140379616616448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:22:08.375973 140379616616448 nn_components.py:261] mlp: residual
I0104 22:22:08.383154 140379616616448 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:22:08.383417 140379616616448 transformer_base.py:443] tbase: final FFN
I0104 22:22:08.384071 140379616616448 nn_components.py:320] mlp: hidden 4096, relu
I0104 22:22:08.396364 140379616616448 nn_components.py:329] mlp: final activation = None
I0104 22:22:08.406294 140379616616448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:22:08.407386 140379616616448 nn_components.py:261] mlp: residual
I0104 22:22:08.408720 140379616616448 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:22:08.409246 140379616616448 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0104 22:22:08.409778 140379616616448 transformer_layer.py:154] tlayer: recurrent = False
I0104 22:22:08.409953 140379616616448 transformer_layer.py:155] tlayer: compute_importance = False
I0104 22:22:08.410045 140379616616448 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0104 22:22:08.415244 140379616616448 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:22:08.420517 140379616616448 transformer_base.py:161] kvq: pre_attn dropout.
I0104 22:22:08.449106 140379616616448 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:22:08.450252 140379616616448 transformer_base.py:194] kvq: normalize keys, queries.
I0104 22:22:08.457349 140379616616448 transformer_layer.py:213] tlayer: windowed attention.
I0104 22:22:08.469441 140379616616448 transformer_layer.py:299] tlayer: num_windows = 1.
I0104 22:22:08.469652 140379616616448 attention.py:418] Single window, no scan.
I0104 22:22:08.469754 140379616616448 transformer_layer.py:389] tlayer: self-attention.
I0104 22:22:08.469818 140379616616448 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:22:08.469966 140379616616448 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:22:08.470836 140379616616448 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[8,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:22:08.471201 140379616616448 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:22:08.472049 140379616616448 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:22:08.474437 140379616616448 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[8,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:22:08.480128 140379616616448 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[8,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:22:08.480952 140379616616448 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:22:08.481153 140379616616448 transformer_layer.py:468] tlayer: End windows.
I0104 22:22:08.481220 140379616616448 transformer_layer.py:472] tlayer: final FFN.
I0104 22:22:08.481319 140379616616448 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:22:08.481794 140379616616448 transformer_base.py:410] tbase: post-attention MLP.
I0104 22:22:08.482518 140379616616448 nn_components.py:325] mlp: activation = None
I0104 22:22:08.482729 140379616616448 nn_components.py:329] mlp: final activation = None
I0104 22:22:08.491595 140379616616448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:22:08.491914 140379616616448 nn_components.py:261] mlp: residual
I0104 22:22:08.498524 140379616616448 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:22:08.498876 140379616616448 transformer_base.py:443] tbase: final FFN
I0104 22:22:08.499504 140379616616448 nn_components.py:320] mlp: hidden 4096, relu
I0104 22:22:08.507378 140379616616448 nn_components.py:329] mlp: final activation = None
I0104 22:22:08.516167 140379616616448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:22:08.516666 140379616616448 nn_components.py:261] mlp: residual
I0104 22:22:08.517559 140379616616448 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:22:08.517765 140379616616448 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0104 22:22:08.517945 140379616616448 transformer_layer.py:154] tlayer: recurrent = False
I0104 22:22:08.518009 140379616616448 transformer_layer.py:155] tlayer: compute_importance = False
I0104 22:22:08.518057 140379616616448 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0104 22:22:08.522082 140379616616448 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:22:08.526601 140379616616448 transformer_base.py:161] kvq: pre_attn dropout.
I0104 22:22:08.550857 140379616616448 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:22:08.552337 140379616616448 transformer_base.py:194] kvq: normalize keys, queries.
I0104 22:22:08.561881 140379616616448 transformer_layer.py:213] tlayer: windowed attention.
I0104 22:22:08.580082 140379616616448 transformer_layer.py:299] tlayer: num_windows = 1.
I0104 22:22:08.580296 140379616616448 attention.py:418] Single window, no scan.
I0104 22:22:08.580399 140379616616448 transformer_layer.py:389] tlayer: self-attention.
I0104 22:22:08.580487 140379616616448 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:22:08.580652 140379616616448 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:22:08.581358 140379616616448 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[8,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:22:08.581534 140379616616448 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:22:08.582262 140379616616448 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:22:08.584782 140379616616448 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[8,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:22:08.592059 140379616616448 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[8,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:22:08.592900 140379616616448 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:22:08.593091 140379616616448 transformer_layer.py:468] tlayer: End windows.
I0104 22:22:08.593175 140379616616448 transformer_layer.py:472] tlayer: final FFN.
I0104 22:22:08.593273 140379616616448 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:22:08.593676 140379616616448 transformer_base.py:410] tbase: post-attention MLP.
I0104 22:22:08.594270 140379616616448 nn_components.py:325] mlp: activation = None
I0104 22:22:08.594507 140379616616448 nn_components.py:329] mlp: final activation = None
I0104 22:22:08.603993 140379616616448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:22:08.604346 140379616616448 nn_components.py:261] mlp: residual
I0104 22:22:08.611070 140379616616448 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:22:08.611381 140379616616448 transformer_base.py:443] tbase: final FFN
I0104 22:22:08.612023 140379616616448 nn_components.py:320] mlp: hidden 4096, relu
I0104 22:22:08.621062 140379616616448 nn_components.py:329] mlp: final activation = None
I0104 22:22:08.629953 140379616616448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:22:08.630643 140379616616448 nn_components.py:261] mlp: residual
I0104 22:22:08.631566 140379616616448 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:22:08.632186 140379616616448 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0104 22:22:08.632315 140379616616448 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0104 22:22:08.632410 140379616616448 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0104 22:22:08.632498 140379616616448 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0104 22:22:08.632581 140379616616448 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0104 22:22:08.632662 140379616616448 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0104 22:22:08.632743 140379616616448 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0104 22:22:08.632823 140379616616448 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0104 22:22:08.632901 140379616616448 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0104 22:22:08.633012 140379616616448 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0104 22:22:08.633090 140379616616448 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0104 22:22:08.633168 140379616616448 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0104 22:22:08.633238 140379616616448 decoder_stack.py:344] dstack: Final layernorm.
I0104 22:22:08.638389 140379616616448 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[8,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0104 22:22:08.773710 140379616616448 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:22:08.773956 140379616616448 decoder_stack.py:333] dstack: autoregressive generator.
I0104 22:22:08.774044 140379616616448 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0104 22:22:08.774218 140379616616448 transformer_layer.py:154] tlayer: recurrent = False
I0104 22:22:08.774353 140379616616448 transformer_layer.py:155] tlayer: compute_importance = False
I0104 22:22:08.774458 140379616616448 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0104 22:22:08.774557 140379616616448 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:22:08.778123 140379616616448 transformer_base.py:161] kvq: pre_attn dropout.
I0104 22:22:08.799337 140379616616448 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:22:08.800292 140379616616448 transformer_base.py:194] kvq: normalize keys, queries.
I0104 22:22:08.804923 140379616616448 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0104 22:22:08.829237 140379616616448 transformer_layer.py:299] tlayer: num_windows = 1.
I0104 22:22:08.829434 140379616616448 attention.py:418] Single window, no scan.
I0104 22:22:08.829531 140379616616448 transformer_layer.py:389] tlayer: self-attention.
I0104 22:22:08.829598 140379616616448 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[8,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:22:08.829765 140379616616448 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:22:08.830415 140379616616448 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:22:08.830579 140379616616448 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:22:08.831369 140379616616448 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:22:08.832673 140379616616448 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:22:08.836734 140379616616448 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:22:08.837499 140379616616448 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:22:08.837675 140379616616448 transformer_layer.py:468] tlayer: End windows.
I0104 22:22:08.837761 140379616616448 transformer_layer.py:472] tlayer: final FFN.
I0104 22:22:08.837879 140379616616448 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:22:08.838291 140379616616448 transformer_base.py:410] tbase: post-attention MLP.
I0104 22:22:08.838536 140379616616448 nn_components.py:325] mlp: activation = None
I0104 22:22:08.838631 140379616616448 nn_components.py:329] mlp: final activation = None
I0104 22:22:08.845436 140379616616448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:22:08.845718 140379616616448 nn_components.py:261] mlp: residual
I0104 22:22:08.850526 140379616616448 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:22:08.850789 140379616616448 transformer_base.py:443] tbase: final FFN
I0104 22:22:08.851140 140379616616448 nn_components.py:320] mlp: hidden 4096, relu
I0104 22:22:08.862035 140379616616448 nn_components.py:329] mlp: final activation = None
I0104 22:22:08.875619 140379616616448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:22:08.876006 140379616616448 nn_components.py:261] mlp: residual
I0104 22:22:08.877383 140379616616448 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:22:08.878077 140379616616448 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0104 22:22:08.878544 140379616616448 transformer_layer.py:154] tlayer: recurrent = False
I0104 22:22:08.878758 140379616616448 transformer_layer.py:155] tlayer: compute_importance = False
I0104 22:22:08.878833 140379616616448 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0104 22:22:08.878971 140379616616448 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:22:08.885304 140379616616448 transformer_base.py:161] kvq: pre_attn dropout.
I0104 22:22:08.911972 140379616616448 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:22:08.913088 140379616616448 transformer_base.py:194] kvq: normalize keys, queries.
I0104 22:22:08.921322 140379616616448 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0104 22:22:08.958320 140379616616448 transformer_layer.py:299] tlayer: num_windows = 1.
I0104 22:22:08.958549 140379616616448 attention.py:418] Single window, no scan.
I0104 22:22:08.958628 140379616616448 transformer_layer.py:389] tlayer: self-attention.
I0104 22:22:08.958714 140379616616448 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[8,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:22:08.958968 140379616616448 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:22:08.959725 140379616616448 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:22:08.959893 140379616616448 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:22:08.960644 140379616616448 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:22:08.964165 140379616616448 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:22:08.968253 140379616616448 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:22:08.968955 140379616616448 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:22:08.969122 140379616616448 transformer_layer.py:468] tlayer: End windows.
I0104 22:22:08.969205 140379616616448 transformer_layer.py:472] tlayer: final FFN.
I0104 22:22:08.969317 140379616616448 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:22:08.969770 140379616616448 transformer_base.py:410] tbase: post-attention MLP.
I0104 22:22:08.970102 140379616616448 nn_components.py:325] mlp: activation = None
I0104 22:22:08.970217 140379616616448 nn_components.py:329] mlp: final activation = None
I0104 22:22:08.979534 140379616616448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:22:08.979821 140379616616448 nn_components.py:261] mlp: residual
I0104 22:22:08.986846 140379616616448 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:22:08.987451 140379616616448 transformer_base.py:443] tbase: final FFN
I0104 22:22:08.987755 140379616616448 nn_components.py:320] mlp: hidden 4096, relu
I0104 22:22:08.996959 140379616616448 nn_components.py:329] mlp: final activation = None
I0104 22:22:09.003777 140379616616448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:22:09.004123 140379616616448 nn_components.py:261] mlp: residual
I0104 22:22:09.004735 140379616616448 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:22:09.004897 140379616616448 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0104 22:22:09.005076 140379616616448 transformer_layer.py:154] tlayer: recurrent = False
I0104 22:22:09.005210 140379616616448 transformer_layer.py:155] tlayer: compute_importance = False
I0104 22:22:09.005295 140379616616448 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0104 22:22:09.005484 140379616616448 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:22:09.011877 140379616616448 transformer_base.py:161] kvq: pre_attn dropout.
I0104 22:22:09.034789 140379616616448 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:22:09.035721 140379616616448 transformer_base.py:194] kvq: normalize keys, queries.
I0104 22:22:09.040402 140379616616448 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0104 22:22:09.073428 140379616616448 transformer_layer.py:299] tlayer: num_windows = 1.
I0104 22:22:09.073995 140379616616448 attention.py:418] Single window, no scan.
I0104 22:22:09.074166 140379616616448 transformer_layer.py:389] tlayer: self-attention.
I0104 22:22:09.074316 140379616616448 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[8,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:22:09.074629 140379616616448 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:22:09.076052 140379616616448 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:22:09.076645 140379616616448 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:22:09.078217 140379616616448 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:22:09.081422 140379616616448 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:22:09.086177 140379616616448 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:22:09.087056 140379616616448 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:22:09.087249 140379616616448 transformer_layer.py:468] tlayer: End windows.
I0104 22:22:09.087340 140379616616448 transformer_layer.py:472] tlayer: final FFN.
I0104 22:22:09.087457 140379616616448 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:22:09.087906 140379616616448 transformer_base.py:410] tbase: post-attention MLP.
I0104 22:22:09.088159 140379616616448 nn_components.py:325] mlp: activation = None
I0104 22:22:09.088256 140379616616448 nn_components.py:329] mlp: final activation = None
I0104 22:22:09.095227 140379616616448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:22:09.095504 140379616616448 nn_components.py:261] mlp: residual
I0104 22:22:09.101557 140379616616448 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:22:09.101904 140379616616448 transformer_base.py:443] tbase: final FFN
I0104 22:22:09.102265 140379616616448 nn_components.py:320] mlp: hidden 4096, relu
I0104 22:22:09.112523 140379616616448 nn_components.py:329] mlp: final activation = None
I0104 22:22:09.120995 140379616616448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:22:09.121295 140379616616448 nn_components.py:261] mlp: residual
I0104 22:22:09.121865 140379616616448 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:22:09.122083 140379616616448 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0104 22:22:09.122425 140379616616448 transformer_layer.py:154] tlayer: recurrent = False
I0104 22:22:09.122653 140379616616448 transformer_layer.py:155] tlayer: compute_importance = False
I0104 22:22:09.122783 140379616616448 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0104 22:22:09.123091 140379616616448 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:22:09.129078 140379616616448 transformer_base.py:161] kvq: pre_attn dropout.
I0104 22:22:09.153094 140379616616448 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:22:09.153895 140379616616448 transformer_base.py:194] kvq: normalize keys, queries.
I0104 22:22:09.160281 140379616616448 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0104 22:22:09.192365 140379616616448 transformer_layer.py:299] tlayer: num_windows = 1.
I0104 22:22:09.192699 140379616616448 attention.py:418] Single window, no scan.
I0104 22:22:09.192793 140379616616448 transformer_layer.py:389] tlayer: self-attention.
I0104 22:22:09.192911 140379616616448 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[8,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:22:09.193104 140379616616448 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:22:09.193856 140379616616448 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:22:09.194033 140379616616448 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:22:09.194830 140379616616448 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:22:09.196321 140379616616448 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:22:09.200815 140379616616448 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:22:09.201754 140379616616448 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:22:09.201977 140379616616448 transformer_layer.py:468] tlayer: End windows.
I0104 22:22:09.202100 140379616616448 transformer_layer.py:472] tlayer: final FFN.
I0104 22:22:09.202275 140379616616448 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:22:09.202995 140379616616448 transformer_base.py:410] tbase: post-attention MLP.
I0104 22:22:09.203368 140379616616448 nn_components.py:325] mlp: activation = None
I0104 22:22:09.203473 140379616616448 nn_components.py:329] mlp: final activation = None
I0104 22:22:09.212356 140379616616448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:22:09.212654 140379616616448 nn_components.py:261] mlp: residual
I0104 22:22:09.217786 140379616616448 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:22:09.218034 140379616616448 transformer_base.py:443] tbase: final FFN
I0104 22:22:09.218230 140379616616448 nn_components.py:320] mlp: hidden 4096, relu
I0104 22:22:09.225788 140379616616448 nn_components.py:329] mlp: final activation = None
I0104 22:22:09.232449 140379616616448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:22:09.232851 140379616616448 nn_components.py:261] mlp: residual
I0104 22:22:09.233517 140379616616448 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:22:09.233727 140379616616448 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0104 22:22:09.233954 140379616616448 transformer_layer.py:154] tlayer: recurrent = False
I0104 22:22:09.234049 140379616616448 transformer_layer.py:155] tlayer: compute_importance = False
I0104 22:22:09.234126 140379616616448 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0104 22:22:09.234233 140379616616448 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:22:09.238282 140379616616448 transformer_base.py:161] kvq: pre_attn dropout.
I0104 22:22:09.257738 140379616616448 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:22:09.258585 140379616616448 transformer_base.py:194] kvq: normalize keys, queries.
I0104 22:22:09.263214 140379616616448 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0104 22:22:09.304171 140379616616448 transformer_layer.py:299] tlayer: num_windows = 1.
I0104 22:22:09.304787 140379616616448 attention.py:418] Single window, no scan.
I0104 22:22:09.304939 140379616616448 transformer_layer.py:389] tlayer: self-attention.
I0104 22:22:09.305010 140379616616448 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[8,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:22:09.305340 140379616616448 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:22:09.306763 140379616616448 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:22:09.307224 140379616616448 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:22:09.308280 140379616616448 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:22:09.310256 140379616616448 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:22:09.319781 140379616616448 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:22:09.320592 140379616616448 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:22:09.320783 140379616616448 transformer_layer.py:468] tlayer: End windows.
I0104 22:22:09.320874 140379616616448 transformer_layer.py:472] tlayer: final FFN.
I0104 22:22:09.320984 140379616616448 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:22:09.321533 140379616616448 transformer_base.py:410] tbase: post-attention MLP.
I0104 22:22:09.321955 140379616616448 nn_components.py:325] mlp: activation = None
I0104 22:22:09.322091 140379616616448 nn_components.py:329] mlp: final activation = None
I0104 22:22:09.332325 140379616616448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:22:09.332757 140379616616448 nn_components.py:261] mlp: residual
I0104 22:22:09.342101 140379616616448 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:22:09.342550 140379616616448 transformer_base.py:443] tbase: final FFN
I0104 22:22:09.343069 140379616616448 nn_components.py:320] mlp: hidden 4096, relu
I0104 22:22:09.359631 140379616616448 nn_components.py:329] mlp: final activation = None
I0104 22:22:09.368024 140379616616448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:22:09.368423 140379616616448 nn_components.py:261] mlp: residual
I0104 22:22:09.369679 140379616616448 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:22:09.370340 140379616616448 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0104 22:22:09.370948 140379616616448 transformer_layer.py:154] tlayer: recurrent = False
I0104 22:22:09.371181 140379616616448 transformer_layer.py:155] tlayer: compute_importance = False
I0104 22:22:09.371286 140379616616448 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0104 22:22:09.371455 140379616616448 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:22:09.379144 140379616616448 transformer_base.py:161] kvq: pre_attn dropout.
I0104 22:22:09.410230 140379616616448 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:22:09.411461 140379616616448 transformer_base.py:194] kvq: normalize keys, queries.
I0104 22:22:09.419924 140379616616448 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0104 22:22:09.465199 140379616616448 transformer_layer.py:299] tlayer: num_windows = 1.
I0104 22:22:09.465408 140379616616448 attention.py:418] Single window, no scan.
I0104 22:22:09.465488 140379616616448 transformer_layer.py:389] tlayer: self-attention.
I0104 22:22:09.465574 140379616616448 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[8,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:22:09.465776 140379616616448 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:22:09.466700 140379616616448 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:22:09.467086 140379616616448 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:22:09.468478 140379616616448 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:22:09.471458 140379616616448 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:22:09.477586 140379616616448 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:22:09.478492 140379616616448 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:22:09.478934 140379616616448 transformer_layer.py:468] tlayer: End windows.
I0104 22:22:09.479183 140379616616448 transformer_layer.py:472] tlayer: final FFN.
I0104 22:22:09.479705 140379616616448 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:22:09.480712 140379616616448 transformer_base.py:410] tbase: post-attention MLP.
I0104 22:22:09.481206 140379616616448 nn_components.py:325] mlp: activation = None
I0104 22:22:09.481446 140379616616448 nn_components.py:329] mlp: final activation = None
I0104 22:22:09.488409 140379616616448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:22:09.488759 140379616616448 nn_components.py:261] mlp: residual
I0104 22:22:09.494893 140379616616448 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:22:09.495188 140379616616448 transformer_base.py:443] tbase: final FFN
I0104 22:22:09.495402 140379616616448 nn_components.py:320] mlp: hidden 4096, relu
I0104 22:22:09.504793 140379616616448 nn_components.py:329] mlp: final activation = None
I0104 22:22:09.514220 140379616616448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:22:09.514683 140379616616448 nn_components.py:261] mlp: residual
I0104 22:22:09.515568 140379616616448 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:22:09.515792 140379616616448 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0104 22:22:09.516101 140379616616448 transformer_layer.py:154] tlayer: recurrent = False
I0104 22:22:09.516215 140379616616448 transformer_layer.py:155] tlayer: compute_importance = False
I0104 22:22:09.516305 140379616616448 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0104 22:22:09.516429 140379616616448 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:22:09.521350 140379616616448 transformer_base.py:161] kvq: pre_attn dropout.
I0104 22:22:09.545095 140379616616448 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:22:09.546243 140379616616448 transformer_base.py:194] kvq: normalize keys, queries.
I0104 22:22:09.551017 140379616616448 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0104 22:22:09.587254 140379616616448 transformer_layer.py:299] tlayer: num_windows = 1.
I0104 22:22:09.587626 140379616616448 attention.py:418] Single window, no scan.
I0104 22:22:09.587763 140379616616448 transformer_layer.py:389] tlayer: self-attention.
I0104 22:22:09.587946 140379616616448 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[8,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:22:09.588265 140379616616448 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:22:09.589767 140379616616448 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:22:09.590461 140379616616448 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:22:09.592194 140379616616448 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:22:09.595478 140379616616448 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:22:09.600198 140379616616448 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:22:09.600932 140379616616448 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:22:09.601104 140379616616448 transformer_layer.py:468] tlayer: End windows.
I0104 22:22:09.601187 140379616616448 transformer_layer.py:472] tlayer: final FFN.
I0104 22:22:09.601300 140379616616448 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:22:09.601686 140379616616448 transformer_base.py:410] tbase: post-attention MLP.
I0104 22:22:09.601955 140379616616448 nn_components.py:325] mlp: activation = None
I0104 22:22:09.602052 140379616616448 nn_components.py:329] mlp: final activation = None
I0104 22:22:09.612447 140379616616448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:22:09.612754 140379616616448 nn_components.py:261] mlp: residual
I0104 22:22:09.620324 140379616616448 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:22:09.620888 140379616616448 transformer_base.py:443] tbase: final FFN
I0104 22:22:09.621350 140379616616448 nn_components.py:320] mlp: hidden 4096, relu
I0104 22:22:09.632292 140379616616448 nn_components.py:329] mlp: final activation = None
I0104 22:22:09.640092 140379616616448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:22:09.640891 140379616616448 nn_components.py:261] mlp: residual
I0104 22:22:09.642038 140379616616448 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:22:09.642507 140379616616448 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0104 22:22:09.642941 140379616616448 transformer_layer.py:154] tlayer: recurrent = False
I0104 22:22:09.643056 140379616616448 transformer_layer.py:155] tlayer: compute_importance = False
I0104 22:22:09.643137 140379616616448 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0104 22:22:09.643234 140379616616448 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:22:09.648215 140379616616448 transformer_base.py:161] kvq: pre_attn dropout.
I0104 22:22:09.672026 140379616616448 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:22:09.673404 140379616616448 transformer_base.py:194] kvq: normalize keys, queries.
I0104 22:22:09.680467 140379616616448 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0104 22:22:09.712195 140379616616448 transformer_layer.py:299] tlayer: num_windows = 1.
I0104 22:22:09.712406 140379616616448 attention.py:418] Single window, no scan.
I0104 22:22:09.712487 140379616616448 transformer_layer.py:389] tlayer: self-attention.
I0104 22:22:09.712581 140379616616448 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[8,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:22:09.712767 140379616616448 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:22:09.713672 140379616616448 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:22:09.713898 140379616616448 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:22:09.714826 140379616616448 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:22:09.716429 140379616616448 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:22:09.722338 140379616616448 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:22:09.723913 140379616616448 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:22:09.724222 140379616616448 transformer_layer.py:468] tlayer: End windows.
I0104 22:22:09.724294 140379616616448 transformer_layer.py:472] tlayer: final FFN.
I0104 22:22:09.724412 140379616616448 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:22:09.724853 140379616616448 transformer_base.py:410] tbase: post-attention MLP.
I0104 22:22:09.725111 140379616616448 nn_components.py:325] mlp: activation = None
I0104 22:22:09.725209 140379616616448 nn_components.py:329] mlp: final activation = None
I0104 22:22:09.734134 140379616616448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:22:09.734401 140379616616448 nn_components.py:261] mlp: residual
I0104 22:22:09.738374 140379616616448 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:22:09.738593 140379616616448 transformer_base.py:443] tbase: final FFN
I0104 22:22:09.738826 140379616616448 nn_components.py:320] mlp: hidden 4096, relu
I0104 22:22:09.748017 140379616616448 nn_components.py:329] mlp: final activation = None
I0104 22:22:09.758737 140379616616448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:22:09.759241 140379616616448 nn_components.py:261] mlp: residual
I0104 22:22:09.759913 140379616616448 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:22:09.760132 140379616616448 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0104 22:22:09.760367 140379616616448 transformer_layer.py:154] tlayer: recurrent = False
I0104 22:22:09.760520 140379616616448 transformer_layer.py:155] tlayer: compute_importance = False
I0104 22:22:09.760595 140379616616448 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0104 22:22:09.760688 140379616616448 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:22:09.767791 140379616616448 transformer_base.py:161] kvq: pre_attn dropout.
I0104 22:22:09.794045 140379616616448 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:22:09.795331 140379616616448 transformer_base.py:194] kvq: normalize keys, queries.
I0104 22:22:09.804261 140379616616448 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0104 22:22:09.833754 140379616616448 transformer_layer.py:299] tlayer: num_windows = 1.
I0104 22:22:09.834032 140379616616448 attention.py:418] Single window, no scan.
I0104 22:22:09.834154 140379616616448 transformer_layer.py:389] tlayer: self-attention.
I0104 22:22:09.834250 140379616616448 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[8,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:22:09.834416 140379616616448 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:22:09.835218 140379616616448 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:22:09.835386 140379616616448 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:22:09.836112 140379616616448 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:22:09.837434 140379616616448 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:22:09.841540 140379616616448 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:22:09.842271 140379616616448 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:22:09.842442 140379616616448 transformer_layer.py:468] tlayer: End windows.
I0104 22:22:09.842528 140379616616448 transformer_layer.py:472] tlayer: final FFN.
I0104 22:22:09.842640 140379616616448 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:22:09.843349 140379616616448 transformer_base.py:410] tbase: post-attention MLP.
I0104 22:22:09.843616 140379616616448 nn_components.py:325] mlp: activation = None
I0104 22:22:09.843713 140379616616448 nn_components.py:329] mlp: final activation = None
I0104 22:22:09.850556 140379616616448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:22:09.850988 140379616616448 nn_components.py:261] mlp: residual
I0104 22:22:09.856000 140379616616448 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:22:09.856268 140379616616448 transformer_base.py:443] tbase: final FFN
I0104 22:22:09.856492 140379616616448 nn_components.py:320] mlp: hidden 4096, relu
I0104 22:22:09.863979 140379616616448 nn_components.py:329] mlp: final activation = None
I0104 22:22:09.871110 140379616616448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:22:09.871691 140379616616448 nn_components.py:261] mlp: residual
I0104 22:22:09.873139 140379616616448 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:22:09.873559 140379616616448 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0104 22:22:09.873934 140379616616448 transformer_layer.py:154] tlayer: recurrent = False
I0104 22:22:09.874145 140379616616448 transformer_layer.py:155] tlayer: compute_importance = False
I0104 22:22:09.874289 140379616616448 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0104 22:22:09.874505 140379616616448 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:22:09.884351 140379616616448 transformer_base.py:161] kvq: pre_attn dropout.
I0104 22:22:09.911626 140379616616448 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:22:09.913849 140379616616448 transformer_base.py:194] kvq: normalize keys, queries.
I0104 22:22:09.919909 140379616616448 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0104 22:22:09.958156 140379616616448 transformer_layer.py:299] tlayer: num_windows = 1.
I0104 22:22:09.958443 140379616616448 attention.py:418] Single window, no scan.
I0104 22:22:09.958528 140379616616448 transformer_layer.py:389] tlayer: self-attention.
I0104 22:22:09.958617 140379616616448 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[8,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:22:09.958764 140379616616448 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:22:09.959584 140379616616448 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:22:09.959810 140379616616448 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:22:09.960678 140379616616448 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:22:09.963894 140379616616448 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:22:09.969396 140379616616448 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:22:09.970478 140379616616448 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:22:09.970832 140379616616448 transformer_layer.py:468] tlayer: End windows.
I0104 22:22:09.971003 140379616616448 transformer_layer.py:472] tlayer: final FFN.
I0104 22:22:09.971147 140379616616448 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:22:09.971894 140379616616448 transformer_base.py:410] tbase: post-attention MLP.
I0104 22:22:09.972304 140379616616448 nn_components.py:325] mlp: activation = None
I0104 22:22:09.972446 140379616616448 nn_components.py:329] mlp: final activation = None
I0104 22:22:09.981545 140379616616448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:22:09.981972 140379616616448 nn_components.py:261] mlp: residual
I0104 22:22:09.989490 140379616616448 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:22:09.989792 140379616616448 transformer_base.py:443] tbase: final FFN
I0104 22:22:09.990071 140379616616448 nn_components.py:320] mlp: hidden 4096, relu
I0104 22:22:09.998158 140379616616448 nn_components.py:329] mlp: final activation = None
I0104 22:22:10.004503 140379616616448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:22:10.004782 140379616616448 nn_components.py:261] mlp: residual
I0104 22:22:10.005377 140379616616448 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:22:10.005569 140379616616448 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0104 22:22:10.005795 140379616616448 transformer_layer.py:154] tlayer: recurrent = False
I0104 22:22:10.005890 140379616616448 transformer_layer.py:155] tlayer: compute_importance = False
I0104 22:22:10.005969 140379616616448 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0104 22:22:10.006095 140379616616448 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:22:10.010169 140379616616448 transformer_base.py:161] kvq: pre_attn dropout.
I0104 22:22:10.029551 140379616616448 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:22:10.030449 140379616616448 transformer_base.py:194] kvq: normalize keys, queries.
I0104 22:22:10.035330 140379616616448 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0104 22:22:10.073925 140379616616448 transformer_layer.py:299] tlayer: num_windows = 1.
I0104 22:22:10.074137 140379616616448 attention.py:418] Single window, no scan.
I0104 22:22:10.074255 140379616616448 transformer_layer.py:389] tlayer: self-attention.
I0104 22:22:10.074340 140379616616448 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[8,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:22:10.074563 140379616616448 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:22:10.076079 140379616616448 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:22:10.076533 140379616616448 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:22:10.077980 140379616616448 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:22:10.080480 140379616616448 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:22:10.088048 140379616616448 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:22:10.089185 140379616616448 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:22:10.089631 140379616616448 transformer_layer.py:468] tlayer: End windows.
I0104 22:22:10.089770 140379616616448 transformer_layer.py:472] tlayer: final FFN.
I0104 22:22:10.089974 140379616616448 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:22:10.090878 140379616616448 transformer_base.py:410] tbase: post-attention MLP.
I0104 22:22:10.091681 140379616616448 nn_components.py:325] mlp: activation = None
I0104 22:22:10.091881 140379616616448 nn_components.py:329] mlp: final activation = None
I0104 22:22:10.102893 140379616616448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:22:10.103221 140379616616448 nn_components.py:261] mlp: residual
I0104 22:22:10.110090 140379616616448 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:22:10.110405 140379616616448 transformer_base.py:443] tbase: final FFN
I0104 22:22:10.110646 140379616616448 nn_components.py:320] mlp: hidden 4096, relu
I0104 22:22:10.119602 140379616616448 nn_components.py:329] mlp: final activation = None
I0104 22:22:10.129354 140379616616448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:22:10.129839 140379616616448 nn_components.py:261] mlp: residual
I0104 22:22:10.130944 140379616616448 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:22:10.131895 140379616616448 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0104 22:22:10.132521 140379616616448 transformer_layer.py:154] tlayer: recurrent = False
I0104 22:22:10.132670 140379616616448 transformer_layer.py:155] tlayer: compute_importance = False
I0104 22:22:10.132747 140379616616448 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0104 22:22:10.132932 140379616616448 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:22:10.139450 140379616616448 transformer_base.py:161] kvq: pre_attn dropout.
I0104 22:22:10.167398 140379616616448 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:22:10.168712 140379616616448 transformer_base.py:194] kvq: normalize keys, queries.
I0104 22:22:10.175115 140379616616448 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0104 22:22:10.204350 140379616616448 transformer_layer.py:299] tlayer: num_windows = 1.
I0104 22:22:10.204598 140379616616448 attention.py:418] Single window, no scan.
I0104 22:22:10.204762 140379616616448 transformer_layer.py:389] tlayer: self-attention.
I0104 22:22:10.204900 140379616616448 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[8,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:22:10.205132 140379616616448 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:22:10.206275 140379616616448 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:22:10.206913 140379616616448 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:22:10.208374 140379616616448 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:22:10.210409 140379616616448 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:22:10.214932 140379616616448 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:22:10.216012 140379616616448 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:22:10.216262 140379616616448 transformer_layer.py:468] tlayer: End windows.
I0104 22:22:10.216336 140379616616448 transformer_layer.py:472] tlayer: final FFN.
I0104 22:22:10.216435 140379616616448 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:22:10.216818 140379616616448 transformer_base.py:410] tbase: post-attention MLP.
I0104 22:22:10.217020 140379616616448 nn_components.py:325] mlp: activation = None
I0104 22:22:10.217086 140379616616448 nn_components.py:329] mlp: final activation = None
I0104 22:22:10.226362 140379616616448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:22:10.226908 140379616616448 nn_components.py:261] mlp: residual
I0104 22:22:10.233546 140379616616448 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:22:10.233826 140379616616448 transformer_base.py:443] tbase: final FFN
I0104 22:22:10.234026 140379616616448 nn_components.py:320] mlp: hidden 4096, relu
I0104 22:22:10.242438 140379616616448 nn_components.py:329] mlp: final activation = None
I0104 22:22:10.250131 140379616616448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:22:10.250508 140379616616448 nn_components.py:261] mlp: residual
I0104 22:22:10.251322 140379616616448 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:22:10.251628 140379616616448 decoder_stack.py:344] dstack: Final layernorm.
I0104 22:22:10.259448 140379616616448 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0104 22:22:10.590158 140379616616448 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:22:10.590659 140379616616448 decoder_stack.py:333] dstack: autoregressive generator.
I0104 22:22:10.590940 140379616616448 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0104 22:22:10.591478 140379616616448 transformer_layer.py:154] tlayer: recurrent = False
I0104 22:22:10.591663 140379616616448 transformer_layer.py:155] tlayer: compute_importance = False
I0104 22:22:10.591741 140379616616448 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0104 22:22:10.591876 140379616616448 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:22:10.598819 140379616616448 transformer_base.py:161] kvq: pre_attn dropout.
I0104 22:22:10.621592 140379616616448 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:22:10.622459 140379616616448 transformer_base.py:194] kvq: normalize keys, queries.
I0104 22:22:10.627008 140379616616448 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0104 22:22:10.649957 140379616616448 transformer_layer.py:299] tlayer: num_windows = 1.
I0104 22:22:10.650156 140379616616448 attention.py:418] Single window, no scan.
I0104 22:22:10.650253 140379616616448 transformer_layer.py:389] tlayer: self-attention.
I0104 22:22:10.650323 140379616616448 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[8,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:22:10.650479 140379616616448 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:22:10.651378 140379616616448 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:22:10.651540 140379616616448 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:22:10.652188 140379616616448 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:22:10.653498 140379616616448 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:22:10.657462 140379616616448 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:22:10.658206 140379616616448 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:22:10.658376 140379616616448 transformer_layer.py:468] tlayer: End windows.
I0104 22:22:10.658459 140379616616448 transformer_layer.py:472] tlayer: final FFN.
I0104 22:22:10.658594 140379616616448 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:22:10.659067 140379616616448 transformer_base.py:410] tbase: post-attention MLP.
I0104 22:22:10.659325 140379616616448 nn_components.py:325] mlp: activation = None
I0104 22:22:10.659424 140379616616448 nn_components.py:329] mlp: final activation = None
I0104 22:22:10.665794 140379616616448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:22:10.666433 140379616616448 nn_components.py:261] mlp: residual
I0104 22:22:10.678215 140379616616448 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:22:10.678888 140379616616448 transformer_base.py:443] tbase: final FFN
I0104 22:22:10.679556 140379616616448 nn_components.py:320] mlp: hidden 4096, relu
I0104 22:22:10.692475 140379616616448 nn_components.py:329] mlp: final activation = None
I0104 22:22:10.701865 140379616616448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:22:10.703196 140379616616448 nn_components.py:261] mlp: residual
I0104 22:22:10.705699 140379616616448 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:22:10.706610 140379616616448 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0104 22:22:10.707362 140379616616448 transformer_layer.py:154] tlayer: recurrent = False
I0104 22:22:10.707554 140379616616448 transformer_layer.py:155] tlayer: compute_importance = False
I0104 22:22:10.707630 140379616616448 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0104 22:22:10.707795 140379616616448 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:22:10.712627 140379616616448 transformer_base.py:161] kvq: pre_attn dropout.
I0104 22:22:10.736079 140379616616448 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:22:10.736989 140379616616448 transformer_base.py:194] kvq: normalize keys, queries.
I0104 22:22:10.741677 140379616616448 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0104 22:22:10.777048 140379616616448 transformer_layer.py:299] tlayer: num_windows = 1.
I0104 22:22:10.777278 140379616616448 attention.py:418] Single window, no scan.
I0104 22:22:10.777382 140379616616448 transformer_layer.py:389] tlayer: self-attention.
I0104 22:22:10.777514 140379616616448 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[8,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:22:10.777709 140379616616448 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:22:10.778591 140379616616448 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:22:10.778790 140379616616448 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:22:10.780010 140379616616448 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:22:10.782811 140379616616448 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:22:10.793162 140379616616448 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:22:10.795440 140379616616448 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:22:10.795993 140379616616448 transformer_layer.py:468] tlayer: End windows.
I0104 22:22:10.796118 140379616616448 transformer_layer.py:472] tlayer: final FFN.
I0104 22:22:10.796318 140379616616448 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:22:10.797316 140379616616448 transformer_base.py:410] tbase: post-attention MLP.
I0104 22:22:10.797981 140379616616448 nn_components.py:325] mlp: activation = None
I0104 22:22:10.798148 140379616616448 nn_components.py:329] mlp: final activation = None
I0104 22:22:10.805364 140379616616448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:22:10.805743 140379616616448 nn_components.py:261] mlp: residual
I0104 22:22:10.815904 140379616616448 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:22:10.816217 140379616616448 transformer_base.py:443] tbase: final FFN
I0104 22:22:10.816472 140379616616448 nn_components.py:320] mlp: hidden 4096, relu
I0104 22:22:10.826867 140379616616448 nn_components.py:329] mlp: final activation = None
I0104 22:22:10.838940 140379616616448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:22:10.839300 140379616616448 nn_components.py:261] mlp: residual
I0104 22:22:10.840043 140379616616448 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:22:10.840282 140379616616448 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0104 22:22:10.840539 140379616616448 transformer_layer.py:154] tlayer: recurrent = False
I0104 22:22:10.840638 140379616616448 transformer_layer.py:155] tlayer: compute_importance = False
I0104 22:22:10.840699 140379616616448 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0104 22:22:10.840795 140379616616448 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:22:10.845125 140379616616448 transformer_base.py:161] kvq: pre_attn dropout.
I0104 22:22:10.869663 140379616616448 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:22:10.870615 140379616616448 transformer_base.py:194] kvq: normalize keys, queries.
I0104 22:22:10.877161 140379616616448 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0104 22:22:10.905704 140379616616448 transformer_layer.py:299] tlayer: num_windows = 1.
I0104 22:22:10.905967 140379616616448 attention.py:418] Single window, no scan.
I0104 22:22:10.906164 140379616616448 transformer_layer.py:389] tlayer: self-attention.
I0104 22:22:10.906408 140379616616448 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[8,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:22:10.906765 140379616616448 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:22:10.908221 140379616616448 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:22:10.908643 140379616616448 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:22:10.909781 140379616616448 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:22:10.911432 140379616616448 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:22:10.916320 140379616616448 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:22:10.917156 140379616616448 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:22:10.917308 140379616616448 transformer_layer.py:468] tlayer: End windows.
I0104 22:22:10.917370 140379616616448 transformer_layer.py:472] tlayer: final FFN.
I0104 22:22:10.917511 140379616616448 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:22:10.918188 140379616616448 transformer_base.py:410] tbase: post-attention MLP.
I0104 22:22:10.918859 140379616616448 nn_components.py:325] mlp: activation = None
I0104 22:22:10.919194 140379616616448 nn_components.py:329] mlp: final activation = None
I0104 22:22:10.928208 140379616616448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:22:10.928611 140379616616448 nn_components.py:261] mlp: residual
I0104 22:22:10.933815 140379616616448 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:22:10.934063 140379616616448 transformer_base.py:443] tbase: final FFN
I0104 22:22:10.934247 140379616616448 nn_components.py:320] mlp: hidden 4096, relu
I0104 22:22:10.944641 140379616616448 nn_components.py:329] mlp: final activation = None
I0104 22:22:10.952302 140379616616448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:22:10.952723 140379616616448 nn_components.py:261] mlp: residual
I0104 22:22:10.953839 140379616616448 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:22:10.954410 140379616616448 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0104 22:22:10.954939 140379616616448 transformer_layer.py:154] tlayer: recurrent = False
I0104 22:22:10.955234 140379616616448 transformer_layer.py:155] tlayer: compute_importance = False
I0104 22:22:10.955380 140379616616448 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0104 22:22:10.955559 140379616616448 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:22:10.960923 140379616616448 transformer_base.py:161] kvq: pre_attn dropout.
I0104 22:22:10.985647 140379616616448 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:22:10.987457 140379616616448 transformer_base.py:194] kvq: normalize keys, queries.
I0104 22:22:10.992568 140379616616448 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0104 22:22:11.025159 140379616616448 transformer_layer.py:299] tlayer: num_windows = 1.
I0104 22:22:11.025369 140379616616448 attention.py:418] Single window, no scan.
I0104 22:22:11.025497 140379616616448 transformer_layer.py:389] tlayer: self-attention.
I0104 22:22:11.025626 140379616616448 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[8,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:22:11.025891 140379616616448 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:22:11.027462 140379616616448 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:22:11.027755 140379616616448 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:22:11.028552 140379616616448 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:22:11.030097 140379616616448 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:22:11.035871 140379616616448 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:22:11.036601 140379616616448 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:22:11.036809 140379616616448 transformer_layer.py:468] tlayer: End windows.
I0104 22:22:11.036895 140379616616448 transformer_layer.py:472] tlayer: final FFN.
I0104 22:22:11.037007 140379616616448 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:22:11.037391 140379616616448 transformer_base.py:410] tbase: post-attention MLP.
I0104 22:22:11.037620 140379616616448 nn_components.py:325] mlp: activation = None
I0104 22:22:11.037738 140379616616448 nn_components.py:329] mlp: final activation = None
I0104 22:22:11.046113 140379616616448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:22:11.046437 140379616616448 nn_components.py:261] mlp: residual
I0104 22:22:11.052194 140379616616448 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:22:11.053045 140379616616448 transformer_base.py:443] tbase: final FFN
I0104 22:22:11.053600 140379616616448 nn_components.py:320] mlp: hidden 4096, relu
I0104 22:22:11.064699 140379616616448 nn_components.py:329] mlp: final activation = None
I0104 22:22:11.071789 140379616616448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:22:11.072129 140379616616448 nn_components.py:261] mlp: residual
I0104 22:22:11.072777 140379616616448 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:22:11.073017 140379616616448 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0104 22:22:11.073266 140379616616448 transformer_layer.py:154] tlayer: recurrent = False
I0104 22:22:11.073373 140379616616448 transformer_layer.py:155] tlayer: compute_importance = False
I0104 22:22:11.073459 140379616616448 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0104 22:22:11.073572 140379616616448 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:22:11.082364 140379616616448 transformer_base.py:161] kvq: pre_attn dropout.
I0104 22:22:11.109712 140379616616448 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:22:11.110741 140379616616448 transformer_base.py:194] kvq: normalize keys, queries.
I0104 22:22:11.116833 140379616616448 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0104 22:22:11.147268 140379616616448 transformer_layer.py:299] tlayer: num_windows = 1.
I0104 22:22:11.147524 140379616616448 attention.py:418] Single window, no scan.
I0104 22:22:11.147616 140379616616448 transformer_layer.py:389] tlayer: self-attention.
I0104 22:22:11.147713 140379616616448 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[8,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:22:11.147917 140379616616448 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:22:11.149102 140379616616448 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:22:11.149395 140379616616448 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:22:11.150462 140379616616448 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:22:11.153083 140379616616448 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:22:11.159130 140379616616448 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:22:11.160661 140379616616448 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:22:11.161041 140379616616448 transformer_layer.py:468] tlayer: End windows.
I0104 22:22:11.161125 140379616616448 transformer_layer.py:472] tlayer: final FFN.
I0104 22:22:11.161222 140379616616448 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:22:11.161680 140379616616448 transformer_base.py:410] tbase: post-attention MLP.
I0104 22:22:11.161916 140379616616448 nn_components.py:325] mlp: activation = None
I0104 22:22:11.162028 140379616616448 nn_components.py:329] mlp: final activation = None
I0104 22:22:11.171204 140379616616448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:22:11.171824 140379616616448 nn_components.py:261] mlp: residual
I0104 22:22:11.178406 140379616616448 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:22:11.178746 140379616616448 transformer_base.py:443] tbase: final FFN
I0104 22:22:11.179016 140379616616448 nn_components.py:320] mlp: hidden 4096, relu
I0104 22:22:11.188624 140379616616448 nn_components.py:329] mlp: final activation = None
I0104 22:22:11.198137 140379616616448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:22:11.198472 140379616616448 nn_components.py:261] mlp: residual
I0104 22:22:11.199524 140379616616448 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:22:11.199910 140379616616448 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0104 22:22:11.200202 140379616616448 transformer_layer.py:154] tlayer: recurrent = False
I0104 22:22:11.200299 140379616616448 transformer_layer.py:155] tlayer: compute_importance = False
I0104 22:22:11.200356 140379616616448 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0104 22:22:11.200454 140379616616448 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:22:11.207496 140379616616448 transformer_base.py:161] kvq: pre_attn dropout.
I0104 22:22:11.230507 140379616616448 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:22:11.231738 140379616616448 transformer_base.py:194] kvq: normalize keys, queries.
I0104 22:22:11.238590 140379616616448 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0104 22:22:11.269783 140379616616448 transformer_layer.py:299] tlayer: num_windows = 1.
I0104 22:22:11.270053 140379616616448 attention.py:418] Single window, no scan.
I0104 22:22:11.270159 140379616616448 transformer_layer.py:389] tlayer: self-attention.
I0104 22:22:11.270281 140379616616448 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[8,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:22:11.270569 140379616616448 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:22:11.272123 140379616616448 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:22:11.272598 140379616616448 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:22:11.273861 140379616616448 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:22:11.275795 140379616616448 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:22:11.282731 140379616616448 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:22:11.283771 140379616616448 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:22:11.284029 140379616616448 transformer_layer.py:468] tlayer: End windows.
I0104 22:22:11.284169 140379616616448 transformer_layer.py:472] tlayer: final FFN.
I0104 22:22:11.284397 140379616616448 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:22:11.284920 140379616616448 transformer_base.py:410] tbase: post-attention MLP.
I0104 22:22:11.285184 140379616616448 nn_components.py:325] mlp: activation = None
I0104 22:22:11.285280 140379616616448 nn_components.py:329] mlp: final activation = None
I0104 22:22:11.294312 140379616616448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:22:11.295286 140379616616448 nn_components.py:261] mlp: residual
I0104 22:22:11.301494 140379616616448 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:22:11.301784 140379616616448 transformer_base.py:443] tbase: final FFN
I0104 22:22:11.302007 140379616616448 nn_components.py:320] mlp: hidden 4096, relu
I0104 22:22:11.309684 140379616616448 nn_components.py:329] mlp: final activation = None
I0104 22:22:11.318821 140379616616448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:22:11.319226 140379616616448 nn_components.py:261] mlp: residual
I0104 22:22:11.319822 140379616616448 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:22:11.319997 140379616616448 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0104 22:22:11.320262 140379616616448 transformer_layer.py:154] tlayer: recurrent = False
I0104 22:22:11.320371 140379616616448 transformer_layer.py:155] tlayer: compute_importance = False
I0104 22:22:11.320449 140379616616448 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0104 22:22:11.320544 140379616616448 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:22:11.326048 140379616616448 transformer_base.py:161] kvq: pre_attn dropout.
I0104 22:22:11.356029 140379616616448 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:22:11.358036 140379616616448 transformer_base.py:194] kvq: normalize keys, queries.
I0104 22:22:11.365024 140379616616448 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0104 22:22:11.404435 140379616616448 transformer_layer.py:299] tlayer: num_windows = 1.
I0104 22:22:11.404672 140379616616448 attention.py:418] Single window, no scan.
I0104 22:22:11.404892 140379616616448 transformer_layer.py:389] tlayer: self-attention.
I0104 22:22:11.404979 140379616616448 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[8,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:22:11.405131 140379616616448 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:22:11.406421 140379616616448 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:22:11.407229 140379616616448 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:22:11.408945 140379616616448 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:22:11.411833 140379616616448 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:22:11.420090 140379616616448 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:22:11.420931 140379616616448 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:22:11.421116 140379616616448 transformer_layer.py:468] tlayer: End windows.
I0104 22:22:11.421236 140379616616448 transformer_layer.py:472] tlayer: final FFN.
I0104 22:22:11.421356 140379616616448 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:22:11.421814 140379616616448 transformer_base.py:410] tbase: post-attention MLP.
I0104 22:22:11.422091 140379616616448 nn_components.py:325] mlp: activation = None
I0104 22:22:11.422277 140379616616448 nn_components.py:329] mlp: final activation = None
I0104 22:22:11.431610 140379616616448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:22:11.431904 140379616616448 nn_components.py:261] mlp: residual
I0104 22:22:11.437119 140379616616448 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:22:11.437509 140379616616448 transformer_base.py:443] tbase: final FFN
I0104 22:22:11.437899 140379616616448 nn_components.py:320] mlp: hidden 4096, relu
I0104 22:22:11.450471 140379616616448 nn_components.py:329] mlp: final activation = None
I0104 22:22:11.457840 140379616616448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:22:11.458513 140379616616448 nn_components.py:261] mlp: residual
I0104 22:22:11.459887 140379616616448 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:22:11.460364 140379616616448 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0104 22:22:11.460765 140379616616448 transformer_layer.py:154] tlayer: recurrent = False
I0104 22:22:11.460882 140379616616448 transformer_layer.py:155] tlayer: compute_importance = False
I0104 22:22:11.460973 140379616616448 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0104 22:22:11.461070 140379616616448 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:22:11.466240 140379616616448 transformer_base.py:161] kvq: pre_attn dropout.
I0104 22:22:11.492820 140379616616448 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:22:11.493850 140379616616448 transformer_base.py:194] kvq: normalize keys, queries.
I0104 22:22:11.499789 140379616616448 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0104 22:22:11.537047 140379616616448 transformer_layer.py:299] tlayer: num_windows = 1.
I0104 22:22:11.537285 140379616616448 attention.py:418] Single window, no scan.
I0104 22:22:11.537366 140379616616448 transformer_layer.py:389] tlayer: self-attention.
I0104 22:22:11.537454 140379616616448 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[8,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:22:11.537665 140379616616448 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:22:11.538502 140379616616448 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:22:11.538770 140379616616448 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:22:11.539565 140379616616448 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:22:11.542435 140379616616448 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:22:11.550088 140379616616448 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:22:11.551064 140379616616448 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:22:11.551273 140379616616448 transformer_layer.py:468] tlayer: End windows.
I0104 22:22:11.551361 140379616616448 transformer_layer.py:472] tlayer: final FFN.
I0104 22:22:11.551480 140379616616448 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:22:11.551911 140379616616448 transformer_base.py:410] tbase: post-attention MLP.
I0104 22:22:11.552176 140379616616448 nn_components.py:325] mlp: activation = None
I0104 22:22:11.552276 140379616616448 nn_components.py:329] mlp: final activation = None
I0104 22:22:11.560758 140379616616448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:22:11.561091 140379616616448 nn_components.py:261] mlp: residual
I0104 22:22:11.567720 140379616616448 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:22:11.567974 140379616616448 transformer_base.py:443] tbase: final FFN
I0104 22:22:11.568167 140379616616448 nn_components.py:320] mlp: hidden 4096, relu
I0104 22:22:11.577288 140379616616448 nn_components.py:329] mlp: final activation = None
I0104 22:22:11.584943 140379616616448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:22:11.585476 140379616616448 nn_components.py:261] mlp: residual
I0104 22:22:11.586965 140379616616448 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:22:11.587583 140379616616448 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0104 22:22:11.588164 140379616616448 transformer_layer.py:154] tlayer: recurrent = False
I0104 22:22:11.588373 140379616616448 transformer_layer.py:155] tlayer: compute_importance = False
I0104 22:22:11.588487 140379616616448 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0104 22:22:11.588669 140379616616448 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:22:11.595438 140379616616448 transformer_base.py:161] kvq: pre_attn dropout.
I0104 22:22:11.630613 140379616616448 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:22:11.632367 140379616616448 transformer_base.py:194] kvq: normalize keys, queries.
I0104 22:22:11.643808 140379616616448 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0104 22:22:11.684224 140379616616448 transformer_layer.py:299] tlayer: num_windows = 1.
I0104 22:22:11.684484 140379616616448 attention.py:418] Single window, no scan.
I0104 22:22:11.684588 140379616616448 transformer_layer.py:389] tlayer: self-attention.
I0104 22:22:11.684662 140379616616448 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[8,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:22:11.684830 140379616616448 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:22:11.685895 140379616616448 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:22:11.686232 140379616616448 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:22:11.687816 140379616616448 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:22:11.692595 140379616616448 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:22:11.701736 140379616616448 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:22:11.703150 140379616616448 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:22:11.703589 140379616616448 transformer_layer.py:468] tlayer: End windows.
I0104 22:22:11.703722 140379616616448 transformer_layer.py:472] tlayer: final FFN.
I0104 22:22:11.703852 140379616616448 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:22:11.704492 140379616616448 transformer_base.py:410] tbase: post-attention MLP.
I0104 22:22:11.704969 140379616616448 nn_components.py:325] mlp: activation = None
I0104 22:22:11.705123 140379616616448 nn_components.py:329] mlp: final activation = None
I0104 22:22:11.714570 140379616616448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:22:11.714938 140379616616448 nn_components.py:261] mlp: residual
I0104 22:22:11.722785 140379616616448 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:22:11.723631 140379616616448 transformer_base.py:443] tbase: final FFN
I0104 22:22:11.724164 140379616616448 nn_components.py:320] mlp: hidden 4096, relu
I0104 22:22:11.735771 140379616616448 nn_components.py:329] mlp: final activation = None
I0104 22:22:11.745645 140379616616448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:22:11.745909 140379616616448 nn_components.py:261] mlp: residual
I0104 22:22:11.746626 140379616616448 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:22:11.746825 140379616616448 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0104 22:22:11.747172 140379616616448 transformer_layer.py:154] tlayer: recurrent = False
I0104 22:22:11.747254 140379616616448 transformer_layer.py:155] tlayer: compute_importance = False
I0104 22:22:11.747305 140379616616448 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0104 22:22:11.747386 140379616616448 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:22:11.754828 140379616616448 transformer_base.py:161] kvq: pre_attn dropout.
I0104 22:22:11.785332 140379616616448 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:22:11.787082 140379616616448 transformer_base.py:194] kvq: normalize keys, queries.
I0104 22:22:11.796403 140379616616448 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0104 22:22:11.832566 140379616616448 transformer_layer.py:299] tlayer: num_windows = 1.
I0104 22:22:11.832801 140379616616448 attention.py:418] Single window, no scan.
I0104 22:22:11.832884 140379616616448 transformer_layer.py:389] tlayer: self-attention.
I0104 22:22:11.832971 140379616616448 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[8,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:22:11.833133 140379616616448 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:22:11.833761 140379616616448 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:22:11.833916 140379616616448 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:22:11.834608 140379616616448 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:22:11.835967 140379616616448 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:22:11.841480 140379616616448 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:22:11.843101 140379616616448 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:22:11.843452 140379616616448 transformer_layer.py:468] tlayer: End windows.
I0104 22:22:11.843533 140379616616448 transformer_layer.py:472] tlayer: final FFN.
I0104 22:22:11.843631 140379616616448 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:22:11.844109 140379616616448 transformer_base.py:410] tbase: post-attention MLP.
I0104 22:22:11.844366 140379616616448 nn_components.py:325] mlp: activation = None
I0104 22:22:11.844552 140379616616448 nn_components.py:329] mlp: final activation = None
I0104 22:22:11.852533 140379616616448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:22:11.853255 140379616616448 nn_components.py:261] mlp: residual
I0104 22:22:11.859050 140379616616448 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:22:11.859320 140379616616448 transformer_base.py:443] tbase: final FFN
I0104 22:22:11.859544 140379616616448 nn_components.py:320] mlp: hidden 4096, relu
I0104 22:22:11.866553 140379616616448 nn_components.py:329] mlp: final activation = None
I0104 22:22:11.873614 140379616616448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:22:11.873981 140379616616448 nn_components.py:261] mlp: residual
I0104 22:22:11.874581 140379616616448 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:22:11.874804 140379616616448 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0104 22:22:11.875253 140379616616448 transformer_layer.py:154] tlayer: recurrent = False
I0104 22:22:11.875379 140379616616448 transformer_layer.py:155] tlayer: compute_importance = False
I0104 22:22:11.875469 140379616616448 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0104 22:22:11.875590 140379616616448 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:22:11.880765 140379616616448 transformer_base.py:161] kvq: pre_attn dropout.
I0104 22:22:11.901343 140379616616448 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:22:11.902190 140379616616448 transformer_base.py:194] kvq: normalize keys, queries.
I0104 22:22:11.906979 140379616616448 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0104 22:22:11.942733 140379616616448 transformer_layer.py:299] tlayer: num_windows = 1.
I0104 22:22:11.943300 140379616616448 attention.py:418] Single window, no scan.
I0104 22:22:11.943522 140379616616448 transformer_layer.py:389] tlayer: self-attention.
I0104 22:22:11.943660 140379616616448 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[8,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:22:11.943878 140379616616448 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:22:11.944728 140379616616448 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:22:11.944929 140379616616448 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:22:11.945766 140379616616448 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:22:11.947784 140379616616448 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:22:11.956646 140379616616448 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:22:11.958949 140379616616448 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:22:11.959418 140379616616448 transformer_layer.py:468] tlayer: End windows.
I0104 22:22:11.959582 140379616616448 transformer_layer.py:472] tlayer: final FFN.
I0104 22:22:11.959733 140379616616448 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:22:11.960273 140379616616448 transformer_base.py:410] tbase: post-attention MLP.
I0104 22:22:11.960567 140379616616448 nn_components.py:325] mlp: activation = None
I0104 22:22:11.960678 140379616616448 nn_components.py:329] mlp: final activation = None
I0104 22:22:11.970035 140379616616448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:22:11.970366 140379616616448 nn_components.py:261] mlp: residual
I0104 22:22:11.974820 140379616616448 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:22:11.975098 140379616616448 transformer_base.py:443] tbase: final FFN
I0104 22:22:11.975313 140379616616448 nn_components.py:320] mlp: hidden 4096, relu
I0104 22:22:11.982781 140379616616448 nn_components.py:329] mlp: final activation = None
I0104 22:22:11.988733 140379616616448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:22:11.989023 140379616616448 nn_components.py:261] mlp: residual
I0104 22:22:11.989848 140379616616448 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:22:11.990515 140379616616448 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0104 22:22:11.991291 140379616616448 transformer_layer.py:154] tlayer: recurrent = False
I0104 22:22:11.991491 140379616616448 transformer_layer.py:155] tlayer: compute_importance = False
I0104 22:22:11.991601 140379616616448 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0104 22:22:11.991740 140379616616448 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:22:11.997001 140379616616448 transformer_base.py:161] kvq: pre_attn dropout.
I0104 22:22:12.023590 140379616616448 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:22:12.025345 140379616616448 transformer_base.py:194] kvq: normalize keys, queries.
I0104 22:22:12.030699 140379616616448 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0104 22:22:12.061470 140379616616448 transformer_layer.py:299] tlayer: num_windows = 1.
I0104 22:22:12.061682 140379616616448 attention.py:418] Single window, no scan.
I0104 22:22:12.061782 140379616616448 transformer_layer.py:389] tlayer: self-attention.
I0104 22:22:12.061873 140379616616448 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[8,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:22:12.062040 140379616616448 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:22:12.062764 140379616616448 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:22:12.063001 140379616616448 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:22:12.063775 140379616616448 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:22:12.065482 140379616616448 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:22:12.072964 140379616616448 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[8,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:22:12.073861 140379616616448 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:22:12.074085 140379616616448 transformer_layer.py:468] tlayer: End windows.
I0104 22:22:12.074172 140379616616448 transformer_layer.py:472] tlayer: final FFN.
I0104 22:22:12.074329 140379616616448 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[8,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:22:12.074961 140379616616448 transformer_base.py:410] tbase: post-attention MLP.
I0104 22:22:12.075288 140379616616448 nn_components.py:325] mlp: activation = None
I0104 22:22:12.075433 140379616616448 nn_components.py:329] mlp: final activation = None
I0104 22:22:12.084021 140379616616448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:22:12.084284 140379616616448 nn_components.py:261] mlp: residual
I0104 22:22:12.088470 140379616616448 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:22:12.088697 140379616616448 transformer_base.py:443] tbase: final FFN
I0104 22:22:12.088910 140379616616448 nn_components.py:320] mlp: hidden 4096, relu
I0104 22:22:12.099608 140379616616448 nn_components.py:329] mlp: final activation = None
I0104 22:22:12.107974 140379616616448 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:22:12.108264 140379616616448 nn_components.py:261] mlp: residual
I0104 22:22:12.108868 140379616616448 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:22:12.109073 140379616616448 decoder_stack.py:344] dstack: Final layernorm.
I0104 22:22:12.116329 140379616616448 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0104 22:30:11.207424 140379616616448 alphageometry.py:394] PID=10401: !! try_translate_constrained_to_construct: string=n : C c d n 20 D c n d n 21 ;
I0104 22:30:11.246797 140379616616448 alphageometry.py:394] PID=10401: !! try_translate_constrained_to_construct: string=n : D d n k l 20 P d n k l 21 ;
I0104 22:30:11.282130 140379616616448 alphageometry.py:394] PID=10401: !! try_translate_constrained_to_construct: string=n : D d e d n 20 T b c d n 21 ;
I0104 22:30:11.300715 140379616616448 alphageometry.py:394] PID=10401: !! try_translate_constrained_to_construct: string=n : D d n d e 20 ;
I0104 22:30:11.314670 140379616616448 alphageometry.py:394] PID=10401: !! try_translate_constrained_to_construct: string=n : D d n d e 20 T b c d n 21 ;
I0104 22:30:11.332308 140379616616448 alphageometry.py:394] PID=10401: !! try_translate_constrained_to_construct: string=n : C b c n 20 D b n c n 21 ;
I0104 22:30:11.354027 140379616616448 alphageometry.py:394] PID=10401: !! try_translate_constrained_to_construct: string=n : D d e d n 20 ;
I0104 22:30:11.363900 140379616616448 alphageometry.py:394] PID=10401: !! try_translate_constrained_to_construct: string=n : C b c n 20 T b c d n 21 ;
I0104 22:30:11.378664 140379616616448 alphageometry.py:574] Worker PID=10401: LM output (score=-0.3721431791782379): "n : C b c n 20 T b c d n 21 ;"
I0104 22:30:11.378842 140379616616448 alphageometry.py:575] Worker PID=10401: Translation: "n = on_line n b c, on_tline n d b c"
I0104 22:30:11.378911 140379616616448 alphageometry.py:585] Worker PID=10401: string=|{S} a : ; b : ; c : ; d : ^ a b a d a d a c 00 ^ c a c d c d c b 01 ; e : C a c e 02 T a c d e 03 ; f : C a b f 04 T a b d f 05 ; g : C d e g 06 D d e d g 07 ; h : C d f h 08 D d f d h 09 ; i : C b c i 10 P a c g i 11 ; j : C b c j 12 P a b h j 13 ; k : C a c k 14 D a k c k 15 ; l : C a b l 16 D a l b l 17 ; m : C a d m 18 O a b c m 19 ? ^ m j m i d l d k {F1} x00| lm_out=|n : C b c n 20 T b c d n 21 ;|
I0104 22:30:11.379029 140379616616448 alphageometry.py:586] Worker PID=10401: Solving: "a b c = triangle a b c; d = incenter d a b c; e = foot e d a c; f = foot f d a b; g = on_line g d e, on_circle g d e; h = on_line h d f, on_circle h d f; i = on_line i b c, on_pline i g a c; j = on_line j b c, on_pline j h a b; k = midpoint k a c; l = midpoint l a b; m = on_line m a d, on_circum m a b c; n = on_line n b c, on_tline n d b c ? eqangle m j m i d l d k"
I0104 22:30:11.380281 140379616616448 graph.py:498] 
I0104 22:30:11.380578 140379616616448 graph.py:499] a b c = triangle a b c; d = incenter d a b c; e = foot e d a c; f = foot f d a b; g = on_line g d e, on_circle g d e; h = on_line h d f, on_circle h d f; i = on_line i b c, on_pline i g a c; j = on_line j b c, on_pline j h a b; k = midpoint k a c; l = midpoint l a b; m = on_line m a d, on_circum m a b c; n = on_line n b c, on_tline n d b c ? eqangle m j m i d l d k
I0104 22:30:14.580483 140379616616448 ddar.py:60] Depth 1/1000 time = 3.088320732116699
I0104 22:30:23.869027 140379616616448 ddar.py:60] Depth 2/1000 time = 9.288252830505371
I0104 22:30:59.462449 140379616616448 ddar.py:60] Depth 3/1000 time = 35.59307408332825
I0104 22:31:23.755461 140379616616448 ddar.py:60] Depth 4/1000 time = 24.292661905288696
I0104 22:31:50.231151 140379616616448 ddar.py:60] Depth 5/1000 time = 26.475298166275024
I0104 22:32:16.997727 140379616616448 ddar.py:60] Depth 6/1000 time = 26.766136407852173
I0104 22:32:44.541972 140379616616448 ddar.py:60] Depth 7/1000 time = 27.543803691864014
I0104 22:33:09.093028 140379616616448 ddar.py:60] Depth 8/1000 time = 24.550540685653687
I0104 22:33:34.841915 140379616616448 ddar.py:60] Depth 9/1000 time = 25.747596740722656
I0104 22:34:00.395315 140379616616448 ddar.py:60] Depth 10/1000 time = 25.39754581451416
I0104 22:34:25.774947 140379616616448 ddar.py:60] Depth 11/1000 time = 25.37899160385132
I0104 22:34:55.811043 140379616616448 ddar.py:60] Depth 12/1000 time = 29.91286015510559
I0104 22:35:24.698767 140379616616448 ddar.py:60] Depth 13/1000 time = 28.886189222335815
I0104 22:35:53.729503 140379616616448 ddar.py:60] Depth 14/1000 time = 29.029617071151733
I0104 22:36:22.409526 140379616616448 ddar.py:60] Depth 15/1000 time = 28.645824432373047
I0104 22:36:51.557674 140379616616448 ddar.py:60] Depth 16/1000 time = 29.114336252212524
I0104 22:36:52.087595 140379616616448 alphageometry.py:201] 
==========================
 * From theorem premises:
A B C D E F G H I J K L M : Points
DCA = BCD [00]
DAB = CAD [01]
A,C,E are collinear [02]
DE  AC [03]
B,A,F are collinear [04]
DF  AB [05]
DG = DE [06]
G,D,E are collinear [07]
H,F,D are collinear [08]
DH = DF [09]
B,I,C are collinear [10]
IG  AC [11]
B,C,J are collinear [12]
JH  AB [13]
K,A,C are collinear [14]
KA = KC [15]
B,A,L are collinear [16]
LA = LB [17]
M,A,D are collinear [18]
B,M,A,C are concyclic [19]

 * Auxiliary Constructions:
N : Points
N,B,C are collinear [20]
DN  BC [21]

 * Proof steps:
001. B,M,A,C are concyclic [19]   BAM = BCM [22]
002. B,M,A,C are concyclic [19]   CMB = CAB [23]
003. B,M,A,C are concyclic [19]   BMA = BCA [24]
004. M,A,D are collinear [18] & C,B,N are collinear [20] & B,F,A are collinear [04] & BAM = BCM [22]   CMD = NBF [25]
005. C,A,E are collinear [02] & B,N,C are collinear [20] & DN  BC [21] & DE  AC [03]   DEC = CND [26]
006. A,C,E are collinear [02] & B,N,C are collinear [20] & DCA = BCD [00]   DCE = NCD [27]
007. DEC = CND [26] & DCE = NCD [27] (Similar Triangles)  DE = DN [28]
008. DEC = CND [26] & DCE = NCD [27] (Similar Triangles)  CE = CN [29]
009. DEC = CND [26] & DCE = NCD [27] (Similar Triangles)  ED:EC = ND:NC [30]
010. DEC = CND [26] & DCE = NCD [27] (Similar Triangles)  EDC = CDN [31]
011. B,F,A are collinear [04] & C,A,E are collinear [02] & DE  AC [03] & DF  AB [05]   DFA = AED [32]
012. B,A,F are collinear [04] & C,A,E are collinear [02] & DAB = CAD [01]   DAF = EAD [33]
013. DFA = AED [32] & DAF = EAD [33] (Similar Triangles)  DF = DE [34]
014. DFA = AED [32] & DAF = EAD [33] (Similar Triangles)  AF = AE [35]
015. DG = DE [06] & DE = DN [28] & DF = DE [34]   D is the circumcenter of \Delta NGF [36]
016. DG = DE [06] & DE = DN [28] & DF = DE [34]   N,G,F,E are concyclic [37]
017. B,F,A are collinear [04] & DF  AB [05]   DF  FB [38]
018. D is the circumcenter of \Delta NGF [36] & DF  FB [38]   BFN = FGN [39]
019. G,D,E are collinear [07] & DG = DE [06]   D is midpoint of GE [40]
020. H,F,D are collinear [08] & DH = DF [09]   D is midpoint of HF [41]
021. D is midpoint of GE [40] & D is midpoint of HF [41]   GF  EH [42]
022. D is midpoint of GE [40] & D is midpoint of HF [41]   GH  EF [43]
023. DE = DN [28] & CE = CN [29]   NE  DC [44]
024. DE = DN [28] & DG = DE [06]   D is the circumcenter of \Delta NEG [45]
025. D is the circumcenter of \Delta NEG [45] & G,D,E are collinear [07]   EN  GN [46]
026. DF = DE [34] & AF = AE [35]   EF  DA [47]
027. DG = DE [06] & DF = DE [34]   D is the circumcenter of \Delta EGF [48]
028. D is the circumcenter of \Delta EGF [48] & G,D,E are collinear [07]   EF  FG [49]
029. M,A,D are collinear [18] & B,F,A are collinear [04] & BFN = FGN [39] & FG  EH [42] & NE  DC [44] & EN  GN [46] & EF  DA [47] & EF  FG [49]   CDM = NFB [50]
030. CMD = NBF [25] & CDM = NFB [50] (Similar Triangles)  MC:DC = BN:NF [51]
031. B,A,L are collinear [16] & D,H,F are collinear [08] & AB  DF [05]   BL  HF [52]
032. B,I,C are collinear [10] & DN  BC [21]   ND  IC [53]
033. BL  HF [52] & ND  IC [53]   (BL-IC) = (HF-ND) [54]
034. BL  HF [52] & ND  IC [53]   (BL-ND) = (HF-IC) [55]
035. B,I,C are collinear [10] & D,H,F are collinear [08] & B,A,L are collinear [16] & (BL-IC) = (HF-ND) [54]   (ND-IC) = (HF-BL) [56]
036. B,I,C are collinear [10] & B,A,L are collinear [16] & M,A,D are collinear [18] & BAM = BCM [22]   ICM = (BL-MA) [57]
037. (ND-IC) = (HF-BL) [56] & ICM = (BL-MA) [57]   (ND-MC) = (HF-MA) [58]
038. M,A,D are collinear [18] & H,D,F are collinear [08] & (ND-MC) = (HF-MA) [58]   CMD = NDH [59]
039. DF = DE [34] & DE = DN [28] & DH = DF [09]   D is the circumcenter of \Delta NFH [60]
040. DF = DE [34] & DE = DN [28] & DH = DF [09]   DN = DH [61]
041. DF = DE [34] & DE = DN [28] & DH = DF [09]   D is the circumcenter of \Delta NEH [62]
042. C,N,B are collinear [20] & B,I,C are collinear [10] & DN  BC [21]   DN  NI [63]
043. D is the circumcenter of \Delta NFH [60] & DN  NI [63]   INF = NHF [64]
044. D is the circumcenter of \Delta NGF [36] & DN  NI [63]   ING = NFG [65]
045. M,A,D are collinear [18] & H,D,F are collinear [08] & INF = NHF [64] & C,N,B are collinear [20] & B,I,C are collinear [10] & ING = NFG [65] & FG  EH [42] & NE  DC [44] & EN  GN [46] & EF  DA [47] & EF  FG [49]   CDM = NHD [66]
046. CMD = NDH [59] & CDM = NHD [66] (Similar Triangles)  DN:DH = MC:MD [67]
047. CMD = NDH [59] & CDM = NHD [66] (Similar Triangles)  MC:DC = ND:NH [68]
048. DN:DH = MC:MD [67] & DN = DH [61]   MC = MD [69]
049. B,F,A are collinear [04] & C,N,B are collinear [20] & (BL-IC) = (HF-ND) [54] & B,A,L are collinear [16] & B,I,C are collinear [10] & H,F,D are collinear [08]   DFB = DNB [70]
050. DFB = DNB [70]   N,B,F,D are concyclic [71]
051. N,B,F,D are concyclic [71]   NBD = NFD [72]
052. N,B,F,D are concyclic [71]   NFB = NDB [73]
053. DE = DN [28] & DF = DE [34]   D is the circumcenter of \Delta NEF [74]
054. DE = DN [28] & DF = DE [34]   DN = DF [75]
055. D is the circumcenter of \Delta NEF [74] & DN  NI [63]   INE = NFE [76]
056. D is the circumcenter of \Delta NEF [74] & DF  FB [38]   BFN = FEN [77]
057. B,F,A are collinear [04] & NBD = NFD [72] & N,B,C are collinear [20] & INE = NFE [76] & B,I,C are collinear [10] & BFN = FEN [77]   NFB = BDF [78]
058. N,B,F,D are concyclic [71] & NFB = BDF [78]   NB = BF [79]
059. MC:DC = BN:NF [51] & MD = MC [69] & BF = BN [79]   MD:DC = BF:NF [80]
060. N,G,F,E are concyclic [37]   NFG = NEG [81]
061. C,A,E are collinear [02] & B,N,C are collinear [20] & DN  BC [21] & DE  AC [03]   DEC = DNC [82]
062. DEC = DNC [82]   N,D,C,E are concyclic [83]
063. N,D,C,E are concyclic [83]   NCD = NED [84]
064. B,C,J are collinear [12] & NFG = NEG [81] & G,D,E are collinear [07] & FG  EH [42] & NCD = NED [84] & N,B,C are collinear [20]   DCJ = GFN [85]
065. H,D,F are collinear [08] & C,N,B are collinear [20] & B,C,J are collinear [12] & (BL-IC) = (HF-ND) [54] & B,A,L are collinear [16] & B,I,C are collinear [10] & AB  HJ [13]   DHJ = DNJ [86]
066. DHJ = DNJ [86]   N,H,D,J are concyclic [87]
067. N,H,D,J are concyclic [87]   NHD = NJD [88]
068. N,H,D,J are concyclic [87]   HND = HJD [89]
069. N,H,D,J are concyclic [87]   NHJ = NDJ [90]
070. B,C,J are collinear [12] & ING = NFG [65] & C,N,B are collinear [20] & B,I,C are collinear [10] & FG  EH [42] & INF = NHF [64] & H,F,D are collinear [08] & NHD = NJD [88]   DJC = FGN [91]
071. DCJ = GFN [85] & DJC = FGN [91] (Similar Triangles)  DC:DJ = NF:GN [92]
072. MD:DC = BF:NF [80] & DC:DJ = NF:GN [92]   MD:DJ = BF:GN [93]
073. MD:DJ = BF:GN [93] & MC = MD [69] & NB = BF [79]   DM:DJ = NB:NG [94]
074. B,I,C are collinear [10] & NHD = NJD [88] & H,F,D are collinear [08] & C,N,B are collinear [20] & B,C,J are collinear [12] & INF = NHF [64]   (NF-IC) = (DJ-IC) [95]
075. (NF-IC) = (DJ-IC) [95]   NF  DJ [96]
076. M,A,D are collinear [18] & C,N,B are collinear [20] & ING = NFG [65] & B,I,C are collinear [10] & FG  EH [42] & EF  DA [47] & EF  FG [49] & FN  DJ [96]   MDJ = GNB [97]
077. DM:DJ = NB:NG [94] & MDJ = GNB [97] (Similar Triangles)  DMJ = GBN [98]
078. DMJ = GBN [98] & M,A,D are collinear [18] & N,B,C are collinear [20]   (AD-JM) = GBC [99]
079. DE = DN [28] & DG = DE [06] & CN = CE [29]   DN:DG = CN:CE [100]
080. G,D,E are collinear [07] & C,A,K are collinear [14] & DE  AC [03]   GD  KA [101]
081. ND  IC [53] & GD  KA [101]   NDG = (IC-KA) [102]
082. G,D,E are collinear [07] & B,N,C are collinear [20] & A,C,E are collinear [02] & NDG = (IC-KA) [102] & B,I,C are collinear [10] & K,A,C are collinear [14]   NDG = NCE [103]
083. DN:DG = CN:CE [100] & NDG = NCE [103] (Similar Triangles)  ND:NC = NG:NE [104]
084. DN:DG = CN:CE [100] & NDG = NCE [103] (Similar Triangles)  DNG = CNE [105]
085. DG = DE [06] & DE = DN [28] & DH = DF [09] & DF = DE [34]   D is the circumcenter of \Delta NGH [106]
086. D is the circumcenter of \Delta NGH [106] & DN  NI [63]   ING = NHG [107]
087. ING = NHG [107] & C,N,B are collinear [20] & B,I,C are collinear [10] & GH  EF [43] & NE  DC [44] & EN  GN [46]   DCB = GHN [108]
088. ING = NHG [107] & C,N,B are collinear [20] & B,I,C are collinear [10] & GH  EF [43] & INF = NHF [64] & H,F,D are collinear [08] & NBD = NFD [72]   DBC = HGN [109]
089. DCB = GHN [108] & DBC = HGN [109] (Similar Triangles)  GN:BD = GH:BC [110]
090. M,A,D are collinear [18] & BAD = DAC [01]   BAM = MAC [111]
091. B,M,A,C are concyclic [19] & BAM = MAC [111]   MB = MC [112]
092. DE = DF [34] & MB = MC [112]   DE:DF = MB:MC [113]
093. BL  HF [52] & GD  KA [101]   (BL-KA) = (HF-GD) [114]
094. CMB = CAB [23] & (BL-KA) = (HF-GD) [114] & B,A,L are collinear [16] & K,A,C are collinear [14] & H,F,D are collinear [08] & G,D,E are collinear [07]   EDF = CMB [115]
095. DE:DF = MB:MC [113] & EDF = CMB [115] (Similar Triangles)  ED:BM = EF:BC [116]
096. G,D,E are collinear [07] & H,D,F are collinear [08]   EDF = GDH [117]
097. DG = DE [06] & DH = DF [09] & EDF = GDH [117] (SAS)  FE = HG [118]
098. GN:BD = GH:BC [110] & ED:BM = EF:BC [116] & DE = DN [28] & BM = MC [112] & FE = HG [118] & MD = MC [69]   ND:MD = GN:BD [119]
099. NG:NE = ND:NC [104] & ND:MD = GN:BD [119]   NC:NE = MD:BD [120]
100. D is the circumcenter of \Delta NEH [62] & DN  NI [63]   INE = NHE [121]
101. D is the circumcenter of \Delta NEG [45] & DN  NI [63]   INE = NGE [122]
102. G,D,E are collinear [07] & C,N,B are collinear [20] & B,I,C are collinear [10] & DN  BC [21] & DE  AC [03] & AC  GI [11]   IGD = IND [123]
103. IGD = IND [123]   N,G,I,D are concyclic [124]
104. N,G,I,D are concyclic [124]   NGD = NID [125]
105. B,I,C are collinear [10] & INE = NHE [121] & C,N,B are collinear [20] & INE = NGE [122] & G,D,E are collinear [07] & NGD = NID [125]   DIB = EHN [126]
106. B,I,C are collinear [10] & INE = NHE [121] & C,N,B are collinear [20] & INF = NHF [64] & H,F,D are collinear [08] & NBD = NFD [72]   DBI = HEN [127]
107. DIB = EHN [126] & DBI = HEN [127] (Similar Triangles)  ID:BD = NH:NE [128]
108. MD:BD = NC:NE [120] & ID:BD = NH:NE [128]   MD:ID = NC:NH [129]
109. G,D,E are collinear [07] & INE = NGE [122] & C,N,B are collinear [20] & B,I,C are collinear [10] & NGD = NID [125]   IDG = (NE-GD) [130]
110. IDG = (NE-GD) [130]   ID  NE [131]
111. M,A,D are collinear [18] & B,N,C are collinear [20] & INE = NHE [121] & B,I,C are collinear [10] & EF  DA [47] & EF  FG [49] & FG  EH [42] & EN  DI [131]   MDI = HNC [132]
112. NC:NH = MD:ID [129] & MDI = HNC [132] (Similar Triangles)  DMI = HCN [133]
113. DMI = HCN [133] & M,A,D are collinear [18] & N,B,C are collinear [20]   (AD-IM) = HCB [134]
114. (AD-JM) = GBC [99] & (AD-IM) = HCB [134] (Angle chase)  (BG-CH) = IMJ [135]
115. B,A,L are collinear [16] & LA = LB [17]   L is midpoint of AB [136]
116. L is midpoint of AB [136] & D is midpoint of GE [40]   LB:BA = DG:GE [137]
117. LB:BA = DG:GE [137] & DG = DE [06]   BL:BA = DE:GE [138]
118. ED:EC = ND:NC [30] & DE = DN [28]   ND:CE = ND:NC [139]
119. B,N,C are collinear [20] & DNG = CNE [105]   CND = ENG [140]
120. D,G,E are collinear [07] & CDN = EDC [31] & NE  DC [44] & EN  GN [46]   CDN = EGN [141]
121. CND = ENG [140] & CDN = EGN [141] (Similar Triangles)  CN:CD = EN:EG [142]
122. GH  EF [43] & H,F,D are collinear [08] & G,D,E are collinear [07]   FD:ED = FH:EG [143]
123. CN:CD = EN:EG [142] & CE = CN [29] & FD:ED = FH:EG [143] & DF = DE [34]   CE:DC = NE:HF [144]
124. B,I,C are collinear [10] & A,C,E are collinear [02] & DCB = ACD [00]   DCI = ECD [145]
125. B,I,C are collinear [10] & NGD = NID [125] & G,D,E are collinear [07] & C,N,B are collinear [20] & NE  DC [44] & EN  GN [46]   DIC = EDC [146]
126. DCI = ECD [145] & DIC = EDC [146] (Similar Triangles)  DC:IC = CE:DC [147]
127. H,D,F are collinear [08] & DF  AB [05] & AB  HJ [13]   DH  HJ [148]
128. D is the circumcenter of \Delta NEH [62] & DH  HJ [148]   JHE = HNE [149]
129. D is the circumcenter of \Delta NEH [62] & DH  HJ [148]   JHN = HEN [150]
130. JHE = HNE [149] & HJ  AB [13] & EF  DA [47] & EF  FG [49] & FG  EH [42]   HNE = BAD [151]
131. DN = DF [75]   FND = DFN [152]
132. NFB = NDB [73] & B,A,F are collinear [04] & FND = DFN [152] & INF = NHF [64] & C,N,B are collinear [20] & B,I,C are collinear [10] & H,F,D are collinear [08] & INE = NHE [121]   HEN = ABD [153]
133. HNE = BAD [151] & HEN = ABD [153] (Similar Triangles)  NH:NE = AD:BA [154]
134. DG = DE [06] & LA = LB [17] & BL:BA = DE:GE [138] & ND:CE = ND:NC [139] & CE:DC = NE:HF [144] & DC:IC = CE:DC [147] & MC:DC = ND:NH [68] & MC = MD [69] & NH:NE = AD:BA [154] (Ratio chase)  AL:AD = MD:IC [155]
135. AL:AD = MD:IC [155] & LA = LB [17] & MC = MD [69]   CM:CI = AL:AD [156]
136. B,I,C are collinear [10] & B,A,L are collinear [16] & BAM = BCM [22] & M,A,D are collinear [18]   MCI = DAL [157]
137. CM:CI = AL:AD [156] & MCI = DAL [157] (Similar Triangles)  MIC = ADL [158]
138. B,I,C are collinear [10] & (AD-IM) = HCB [134] & MIC = ADL [158]   (LD-IC) = HCI [159]
139. (LD-IC) = HCI [159]   LD  HC [160]
140. L is midpoint of AB [136] & D is midpoint of HF [41]   LB:BA = DH:HF [161]
141. LB:BA = DH:HF [161] & DH = DF [09]   BL:BA = FD:HF [162]
142. K,A,C are collinear [14] & KA = KC [15]   K is midpoint of AC [163]
143. K is midpoint of AC [163] & D is midpoint of GE [40]   KA:AC = DG:GE [164]
144. KA:AC = DG:GE [164] & KA = KC [15] & DG = DE [06]   KC:AC = DE:GE [165]
145. H,D,F are collinear [08] & NHD = NJD [88] & C,N,B are collinear [20] & B,C,J are collinear [12] & INE = NHE [121] & B,I,C are collinear [10] & JHN = HEN [150] & HJ  AB [13]   NHJ = JDH [166]
146. N,H,D,J are concyclic [87] & NHJ = JDH [166]   NJ = JH [167]
147. H,D,F are collinear [08] & B,F,A are collinear [04] & DF  AB [05] & AB  HJ [13]   DHJ = BFD [168]
148. DN = DH [61]   DNH = NHD [169]
149. NBD = NFD [72] & N,B,C are collinear [20] & INF = NHF [64] & B,I,C are collinear [10] & H,F,D are collinear [08] & DNH = NHD [169] & HND = HJD [89] & HJ  AB [13]   DJH = BDF [170]
150. DHJ = BFD [168] & DJH = BDF [170] (Similar Triangles)  DH:DJ = BF:BD [171]
151. DHJ = BFD [168] & DJH = BDF [170] (Similar Triangles)  HD:HJ = FB:FD [172]
152. DH:DJ = BF:BD [171] & HD = ND [61] & NB = BF [79]   ND:DJ = BN:BD [173]
153. HD:HJ = FB:FD [172] & HD = ND [61] & NJ = JH [167] & NB = BF [79] & FD = ND [75]   ND:NJ = BN:ND [174]
154. D is the circumcenter of \Delta NFH [60] & DF  FB [38]   BFN = FHN [175]
155. C,N,B are collinear [20] & B,C,J are collinear [12] & (BL-ND) = (HF-IC) [55] & B,A,L are collinear [16] & H,F,D are collinear [08] & B,I,C are collinear [10] & BFN = FHN [175] & B,A,F are collinear [04]   HNF = JND [176]
156. D is the circumcenter of \Delta NFH [60] & DH  HJ [148]   JHN = HFN [177]
157. D,H,F are collinear [08] & NHJ = NDJ [90] & HJ  AB [13] & JHN = HFN [177]   NFH = NDJ [178]
158. HNF = JND [176] & NFH = NDJ [178] (Similar Triangles)  NH:HF = NJ:DJ [179]
159. B,C,J are collinear [12] & DCB = ACD [00]   DCJ = ACD [180]
160. B,C,J are collinear [12] & ING = NFG [65] & C,N,B are collinear [20] & B,I,C are collinear [10] & FG  EH [42] & INF = NHF [64] & H,F,D are collinear [08] & NHD = NJD [88] & EF  DA [47] & EF  FG [49] & NE  DC [44] & EN  GN [46]   DJC = ADC [181]
161. DCJ = ACD [180] & DJC = ADC [181] (Similar Triangles)  DC:DJ = AC:AD [182]
162. B,C,J are collinear [12] & NBD = NFD [72] & N,B,C are collinear [20] & INF = NHF [64] & B,I,C are collinear [10] & H,F,D are collinear [08] & NHD = NJD [88]   DJB = FDB [183]
163. B,C,J are collinear [12] & B,F,A are collinear [04] & NFB = NDB [73] & FND = DFN [152] & NBD = NFD [72] & N,B,C are collinear [20]   DBJ = FBD [184]
164. DJB = FDB [183] & DBJ = FBD [184] (Similar Triangles)  JD:JB = DF:DB [185]
165. JD:JB = DF:DB [185] & FD = ND [75]   DJ:BJ = ND:BD [186]
166. DH = DF [09] & BL:BA = DE:GE [138] & BL:BA = FD:HF [162] & KC:AC = DE:GE [165] & DF = DE [34] & DE = DN [28] & NJ = JH [167] & ND:DJ = BN:BD [173] & ND:NJ = BN:ND [174] & NH:HF = NJ:DJ [179] & DC:DJ = AC:AD [182] & MC:DC = ND:NH [68] & MC = MD [69] & DJ:BJ = ND:BD [186] (Ratio chase)  AK:AD = MD:BJ [187]
167. AK:AD = MD:BJ [187] & KA = KC [15] & MC = MD [69] & BM = MC [112]   BM:BJ = AK:AD [188]
168. B,C,J are collinear [12] & C,A,K are collinear [14] & BMA = BCA [24] & M,A,D are collinear [18]   MBJ = DAK [189]
169. BM:BJ = AK:AD [188] & MBJ = DAK [189] (Similar Triangles)  MJB = ADK [190]
170. B,I,C are collinear [10] & MJB = ADK [190] & B,C,J are collinear [12] & (AD-JM) = GBC [99]   (BG-IC) = (KD-IC) [191]
171. (BG-IC) = (KD-IC) [191]   BG  KD [192]
172. (BG-CH) = IMJ [135] & DL  CH [160] & BG  DK [192]   JMI = LDK
==========================

I0104 22:36:52.088416 140379616616448 alphageometry.py:205] Solution written to /home/peng/ag4mtest/imo-2024-q4.out.
I0104 22:38:39.671459 140379616616448 alphageometry.py:594] Worker PID=10401: Solved.
